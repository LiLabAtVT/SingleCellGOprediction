{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "508ac4e0-efd9-4afe-bbb4-2a928d648b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "#sys.path.append(\"code/.\")\n",
    "\n",
    "#import mglearn\n",
    "from IPython.display import display\n",
    "#from plotting_functions import *\n",
    "\n",
    "# Classifiers and regressors\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "# Preprocessing and pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# train test split and cross validation\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split, cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    "    LabelEncoder,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "#from utils import *\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c983ab6-9715-479b-becf-7caee77cbba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_33clust = pd.read_csv(\"DAP_seq_33clusters_scRNAseq_combined_24March2023.csv\", index_col = 0)\n",
    "cols_to_drop = []\n",
    "counts_1=[]\n",
    "for i in range(602,len(dap_33clust.columns)):\n",
    "    y=dap_33clust[dap_33clust.columns[i]]\n",
    "    counter=Counter(y)\n",
    "    if(counter[1]<100):\n",
    "        cols_to_drop.append(dap_33clust.columns[i])\n",
    "    else:\n",
    "        counts_1.append(counter[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa7f7943-3d78-49de-8ce5-ff2041217d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>GO:0061630</th>\n",
       "      <th>GO:0071456</th>\n",
       "      <th>GO:0071555</th>\n",
       "      <th>GO:0071704</th>\n",
       "      <th>GO:0090305</th>\n",
       "      <th>GO:0090502</th>\n",
       "      <th>GO:0098869</th>\n",
       "      <th>GO:0099503</th>\n",
       "      <th>GO:0106310</th>\n",
       "      <th>GO:0110165</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gene ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT1G01020</th>\n",
       "      <td>0.825579</td>\n",
       "      <td>0.410447</td>\n",
       "      <td>0.261396</td>\n",
       "      <td>0.152758</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.272320</td>\n",
       "      <td>0.258516</td>\n",
       "      <td>0.457860</td>\n",
       "      <td>0.321913</td>\n",
       "      <td>0.371860</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT1G01030</th>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.015176</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.275636</td>\n",
       "      <td>0.115203</td>\n",
       "      <td>0.008890</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>0.055250</td>\n",
       "      <td>0.003478</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT1G01040</th>\n",
       "      <td>0.022025</td>\n",
       "      <td>0.023524</td>\n",
       "      <td>0.100888</td>\n",
       "      <td>0.008440</td>\n",
       "      <td>0.090527</td>\n",
       "      <td>0.108768</td>\n",
       "      <td>0.104148</td>\n",
       "      <td>0.046074</td>\n",
       "      <td>0.076399</td>\n",
       "      <td>0.018313</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT1G01050</th>\n",
       "      <td>0.958800</td>\n",
       "      <td>1.651358</td>\n",
       "      <td>2.126612</td>\n",
       "      <td>1.235073</td>\n",
       "      <td>1.609432</td>\n",
       "      <td>0.692831</td>\n",
       "      <td>3.054592</td>\n",
       "      <td>0.215002</td>\n",
       "      <td>0.714507</td>\n",
       "      <td>5.115776</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1         2         3         4         5         6   \n",
       "Gene ID                                                                 \n",
       "AT1G01020  0.825579  0.410447  0.261396  0.152758  0.163924  0.272320  \\\n",
       "AT1G01030  0.001090  0.003155  0.015176  0.000867  0.275636  0.115203   \n",
       "AT1G01040  0.022025  0.023524  0.100888  0.008440  0.090527  0.108768   \n",
       "AT1G01050  0.958800  1.651358  2.126612  1.235073  1.609432  0.692831   \n",
       "\n",
       "                  7         8         9        10  ...  GO:0061630   \n",
       "Gene ID                                            ...               \n",
       "AT1G01020  0.258516  0.457860  0.321913  0.371860  ...           0  \\\n",
       "AT1G01030  0.008890  0.002562  0.055250  0.003478  ...           0   \n",
       "AT1G01040  0.104148  0.046074  0.076399  0.018313  ...           0   \n",
       "AT1G01050  3.054592  0.215002  0.714507  5.115776  ...           0   \n",
       "\n",
       "           GO:0071456  GO:0071555  GO:0071704  GO:0090305  GO:0090502   \n",
       "Gene ID                                                                 \n",
       "AT1G01020           0           0           0           0           0  \\\n",
       "AT1G01030           0           0           0           0           0   \n",
       "AT1G01040           0           0           0           1           1   \n",
       "AT1G01050           0           0           0           0           0   \n",
       "\n",
       "           GO:0098869  GO:0099503  GO:0106310  GO:0110165  \n",
       "Gene ID                                                    \n",
       "AT1G01020           0           0           0           0  \n",
       "AT1G01030           0           0           0           0  \n",
       "AT1G01040           0           0           0           0  \n",
       "AT1G01050           0           0           0           0  \n",
       "\n",
       "[4 rows x 261 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dap_33clust.drop(cols_to_drop, axis=1, inplace=True)\n",
    "dap_33clust.iloc[1:5, 568:5670]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5996aec8-6b0d-4cd6-af5c-d0e99ca16664",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAP_columns = list(range(568))\n",
    "Cluster_columns = list(range(568,602))\n",
    "GO_columns = list(range(602,829))\n",
    "Root33_data_setA, Root33_data_setB = train_test_split(dap_33clust, test_size = 0.5, random_state = 42)\n",
    "# variable_genes = pd.read_csv('ATH_6000_variable_gens_14Feb2023.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17811165-576e-45bc-8f5e-54c8d1e6e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of machine learning models to test\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'MLP': MLPClassifier(),\n",
    "    'LDA': LinearDiscriminantAnalysis(),\n",
    "    'Adaboost': AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc32dc9-b6e3-428d-8132-0d7d7ef80fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(602, 829)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_validation(model, _X, _y, _cv=5):\n",
    "      '''Function to perform 5 Folds Cross-Validation\n",
    "       Parameters\n",
    "       ----------\n",
    "      model: Python Class, default=None\n",
    "              This is the machine learning algorithm to be used for training.\n",
    "      _X: array\n",
    "           This is the matrix of features.\n",
    "      _y: array\n",
    "           This is the target variable.\n",
    "      _cv: int, default=5\n",
    "          Determines the number of folds for cross-validation.\n",
    "       Returns\n",
    "       -------\n",
    "       The function returns a dictionary containing the metrics 'accuracy', 'precision',\n",
    "       'recall', 'f1' for both training set and validation set.\n",
    "      '''\n",
    "      _scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "      results = cross_validate(estimator=model,\n",
    "                               X=_X,\n",
    "                               y=_y,\n",
    "                               cv=_cv,\n",
    "                               scoring=_scoring,\n",
    "                               return_train_score=True)\n",
    "\n",
    "      # Adjust predictions using the new threshold\n",
    "      y_pred = (results['test_precision'] > 0.7).astype(int)\n",
    "      y_true = _y\n",
    "      \n",
    "      return {\"Training Accuracy scores\": results['train_accuracy'],\n",
    "              \"Mean Training Accuracy\": results['train_accuracy'].mean()*100,\n",
    "              \"Training Precision scores\": results['train_precision'],\n",
    "              \"Mean Training Precision\": results['train_precision'].mean(),\n",
    "              \"Training Recall scores\": results['train_recall'],\n",
    "              \"Mean Training Recall\": results['train_recall'].mean(),\n",
    "              \"Training F1 scores\": results['train_f1'],\n",
    "              \"Mean Training F1 Score\": results['train_f1'].mean(),\n",
    "              \"Validation Accuracy scores\": results['test_accuracy'],\n",
    "              \"Mean Validation Accuracy\": results['test_accuracy'].mean()*100,\n",
    "              \"Std Validation Accuracy\":results['test_accuracy'].std()*100,\n",
    "              \"Validation Precision scores\": results['test_precision'],\n",
    "              \"Mean Validation Precision\": results['test_precision'].mean(),\n",
    "              \"Validation Recall scores\": results['test_recall'],\n",
    "              \"Mean Validation Recall\": results['test_recall'].mean(),\n",
    "              \"Validation F1 scores\": results['test_f1'],\n",
    "              \"Mean Validation F1 Score\": results['test_f1'].mean()\n",
    "              \n",
    "              }\n",
    "    \n",
    "def plot_result(x_label, y_label, plot_title, train_data, val_data):\n",
    "        '''Function to plot a grouped bar chart showing the training and validation\n",
    "          results of the ML model in each fold after applying K-fold cross-validation.\n",
    "         Parameters\n",
    "         ----------\n",
    "         x_label: str, \n",
    "            Name of the algorithm used for training e.g 'Decision Tree'\n",
    "          \n",
    "         y_label: str, \n",
    "            Name of metric being visualized e.g 'Accuracy'\n",
    "         plot_title: str, \n",
    "            This is the title of the plot e.g 'Accuracy Plot'\n",
    "         \n",
    "         train_result: list, array\n",
    "            This is the list containing either training precision, accuracy, or f1 score.\n",
    "        \n",
    "         val_result: list, array\n",
    "            This is the list containing either validation precision, accuracy, or f1 score.\n",
    "         Returns\n",
    "         -------\n",
    "         The function returns a Grouped Barchart showing the training and validation result\n",
    "         in each fold.\n",
    "        '''\n",
    "        \n",
    "        # Set size of plot\n",
    "        plt.figure(figsize=(12,6))\n",
    "        labels = [\"1st Fold\", \"2nd Fold\", \"3rd Fold\", \"4th Fold\", \"5th Fold\",\"6th Fold\", \"7th Fold\", \"8th Fold\", \"9th Fold\", \"10th Fold\"]\n",
    "        X_axis = np.arange(len(labels))\n",
    "        ax = plt.gca()\n",
    "        plt.ylim(0.40000, 1)\n",
    "        plt.bar(X_axis-0.2, train_data, 0.4, color='blue', label='Training')\n",
    "        plt.bar(X_axis+0.2, val_data, 0.4, color='red', label='Validation')\n",
    "        plt.title(plot_title, fontsize=30)\n",
    "        plt.xticks(X_axis, labels)\n",
    "        plt.xlabel(x_label, fontsize=14)\n",
    "        plt.ylabel(y_label, fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)\n",
    "    \n",
    "# Create the column transformer to apply the transformers to specific columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('skip_568', 'passthrough', slice(0, 568)),  # skip columns 1-500\n",
    "        ('minmax_scaler', MinMaxScaler(), slice(568, 602)),  # apply min-max scaling to columns 501-553\n",
    "        ('skip_46', 'passthrough', slice(602, 829)),  # skip columns 554-600\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        #('undersample', RandomUnderSampler(sampling_strategy=\"majority\")),   \n",
    "        ('model', RandomForestClassifier(max_depth = 10, n_estimators=20)),\n",
    "    ])\n",
    "\n",
    "Feature_columns = DAP_columns + Cluster_columns \n",
    "Prediction_columns = GO_columns\n",
    "len(Feature_columns), len(Feature_columns + Prediction_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08994c5b-0bd9-49ec-81d7-f494c26fe6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>33</th>\n",
       "      <th>GO:0000139</th>\n",
       "      <th>GO:0000166</th>\n",
       "      <th>GO:0000287</th>\n",
       "      <th>GO:0000325</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gene ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT4G13263</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT2G17770</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            33  GO:0000139  GO:0000166  GO:0000287  GO:0000325\n",
       "Gene ID                                                       \n",
       "AT4G13263  0.0           0           0           0           0\n",
       "AT2G17770  0.0           0           0           0           0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_names = Root33_data_setA.iloc[:,len(Feature_columns) :].columns\n",
    "feature_names = Root33_data_setA.iloc[:, :len(Feature_columns)-1 ].columns\n",
    "Root33_data_setA.iloc[3:5, 600:605]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175bbc9b-2574-43b3-9d86-b7709b17de13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "829"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Root33_data_setA.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147f462-5f5b-47e1-88e6-f69c4c0fc19c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12015bfc-1063-401d-b3b7-40a1881e6d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m y_sample \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m     18\u001b[0m goterm \u001b[38;5;241m=\u001b[39m column_headers[i]\n\u001b[0;32m---> 19\u001b[0m Root33_results_dict[goterm] \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mcross_validation\u001b[0;34m(model, _X, _y, _cv)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Function to perform 5 Folds Cross-Validation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m Parameters\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m 'recall', 'f1' for both training set and validation set.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     18\u001b[0m _scoring \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 19\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                         \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_scoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Accuracy scores\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Training Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Precision scores\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_precision\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \n\u001b[1;32m     44\u001b[0m         }\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    465\u001b[0m ]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    182\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 184\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "Root33_Go_Mean_Accuracy=[]\n",
    "Root33_Go_Std_Accuracy=[]\n",
    "Root33_Go_10cross_Accuracies=[]\n",
    "Random_forest_result_pred_list = []\n",
    "Root33_results_dict = {}\n",
    "column_headers = list(Root33_data_setA.columns)\n",
    "\n",
    "X=Root33_data_setA.iloc[:, :len(Feature_columns) ]\n",
    "\n",
    "for i in range(len(Feature_columns), Root33_data_setA.shape[1]):\n",
    "    y=Root33_data_setA.iloc[:,i]\n",
    "    \n",
    "    # X_sample, y_sample = undersample.fit_resample(X, y)\n",
    "    X_sample = X\n",
    "    y_sample = y\n",
    "    goterm = column_headers[i]\n",
    "    Root33_results_dict[goterm] = cross_validation(pipe,X_sample, y_sample, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc2630d8-eff4-4a58-b476-c1f0614ba189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b24d70a-dfd1-4e5b-a020-777626b6c1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy scores</th>\n",
       "      <th>Mean Training Accuracy</th>\n",
       "      <th>Training Precision scores</th>\n",
       "      <th>Mean Training Precision</th>\n",
       "      <th>Training Recall scores</th>\n",
       "      <th>Mean Training Recall</th>\n",
       "      <th>Training F1 scores</th>\n",
       "      <th>Mean Training F1 Score</th>\n",
       "      <th>Validation Accuracy scores</th>\n",
       "      <th>Mean Validation Accuracy</th>\n",
       "      <th>Std Validation Accuracy</th>\n",
       "      <th>Validation Precision scores</th>\n",
       "      <th>Mean Validation Precision</th>\n",
       "      <th>Validation Recall scores</th>\n",
       "      <th>Mean Validation Recall</th>\n",
       "      <th>Validation F1 scores</th>\n",
       "      <th>Mean Validation F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0000166</th>\n",
       "      <td>[0.911616786214857, 0.913018385687196, 0.9119465743259956, 0.9139253029928271, 0.9106274218814412, 0.911616786214857, 0.9103800807980872, 0.9122011541632317, 0.910717230008244, 0.912366034624897]</td>\n",
       "      <td>91.184158</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.06944444444444445, 0.08499566348655681, 0.07372072853425846, 0.09453599306157849, 0.0598438855160451, 0.07025151777970512, 0.05724197745013009, 0.07632263660017347, 0.060711188204683436, 0.0780...</td>\n",
       "      <td>0.072513</td>\n",
       "      <td>[0.12987012987012989, 0.1566746602717826, 0.13731825525040386, 0.17274167987321712, 0.11292962356792145, 0.1312803889789303, 0.10828547990155865, 0.14182111200644643, 0.11447260834014719, 0.144810...</td>\n",
       "      <td>0.13502</td>\n",
       "      <td>[0.9043026706231454, 0.9050445103857567, 0.9050445103857567, 0.9050445103857567, 0.9050445103857567, 0.9050445103857567, 0.9050445103857567, 0.9049740163325909, 0.9042316258351893, 0.9049740163325...</td>\n",
       "      <td>90.487494</td>\n",
       "      <td>0.030553</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000287</th>\n",
       "      <td>[0.996702118888614, 0.9962074367219061, 0.9963723307774754, 0.9961249896941216, 0.9963723307774754, 0.9960425426663368, 0.9962898837496909, 0.9965375103050289, 0.9958779884583677, 0.9966199505358615]</td>\n",
       "      <td>99.631471</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.4444444444444444, 0.3611111111111111, 0.3888888888888889, 0.3472222222222222, 0.3888888888888889, 0.3333333333333333, 0.375, 0.4166666666666667, 0.3055555555555556, 0.4305555555555556]</td>\n",
       "      <td>0.379167</td>\n",
       "      <td>[0.6153846153846153, 0.5306122448979591, 0.56, 0.5154639175257731, 0.56, 0.5, 0.5454545454545454, 0.5882352941176471, 0.46808510638297873, 0.6019417475728156]</td>\n",
       "      <td>0.548518</td>\n",
       "      <td>[0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.994060876020787, 0.994060876020787, 0.994060876020787]</td>\n",
       "      <td>99.406396</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000325</th>\n",
       "      <td>[0.9803776073872537, 0.9807073954983923, 0.9794706900816226, 0.980542501442823, 0.9812020776651001, 0.9806249484706077, 0.981696759831808, 0.9807914262159934, 0.9806265457543281, 0.9804616652926628]</td>\n",
       "      <td>98.065016</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.2347266881028939, 0.24758842443729903, 0.1967741935483871, 0.23870967741935484, 0.2645161290322581, 0.24193548387096775, 0.2838709677419355, 0.2508038585209003, 0.24437299035369775, 0.237942122...</td>\n",
       "      <td>0.244124</td>\n",
       "      <td>[0.3802083333333333, 0.39690721649484534, 0.3288409703504043, 0.38541666666666663, 0.4183673469387755, 0.38961038961038963, 0.4422110552763819, 0.40102827763496146, 0.39276485788113696, 0.38441558...</td>\n",
       "      <td>0.391977</td>\n",
       "      <td>[0.9747774480712166, 0.9747774480712166, 0.9747774480712166, 0.9747774480712166, 0.9740356083086054, 0.973293768545994, 0.973293768545994, 0.9762435040831478, 0.9732739420935412, 0.9747587230883444]</td>\n",
       "      <td>97.440091</td>\n",
       "      <td>0.089137</td>\n",
       "      <td>[0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>0.35</td>\n",
       "      <td>[0.0, 0.029411764705882353, 0.02857142857142857, 0.02857142857142857, 0.0, 0.0, 0.0, 0.058823529411764705, 0.0, 0.0]</td>\n",
       "      <td>0.014538</td>\n",
       "      <td>[0.0, 0.05555555555555555, 0.05555555555555556, 0.05555555555555556, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0]</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000398</th>\n",
       "      <td>[0.9957127545551983, 0.9958776486107676, 0.9956303075274137, 0.9960425426663368, 0.995795201582983, 0.9956303075274137, 0.9957127545551983, 0.9957131079967024, 0.9956306677658697, 0.9949711459192085]</td>\n",
       "      <td>99.567164</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.30666666666666664, 0.3333333333333333, 0.29333333333333333, 0.36, 0.3108108108108108, 0.28378378378378377, 0.2972972972972973, 0.30666666666666664, 0.29333333333333333, 0.18666666666666668]</td>\n",
       "      <td>0.297189</td>\n",
       "      <td>[0.4693877551020408, 0.5, 0.45360824742268036, 0.5294117647058824, 0.4742268041237113, 0.4421052631578947, 0.4583333333333333, 0.4693877551020408, 0.45360824742268036, 0.3146067415730337]</td>\n",
       "      <td>0.456468</td>\n",
       "      <td>[0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9933234421364985, 0.9933234421364985, 0.9933234421364985, 0.994060876020787, 0.994060876020787, 0.994060876020787]</td>\n",
       "      <td>99.384141</td>\n",
       "      <td>0.033909</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000976</th>\n",
       "      <td>[0.9808722895539616, 0.9798829252205458, 0.9795531371094072, 0.9794706900816226, 0.9795531371094072, 0.981944100915162, 0.9808722895539616, 0.9804616652926628, 0.9803792250618302, 0.9804616652926628]</td>\n",
       "      <td>98.034511</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.3074626865671642, 0.2716417910447761, 0.25970149253731345, 0.25671641791044775, 0.25970149253731345, 0.344311377245509, 0.30538922155688625, 0.29253731343283584, 0.28955223880597014, 0.29253731...</td>\n",
       "      <td>0.287955</td>\n",
       "      <td>[0.4703196347031964, 0.42723004694835676, 0.4123222748815166, 0.4085510688836104, 0.4123222748815166, 0.512249443207127, 0.46788990825688076, 0.45265588914549654, 0.44907407407407407, 0.4526558891...</td>\n",
       "      <td>0.446527</td>\n",
       "      <td>[0.9725519287833828, 0.9725519287833828, 0.9725519287833828, 0.9725519287833828, 0.9725519287833828, 0.9718100890207715, 0.9718100890207715, 0.9725315515961396, 0.9725315515961396, 0.9725315515961...</td>\n",
       "      <td>97.239745</td>\n",
       "      <td>0.029381</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                           Training Accuracy scores   \n",
       "GO:0000166      [0.911616786214857, 0.913018385687196, 0.9119465743259956, 0.9139253029928271, 0.9106274218814412, 0.911616786214857, 0.9103800807980872, 0.9122011541632317, 0.910717230008244, 0.912366034624897]  \\\n",
       "GO:0000287  [0.996702118888614, 0.9962074367219061, 0.9963723307774754, 0.9961249896941216, 0.9963723307774754, 0.9960425426663368, 0.9962898837496909, 0.9965375103050289, 0.9958779884583677, 0.9966199505358615]   \n",
       "GO:0000325   [0.9803776073872537, 0.9807073954983923, 0.9794706900816226, 0.980542501442823, 0.9812020776651001, 0.9806249484706077, 0.981696759831808, 0.9807914262159934, 0.9806265457543281, 0.9804616652926628]   \n",
       "GO:0000398  [0.9957127545551983, 0.9958776486107676, 0.9956303075274137, 0.9960425426663368, 0.995795201582983, 0.9956303075274137, 0.9957127545551983, 0.9957131079967024, 0.9956306677658697, 0.9949711459192085]   \n",
       "GO:0000976  [0.9808722895539616, 0.9798829252205458, 0.9795531371094072, 0.9794706900816226, 0.9795531371094072, 0.981944100915162, 0.9808722895539616, 0.9804616652926628, 0.9803792250618302, 0.9804616652926628]   \n",
       "\n",
       "           Mean Training Accuracy   \n",
       "GO:0000166              91.184158  \\\n",
       "GO:0000287              99.631471   \n",
       "GO:0000325              98.065016   \n",
       "GO:0000398              99.567164   \n",
       "GO:0000976              98.034511   \n",
       "\n",
       "                                     Training Precision scores   \n",
       "GO:0000166  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \\\n",
       "GO:0000287  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000325  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000398  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000976  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "\n",
       "           Mean Training Precision   \n",
       "GO:0000166                     1.0  \\\n",
       "GO:0000287                     1.0   \n",
       "GO:0000325                     1.0   \n",
       "GO:0000398                     1.0   \n",
       "GO:0000976                     1.0   \n",
       "\n",
       "                                                                                                                                                                                             Training Recall scores   \n",
       "GO:0000166  [0.06944444444444445, 0.08499566348655681, 0.07372072853425846, 0.09453599306157849, 0.0598438855160451, 0.07025151777970512, 0.05724197745013009, 0.07632263660017347, 0.060711188204683436, 0.0780...  \\\n",
       "GO:0000287              [0.4444444444444444, 0.3611111111111111, 0.3888888888888889, 0.3472222222222222, 0.3888888888888889, 0.3333333333333333, 0.375, 0.4166666666666667, 0.3055555555555556, 0.4305555555555556]   \n",
       "GO:0000325  [0.2347266881028939, 0.24758842443729903, 0.1967741935483871, 0.23870967741935484, 0.2645161290322581, 0.24193548387096775, 0.2838709677419355, 0.2508038585209003, 0.24437299035369775, 0.237942122...   \n",
       "GO:0000398         [0.30666666666666664, 0.3333333333333333, 0.29333333333333333, 0.36, 0.3108108108108108, 0.28378378378378377, 0.2972972972972973, 0.30666666666666664, 0.29333333333333333, 0.18666666666666668]   \n",
       "GO:0000976  [0.3074626865671642, 0.2716417910447761, 0.25970149253731345, 0.25671641791044775, 0.25970149253731345, 0.344311377245509, 0.30538922155688625, 0.29253731343283584, 0.28955223880597014, 0.29253731...   \n",
       "\n",
       "           Mean Training Recall   \n",
       "GO:0000166             0.072513  \\\n",
       "GO:0000287             0.379167   \n",
       "GO:0000325             0.244124   \n",
       "GO:0000398             0.297189   \n",
       "GO:0000976             0.287955   \n",
       "\n",
       "                                                                                                                                                                                                 Training F1 scores   \n",
       "GO:0000166  [0.12987012987012989, 0.1566746602717826, 0.13731825525040386, 0.17274167987321712, 0.11292962356792145, 0.1312803889789303, 0.10828547990155865, 0.14182111200644643, 0.11447260834014719, 0.144810...  \\\n",
       "GO:0000287                                           [0.6153846153846153, 0.5306122448979591, 0.56, 0.5154639175257731, 0.56, 0.5, 0.5454545454545454, 0.5882352941176471, 0.46808510638297873, 0.6019417475728156]   \n",
       "GO:0000325  [0.3802083333333333, 0.39690721649484534, 0.3288409703504043, 0.38541666666666663, 0.4183673469387755, 0.38961038961038963, 0.4422110552763819, 0.40102827763496146, 0.39276485788113696, 0.38441558...   \n",
       "GO:0000398              [0.4693877551020408, 0.5, 0.45360824742268036, 0.5294117647058824, 0.4742268041237113, 0.4421052631578947, 0.4583333333333333, 0.4693877551020408, 0.45360824742268036, 0.3146067415730337]   \n",
       "GO:0000976  [0.4703196347031964, 0.42723004694835676, 0.4123222748815166, 0.4085510688836104, 0.4123222748815166, 0.512249443207127, 0.46788990825688076, 0.45265588914549654, 0.44907407407407407, 0.4526558891...   \n",
       "\n",
       "           Mean Training F1 Score   \n",
       "GO:0000166                0.13502  \\\n",
       "GO:0000287               0.548518   \n",
       "GO:0000325               0.391977   \n",
       "GO:0000398               0.456468   \n",
       "GO:0000976               0.446527   \n",
       "\n",
       "                                                                                                                                                                                         Validation Accuracy scores   \n",
       "GO:0000166  [0.9043026706231454, 0.9050445103857567, 0.9050445103857567, 0.9050445103857567, 0.9050445103857567, 0.9050445103857567, 0.9050445103857567, 0.9049740163325909, 0.9042316258351893, 0.9049740163325...  \\\n",
       "GO:0000287    [0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.994060876020787, 0.994060876020787, 0.994060876020787]   \n",
       "GO:0000325   [0.9747774480712166, 0.9747774480712166, 0.9747774480712166, 0.9747774480712166, 0.9740356083086054, 0.973293768545994, 0.973293768545994, 0.9762435040831478, 0.9732739420935412, 0.9747587230883444]   \n",
       "GO:0000398    [0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9940652818991098, 0.9933234421364985, 0.9933234421364985, 0.9933234421364985, 0.994060876020787, 0.994060876020787, 0.994060876020787]   \n",
       "GO:0000976  [0.9725519287833828, 0.9725519287833828, 0.9725519287833828, 0.9725519287833828, 0.9725519287833828, 0.9718100890207715, 0.9718100890207715, 0.9725315515961396, 0.9725315515961396, 0.9725315515961...   \n",
       "\n",
       "           Mean Validation Accuracy Std Validation Accuracy   \n",
       "GO:0000166                90.487494                0.030553  \\\n",
       "GO:0000287                99.406396                0.000202   \n",
       "GO:0000325                97.440091                0.089137   \n",
       "GO:0000398                99.384141                0.033909   \n",
       "GO:0000976                97.239745                0.029381   \n",
       "\n",
       "                                   Validation Precision scores   \n",
       "GO:0000166  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \\\n",
       "GO:0000287  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "GO:0000325  [0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]   \n",
       "GO:0000398  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "GO:0000976  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "           Mean Validation Precision   \n",
       "GO:0000166                       0.0  \\\n",
       "GO:0000287                       0.0   \n",
       "GO:0000325                      0.35   \n",
       "GO:0000398                       0.0   \n",
       "GO:0000976                       0.0   \n",
       "\n",
       "                                                                                                        Validation Recall scores   \n",
       "GO:0000166                                                                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \\\n",
       "GO:0000287                                                                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "GO:0000325  [0.0, 0.029411764705882353, 0.02857142857142857, 0.02857142857142857, 0.0, 0.0, 0.0, 0.058823529411764705, 0.0, 0.0]   \n",
       "GO:0000398                                                                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "GO:0000976                                                                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "           Mean Validation Recall   \n",
       "GO:0000166                    0.0  \\\n",
       "GO:0000287                    0.0   \n",
       "GO:0000325               0.014538   \n",
       "GO:0000398                    0.0   \n",
       "GO:0000976                    0.0   \n",
       "\n",
       "                                                                                                         Validation F1 scores   \n",
       "GO:0000166                                                                 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \\\n",
       "GO:0000287                                                                 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "GO:0000325  [0.0, 0.05555555555555555, 0.05555555555555556, 0.05555555555555556, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0]   \n",
       "GO:0000398                                                                 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "GO:0000976                                                                 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "           Mean Validation F1 Score  \n",
       "GO:0000166                      0.0  \n",
       "GO:0000287                      0.0  \n",
       "GO:0000325                 0.027778  \n",
       "GO:0000398                      0.0  \n",
       "GO:0000976                      0.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Root33_results_dict).T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f1d1af8-cd0e-46bc-9168-2fa04d3863c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Training Accuracy scores', 'Mean Training Accuracy',\n",
       "       'Training Precision scores', 'Mean Training Precision',\n",
       "       'Training Recall scores', 'Mean Training Recall', 'Training F1 scores',\n",
       "       'Mean Training F1 Score', 'Validation Accuracy scores',\n",
       "       'Mean Validation Accuracy', 'Std Validation Accuracy',\n",
       "       'Validation Precision scores', 'Mean Validation Precision',\n",
       "       'Validation Recall scores', 'Mean Validation Recall',\n",
       "       'Validation F1 scores', 'Mean Validation F1 Score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Root33_results_df = pd.DataFrame(Root33_results_dict).T\n",
    "Root33_results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85d8e8d6-6472-447d-9721-db4c44c6a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_keep = ['Mean Training Accuracy', 'Mean Training F1 Score', 'Mean Validation Accuracy', 'Std Validation Accuracy','Mean Validation Precision', 'Mean Validation Recall','Mean Validation F1 Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59749b1c-f8aa-438a-8fcf-bb9c14dacdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Root33_results_df[results_to_keep].to_csv(\"12Dec2023_DAPSeq_Root33_results_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b836fd63-cdf9-4da6-a070-29a791717266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GO:0000166     0.13502\n",
       "GO:0000287    0.548518\n",
       "GO:0000325    0.391977\n",
       "GO:0000398    0.456468\n",
       "GO:0000976    0.446527\n",
       "Name: Mean Training F1 Score, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Root33_results_df[\"Mean Training F1 Score\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6fcd69a-d927-4f65-8f6c-ae9250355aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_gos_list = Root33_results_df.loc[Root33_results_df['Mean Validation Accuracy'] >= 75].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb99e35a-09c0-4c0a-af38-abb6fe8daf24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GO:0000166',\n",
       " 'GO:0000287',\n",
       " 'GO:0000325',\n",
       " 'GO:0000398',\n",
       " 'GO:0000976',\n",
       " 'GO:0000978',\n",
       " 'GO:0000981',\n",
       " 'GO:0003674',\n",
       " 'GO:0003676',\n",
       " 'GO:0003677',\n",
       " 'GO:0003682',\n",
       " 'GO:0003690',\n",
       " 'GO:0003700',\n",
       " 'GO:0003723',\n",
       " 'GO:0003729',\n",
       " 'GO:0003735',\n",
       " 'GO:0003743',\n",
       " 'GO:0003779',\n",
       " 'GO:0003824',\n",
       " 'GO:0003924',\n",
       " 'GO:0004252',\n",
       " 'GO:0004386',\n",
       " 'GO:0004497',\n",
       " 'GO:0004518',\n",
       " 'GO:0004519',\n",
       " 'GO:0004553',\n",
       " 'GO:0004601',\n",
       " 'GO:0004672',\n",
       " 'GO:0004674',\n",
       " 'GO:0004712',\n",
       " 'GO:0004721',\n",
       " 'GO:0004722',\n",
       " 'GO:0004842',\n",
       " 'GO:0004857',\n",
       " 'GO:0005506',\n",
       " 'GO:0005507',\n",
       " 'GO:0005509',\n",
       " 'GO:0005515',\n",
       " 'GO:0005516',\n",
       " 'GO:0005524',\n",
       " 'GO:0005525',\n",
       " 'GO:0005575',\n",
       " 'GO:0005576',\n",
       " 'GO:0005615',\n",
       " 'GO:0005634',\n",
       " 'GO:0005654',\n",
       " 'GO:0005681',\n",
       " 'GO:0005694',\n",
       " 'GO:0005730',\n",
       " 'GO:0005737',\n",
       " 'GO:0005739',\n",
       " 'GO:0005743',\n",
       " 'GO:0005759',\n",
       " 'GO:0005768',\n",
       " 'GO:0005773',\n",
       " 'GO:0005774',\n",
       " 'GO:0005777',\n",
       " 'GO:0005783',\n",
       " 'GO:0005789',\n",
       " 'GO:0005794',\n",
       " 'GO:0005802',\n",
       " 'GO:0005829',\n",
       " 'GO:0005840',\n",
       " 'GO:0005856',\n",
       " 'GO:0005874',\n",
       " 'GO:0005886',\n",
       " 'GO:0005887',\n",
       " 'GO:0005975',\n",
       " 'GO:0006260',\n",
       " 'GO:0006281',\n",
       " 'GO:0006325',\n",
       " 'GO:0006351',\n",
       " 'GO:0006355',\n",
       " 'GO:0006357',\n",
       " 'GO:0006364',\n",
       " 'GO:0006396',\n",
       " 'GO:0006397',\n",
       " 'GO:0006412',\n",
       " 'GO:0006413',\n",
       " 'GO:0006457',\n",
       " 'GO:0006468',\n",
       " 'GO:0006486',\n",
       " 'GO:0006508',\n",
       " 'GO:0006511',\n",
       " 'GO:0006629',\n",
       " 'GO:0006631',\n",
       " 'GO:0006633',\n",
       " 'GO:0006811',\n",
       " 'GO:0006869',\n",
       " 'GO:0006886',\n",
       " 'GO:0006952',\n",
       " 'GO:0006970',\n",
       " 'GO:0006974',\n",
       " 'GO:0006979',\n",
       " 'GO:0007049',\n",
       " 'GO:0007165',\n",
       " 'GO:0007623',\n",
       " 'GO:0008017',\n",
       " 'GO:0008150',\n",
       " 'GO:0008152',\n",
       " 'GO:0008168',\n",
       " 'GO:0008194',\n",
       " 'GO:0008233',\n",
       " 'GO:0008234',\n",
       " 'GO:0008236',\n",
       " 'GO:0008270',\n",
       " 'GO:0008289',\n",
       " 'GO:0008380',\n",
       " 'GO:0008643',\n",
       " 'GO:0008652',\n",
       " 'GO:0009058',\n",
       " 'GO:0009408',\n",
       " 'GO:0009409',\n",
       " 'GO:0009414',\n",
       " 'GO:0009416',\n",
       " 'GO:0009451',\n",
       " 'GO:0009505',\n",
       " 'GO:0009506',\n",
       " 'GO:0009507',\n",
       " 'GO:0009524',\n",
       " 'GO:0009534',\n",
       " 'GO:0009535',\n",
       " 'GO:0009536',\n",
       " 'GO:0009555',\n",
       " 'GO:0009570',\n",
       " 'GO:0009579',\n",
       " 'GO:0009611',\n",
       " 'GO:0009617',\n",
       " 'GO:0009651',\n",
       " 'GO:0009658',\n",
       " 'GO:0009705',\n",
       " 'GO:0009733',\n",
       " 'GO:0009734',\n",
       " 'GO:0009737',\n",
       " 'GO:0009738',\n",
       " 'GO:0009751',\n",
       " 'GO:0009753',\n",
       " 'GO:0009793',\n",
       " 'GO:0009860',\n",
       " 'GO:0009873',\n",
       " 'GO:0009908',\n",
       " 'GO:0009941',\n",
       " 'GO:0010008',\n",
       " 'GO:0010150',\n",
       " 'GO:0010228',\n",
       " 'GO:0010468',\n",
       " 'GO:0012505',\n",
       " 'GO:0015031',\n",
       " 'GO:0015297',\n",
       " 'GO:0015979',\n",
       " 'GO:0016021',\n",
       " 'GO:0016042',\n",
       " 'GO:0016192',\n",
       " 'GO:0016301',\n",
       " 'GO:0016310',\n",
       " 'GO:0016491',\n",
       " 'GO:0016567',\n",
       " 'GO:0016616',\n",
       " 'GO:0016705',\n",
       " 'GO:0016740',\n",
       " 'GO:0016746',\n",
       " 'GO:0016757',\n",
       " 'GO:0016779',\n",
       " 'GO:0016787',\n",
       " 'GO:0016788',\n",
       " 'GO:0016798',\n",
       " 'GO:0016829',\n",
       " 'GO:0016853',\n",
       " 'GO:0016874',\n",
       " 'GO:0016887',\n",
       " 'GO:0017018',\n",
       " 'GO:0018105',\n",
       " 'GO:0020037',\n",
       " 'GO:0022625',\n",
       " 'GO:0022626',\n",
       " 'GO:0022857',\n",
       " 'GO:0030154',\n",
       " 'GO:0030246',\n",
       " 'GO:0031225',\n",
       " 'GO:0031410',\n",
       " 'GO:0031640',\n",
       " 'GO:0031969',\n",
       " 'GO:0032259',\n",
       " 'GO:0034220',\n",
       " 'GO:0035556',\n",
       " 'GO:0040008',\n",
       " 'GO:0042254',\n",
       " 'GO:0042742',\n",
       " 'GO:0042802',\n",
       " 'GO:0042803',\n",
       " 'GO:0043086',\n",
       " 'GO:0043231',\n",
       " 'GO:0043531',\n",
       " 'GO:0043565',\n",
       " 'GO:0043621',\n",
       " 'GO:0045087',\n",
       " 'GO:0045892',\n",
       " 'GO:0045893',\n",
       " 'GO:0045944',\n",
       " 'GO:0046658',\n",
       " 'GO:0046777',\n",
       " 'GO:0046872',\n",
       " 'GO:0046982',\n",
       " 'GO:0046983',\n",
       " 'GO:0048046',\n",
       " 'GO:0048364',\n",
       " 'GO:0048366',\n",
       " 'GO:0050660',\n",
       " 'GO:0050790',\n",
       " 'GO:0050832',\n",
       " 'GO:0050896',\n",
       " 'GO:0051082',\n",
       " 'GO:0051213',\n",
       " 'GO:0051301',\n",
       " 'GO:0051536',\n",
       " 'GO:0055085',\n",
       " 'GO:0061630',\n",
       " 'GO:0071456',\n",
       " 'GO:0071555',\n",
       " 'GO:0071704',\n",
       " 'GO:0090305',\n",
       " 'GO:0090502',\n",
       " 'GO:0098869',\n",
       " 'GO:0099503',\n",
       " 'GO:0106310',\n",
       " 'GO:0110165']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_gos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70084822-1f38-4ad4-9c92-01afbdf324f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GO:0003735': array([1, 0, 0, ..., 0, 0, 0]),\n",
       " 'GO:0005840': array([0, 0, 0, ..., 0, 0, 0]),\n",
       " 'GO:0006260': array([0, 0, 0, ..., 0, 0, 0]),\n",
       " 'GO:0006412': array([1, 0, 0, ..., 0, 0, 0]),\n",
       " 'GO:0022625': array([1, 0, 0, ..., 0, 0, 0]),\n",
       " 'GO:0022626': array([0, 0, 0, ..., 0, 0, 0]),\n",
       " 'GO:0031640': array([0, 0, 0, ..., 0, 1, 1]),\n",
       " 'GO:0042254': array([0, 0, 0, ..., 0, 0, 0])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training on set A and testing to setB\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "Root33_pred_Mean_Accuracy=[]\n",
    "Root33_pred_Std_Accuracy=[]\n",
    "Root33_pred_10cross_Accuracies=[]\n",
    "Random_forest_result_pred_list2 = []\n",
    "Root33_results_dict3 = {}\n",
    "predictions_dict_Root33 = {}\n",
    "column_headers = list(Root33_data_setA.columns)\n",
    "\n",
    "pipe2 = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\"classifier\", RandomForestClassifier(max_depth = 10, n_estimators=20)),\n",
    "    ]\n",
    ")\n",
    "X=Root33_data_setA.iloc[:, :len(Feature_columns)-1 ]\n",
    "Xb=Root33_data_setB.iloc[:, :len(Feature_columns)-1 ]\n",
    "new_threshold = 0.7\n",
    "for i in selected_gos_list:\n",
    "    y=Root33_data_setA[i]\n",
    "    X_sample, y_sample = undersample.fit_resample(X, y)\n",
    "    #goterm = column_headers[i]\n",
    "    yb=Root33_data_setB[i]\n",
    "    X_test, y_test = (Xb, yb)\n",
    "    #goterm.append(column_headers[i])\n",
    "    \n",
    "    #Random_forest_result = cross_validation(pipe,X_sample, y_sample, 5)\n",
    "    #Root33_pred_Mean_Accuracy.append(Random_forest_result[\"Mean Validation Accuracy\"])\n",
    "    #Root33_pred_Std_Accuracy.append(Random_forest_result[\"Std Validation Accuracy\"])\n",
    "    #Root33_pred_10cross_Accuracies.append(Random_forest_result[\"Validation Accuracy scores\"])\n",
    "    #Random_forest_result_pred = cross_val_predict(pipe, X_sample, y_sample, cv=5, method='predict_proba')\n",
    "    #Random_forest_result_pred_list2.append(Random_forest_result_pred)\n",
    "    pipe2.fit(X_sample, y_sample)\n",
    "    Root33_results_dict3[i] = pipe2.predict(X_test)\n",
    "    predictor = pipe2.fit(X_sample, y_sample)\n",
    "    predictions_dict_Root33[i] = (predictor.predict_proba(X_test)[:,1] > new_threshold).astype(int)\n",
    "\n",
    "predictions_dict_Root33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f1b9ab3-af85-4e35-8802-61012a920c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GO:0003735</th>\n",
       "      <th>GO:0005840</th>\n",
       "      <th>GO:0006260</th>\n",
       "      <th>GO:0006412</th>\n",
       "      <th>GO:0022625</th>\n",
       "      <th>GO:0022626</th>\n",
       "      <th>GO:0031640</th>\n",
       "      <th>GO:0042254</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT1G65220</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT3G59690</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT4G25830</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT3G58960</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT2G05590</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT5G23380</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT5G09900</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT4G10000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT3G05975</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT5G45960</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13478 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           GO:0003735  GO:0005840  GO:0006260  GO:0006412  GO:0022625   \n",
       "AT1G65220           1           1           0           1           1  \\\n",
       "AT3G59690           1           0           0           0           0   \n",
       "AT4G25830           0           0           0           0           0   \n",
       "AT3G58960           0           0           0           0           0   \n",
       "AT2G05590           0           0           0           1           0   \n",
       "...               ...         ...         ...         ...         ...   \n",
       "AT5G23380           0           0           0           0           0   \n",
       "AT5G09900           1           1           0           0           1   \n",
       "AT4G10000           0           0           1           0           0   \n",
       "AT3G05975           0           0           0           0           0   \n",
       "AT5G45960           0           0           0           0           0   \n",
       "\n",
       "           GO:0022626  GO:0031640  GO:0042254  \n",
       "AT1G65220           1           0           0  \n",
       "AT3G59690           0           0           1  \n",
       "AT4G25830           0           0           0  \n",
       "AT3G58960           0           1           0  \n",
       "AT2G05590           0           0           0  \n",
       "...               ...         ...         ...  \n",
       "AT5G23380           0           0           0  \n",
       "AT5G09900           1           0           1  \n",
       "AT4G10000           0           0           0  \n",
       "AT3G05975           0           1           0  \n",
       "AT5G45960           0           1           0  \n",
       "\n",
       "[13478 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes = Root33_data_setB.index.tolist()\n",
    " \n",
    "novel_predictions_df = pd.DataFrame(Root33_results_dict3, index = genes)\n",
    "novel_predictions_thresh = pd.DataFrame(predictions_dict_Root33, index = genes)\n",
    "\n",
    "novel_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80a31164-2e57-4a8c-870f-78035e462e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT3G12130_colamp_a_AT3G12130</th>\n",
       "      <th>AT5G63260_col_a_AT5G63260</th>\n",
       "      <th>CDM1_colamp_a_AT1G68200</th>\n",
       "      <th>At5g08750_col_a_AT5G08750</th>\n",
       "      <th>AT5G63260_colamp_a_AT5G63260</th>\n",
       "      <th>FRS9_col_a_AT4G38170</th>\n",
       "      <th>FRS9_colamp_a_AT4G38170</th>\n",
       "      <th>HAT2_col_v31_AT5G47370</th>\n",
       "      <th>AT4G00250_col_a_AT4G00250</th>\n",
       "      <th>ARF2_col_v31_AT5G62000</th>\n",
       "      <th>...</th>\n",
       "      <th>GO:0061630</th>\n",
       "      <th>GO:0071456</th>\n",
       "      <th>GO:0071555</th>\n",
       "      <th>GO:0071704</th>\n",
       "      <th>GO:0090305</th>\n",
       "      <th>GO:0090502</th>\n",
       "      <th>GO:0098869</th>\n",
       "      <th>GO:0099503</th>\n",
       "      <th>GO:0106310</th>\n",
       "      <th>GO:0110165</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gene ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT1G01010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT1G01020</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT1G01030</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT1G01040</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT1G01050</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT5G67600</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT5G67610</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT5G67620</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT5G67630</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT5G67640</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26955 rows Ã— 829 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT3G12130_colamp_a_AT3G12130  AT5G63260_col_a_AT5G63260   \n",
       "Gene ID                                                              \n",
       "AT1G01010                             0                          0  \\\n",
       "AT1G01020                             0                          0   \n",
       "AT1G01030                             0                          1   \n",
       "AT1G01040                             1                          1   \n",
       "AT1G01050                             1                          0   \n",
       "...                                 ...                        ...   \n",
       "AT5G67600                             1                          1   \n",
       "AT5G67610                             0                          0   \n",
       "AT5G67620                             0                          0   \n",
       "AT5G67630                             0                          1   \n",
       "AT5G67640                             0                          1   \n",
       "\n",
       "           CDM1_colamp_a_AT1G68200  At5g08750_col_a_AT5G08750   \n",
       "Gene ID                                                         \n",
       "AT1G01010                        0                          0  \\\n",
       "AT1G01020                        0                          0   \n",
       "AT1G01030                        0                          0   \n",
       "AT1G01040                        0                          0   \n",
       "AT1G01050                        1                          0   \n",
       "...                            ...                        ...   \n",
       "AT5G67600                        0                          0   \n",
       "AT5G67610                        0                          0   \n",
       "AT5G67620                        0                          0   \n",
       "AT5G67630                        0                          0   \n",
       "AT5G67640                        0                          0   \n",
       "\n",
       "           AT5G63260_colamp_a_AT5G63260  FRS9_col_a_AT4G38170   \n",
       "Gene ID                                                         \n",
       "AT1G01010                             1                     0  \\\n",
       "AT1G01020                             1                     1   \n",
       "AT1G01030                             1                     1   \n",
       "AT1G01040                             0                     0   \n",
       "AT1G01050                             0                     0   \n",
       "...                                 ...                   ...   \n",
       "AT5G67600                             1                     0   \n",
       "AT5G67610                             0                     0   \n",
       "AT5G67620                             0                     0   \n",
       "AT5G67630                             0                     0   \n",
       "AT5G67640                             1                     0   \n",
       "\n",
       "           FRS9_colamp_a_AT4G38170  HAT2_col_v31_AT5G47370   \n",
       "Gene ID                                                      \n",
       "AT1G01010                        0                       0  \\\n",
       "AT1G01020                        0                       1   \n",
       "AT1G01030                        0                       1   \n",
       "AT1G01040                        0                       1   \n",
       "AT1G01050                        0                       0   \n",
       "...                            ...                     ...   \n",
       "AT5G67600                        0                       0   \n",
       "AT5G67610                        0                       1   \n",
       "AT5G67620                        0                       1   \n",
       "AT5G67630                        0                       0   \n",
       "AT5G67640                        0                       0   \n",
       "\n",
       "           AT4G00250_col_a_AT4G00250  ARF2_col_v31_AT5G62000  ...  GO:0061630   \n",
       "Gene ID                                                       ...               \n",
       "AT1G01010                          0                       0  ...           0  \\\n",
       "AT1G01020                          1                       0  ...           0   \n",
       "AT1G01030                          1                       1  ...           0   \n",
       "AT1G01040                          0                       1  ...           0   \n",
       "AT1G01050                          0                       0  ...           0   \n",
       "...                              ...                     ...  ...         ...   \n",
       "AT5G67600                          0                       0  ...           0   \n",
       "AT5G67610                          1                       0  ...           0   \n",
       "AT5G67620                          1                       0  ...           0   \n",
       "AT5G67630                          0                       0  ...           0   \n",
       "AT5G67640                          0                       1  ...           0   \n",
       "\n",
       "           GO:0071456  GO:0071555  GO:0071704  GO:0090305  GO:0090502   \n",
       "Gene ID                                                                 \n",
       "AT1G01010           0           0           0           0           0  \\\n",
       "AT1G01020           0           0           0           0           0   \n",
       "AT1G01030           0           0           0           0           0   \n",
       "AT1G01040           0           0           0           1           1   \n",
       "AT1G01050           0           0           0           0           0   \n",
       "...               ...         ...         ...         ...         ...   \n",
       "AT5G67600           0           0           0           0           0   \n",
       "AT5G67610           0           0           0           0           0   \n",
       "AT5G67620           0           0           0           0           0   \n",
       "AT5G67630           0           0           0           0           0   \n",
       "AT5G67640           0           0           0           0           0   \n",
       "\n",
       "           GO:0098869  GO:0099503  GO:0106310  GO:0110165  \n",
       "Gene ID                                                    \n",
       "AT1G01010           0           0           0           0  \n",
       "AT1G01020           0           0           0           0  \n",
       "AT1G01030           0           0           0           0  \n",
       "AT1G01040           0           0           0           0  \n",
       "AT1G01050           0           0           0           0  \n",
       "...               ...         ...         ...         ...  \n",
       "AT5G67600           0           0           0           0  \n",
       "AT5G67610           0           0           0           0  \n",
       "AT5G67620           0           0           0           0  \n",
       "AT5G67630           0           0           0           0  \n",
       "AT5G67640           0           0           0           0  \n",
       "\n",
       "[26955 rows x 829 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dap_33clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31bbe5b6-63ad-4e84-80b9-f8371d4ca69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT3G12130_colamp_a_AT3G12130</th>\n",
       "      <th>AT5G63260_col_a_AT5G63260</th>\n",
       "      <th>CDM1_colamp_a_AT1G68200</th>\n",
       "      <th>At5g08750_col_a_AT5G08750</th>\n",
       "      <th>AT5G63260_colamp_a_AT5G63260</th>\n",
       "      <th>FRS9_col_a_AT4G38170</th>\n",
       "      <th>FRS9_colamp_a_AT4G38170</th>\n",
       "      <th>HAT2_col_v31_AT5G47370</th>\n",
       "      <th>AT4G00250_col_a_AT4G00250</th>\n",
       "      <th>ARF2_col_v31_AT5G62000</th>\n",
       "      <th>...</th>\n",
       "      <th>GO:0061630</th>\n",
       "      <th>GO:0071456</th>\n",
       "      <th>GO:0071555</th>\n",
       "      <th>GO:0071704</th>\n",
       "      <th>GO:0090305</th>\n",
       "      <th>GO:0090502</th>\n",
       "      <th>GO:0098869</th>\n",
       "      <th>GO:0099503</th>\n",
       "      <th>GO:0106310</th>\n",
       "      <th>GO:0110165</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gene ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT1G01010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT1G01020</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT1G01030</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT1G01040</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT1G01050</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 829 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT3G12130_colamp_a_AT3G12130  AT5G63260_col_a_AT5G63260   \n",
       "Gene ID                                                              \n",
       "AT1G01010                             0                          0  \\\n",
       "AT1G01020                             0                          0   \n",
       "AT1G01030                             0                          1   \n",
       "AT1G01040                             1                          1   \n",
       "AT1G01050                             1                          0   \n",
       "\n",
       "           CDM1_colamp_a_AT1G68200  At5g08750_col_a_AT5G08750   \n",
       "Gene ID                                                         \n",
       "AT1G01010                        0                          0  \\\n",
       "AT1G01020                        0                          0   \n",
       "AT1G01030                        0                          0   \n",
       "AT1G01040                        0                          0   \n",
       "AT1G01050                        1                          0   \n",
       "\n",
       "           AT5G63260_colamp_a_AT5G63260  FRS9_col_a_AT4G38170   \n",
       "Gene ID                                                         \n",
       "AT1G01010                             1                     0  \\\n",
       "AT1G01020                             1                     1   \n",
       "AT1G01030                             1                     1   \n",
       "AT1G01040                             0                     0   \n",
       "AT1G01050                             0                     0   \n",
       "\n",
       "           FRS9_colamp_a_AT4G38170  HAT2_col_v31_AT5G47370   \n",
       "Gene ID                                                      \n",
       "AT1G01010                        0                       0  \\\n",
       "AT1G01020                        0                       1   \n",
       "AT1G01030                        0                       1   \n",
       "AT1G01040                        0                       1   \n",
       "AT1G01050                        0                       0   \n",
       "\n",
       "           AT4G00250_col_a_AT4G00250  ARF2_col_v31_AT5G62000  ...  GO:0061630   \n",
       "Gene ID                                                       ...               \n",
       "AT1G01010                          0                       0  ...           0  \\\n",
       "AT1G01020                          1                       0  ...           0   \n",
       "AT1G01030                          1                       1  ...           0   \n",
       "AT1G01040                          0                       1  ...           0   \n",
       "AT1G01050                          0                       0  ...           0   \n",
       "\n",
       "           GO:0071456  GO:0071555  GO:0071704  GO:0090305  GO:0090502   \n",
       "Gene ID                                                                 \n",
       "AT1G01010           0           0           0           0           0  \\\n",
       "AT1G01020           0           0           0           0           0   \n",
       "AT1G01030           0           0           0           0           0   \n",
       "AT1G01040           0           0           0           1           1   \n",
       "AT1G01050           0           0           0           0           0   \n",
       "\n",
       "           GO:0098869  GO:0099503  GO:0106310  GO:0110165  \n",
       "Gene ID                                                    \n",
       "AT1G01010           0           0           0           0  \n",
       "AT1G01020           0           0           0           0  \n",
       "AT1G01030           0           0           0           0  \n",
       "AT1G01040           0           0           0           0  \n",
       "AT1G01050           0           0           0           0  \n",
       "\n",
       "[5 rows x 829 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Root33_with_ind = dap_33clust\n",
    "Root33_with_ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff9f83e5-a072-42b8-8447-f64f453ea3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GO:0003735</th>\n",
       "      <th>GO:0005840</th>\n",
       "      <th>GO:0006260</th>\n",
       "      <th>GO:0006412</th>\n",
       "      <th>GO:0022625</th>\n",
       "      <th>GO:0022626</th>\n",
       "      <th>GO:0031640</th>\n",
       "      <th>GO:0042254</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT1G65220</th>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT3G59690</th>\n",
       "      <td>+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT4G25830</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT3G58960</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT2G05590</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          GO:0003735 GO:0005840 GO:0006260 GO:0006412 GO:0022625 GO:0022626   \n",
       "AT1G65220          +          +          0          +          +          +  \\\n",
       "AT3G59690          +          0          0          0          0          0   \n",
       "AT4G25830          0          0          0          0          0          0   \n",
       "AT3G58960          0          0          0          0          0          0   \n",
       "AT2G05590          0          0          0          +          0          0   \n",
       "\n",
       "          GO:0031640 GO:0042254  \n",
       "AT1G65220          0          0  \n",
       "AT3G59690          0          +  \n",
       "AT4G25830          0          0  \n",
       "AT3G58960          +          0  \n",
       "AT2G05590          0          0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_predictions_df2 = Root33_with_ind.loc[novel_predictions_df.index, selected_gos_list]\n",
    "\n",
    "def compare_cells(x, y):\n",
    "    if x == 0 and y == 0:\n",
    "        return '0'\n",
    "    elif x == 1 and y == 1:\n",
    "        return '1'\n",
    "    elif x == 0 and y == 1:\n",
    "        return \"+\"\n",
    "    elif x == 1 and y == 0:\n",
    "        return \"-\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid cell values: x={}, y={}\".format(x, y))\n",
    "\n",
    "#result = orig_predictions_df3.apply(lambda x: x.apply(lambda y: compare_cells(y, novel_predictions_df.loc[x.name, y])), axis=0)\n",
    "result = orig_predictions_df2.apply(lambda x: x.combine(novel_predictions_df[x.name], compare_cells))\n",
    "result2 = orig_predictions_df2.apply(lambda x: x.combine(novel_predictions_thresh[x.name], compare_cells))\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89a79fb6-81ca-4ac1-a1ed-5391ef936077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0003735</th>\n",
       "      <td>149</td>\n",
       "      <td>1139</td>\n",
       "      <td>12148</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0005802</th>\n",
       "      <td>113</td>\n",
       "      <td>4189</td>\n",
       "      <td>9153</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0005840</th>\n",
       "      <td>164</td>\n",
       "      <td>1629</td>\n",
       "      <td>11637</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0006412</th>\n",
       "      <td>196</td>\n",
       "      <td>2531</td>\n",
       "      <td>10693</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0009451</th>\n",
       "      <td>51</td>\n",
       "      <td>3910</td>\n",
       "      <td>9504</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0022625</th>\n",
       "      <td>57</td>\n",
       "      <td>1509</td>\n",
       "      <td>11902</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0022626</th>\n",
       "      <td>76</td>\n",
       "      <td>1552</td>\n",
       "      <td>11827</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0031640</th>\n",
       "      <td>114</td>\n",
       "      <td>3302</td>\n",
       "      <td>10043</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0042254</th>\n",
       "      <td>29</td>\n",
       "      <td>2897</td>\n",
       "      <td>10532</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TP    FP     TN  FN\n",
       "GO:0003735  149  1139  12148  42\n",
       "GO:0005802  113  4189   9153  23\n",
       "GO:0005840  164  1629  11637  48\n",
       "GO:0006412  196  2531  10693  58\n",
       "GO:0009451   51  3910   9504  13\n",
       "GO:0022625   57  1509  11902  10\n",
       "GO:0022626   76  1552  11827  23\n",
       "GO:0031640  114  3302  10043  19\n",
       "GO:0042254   29  2897  10532  20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP =  result.apply(lambda x: x.value_counts().get('+', 0))\n",
    "TP =  result.apply(lambda x: x.value_counts().get('1', 0))\n",
    "FN =  result.apply(lambda x: x.value_counts().get('-', 0))\n",
    "TN =  result.apply(lambda x: x.value_counts().get('0', 0))\n",
    "FP2 =  result2.apply(lambda x: x.value_counts().get('+', 0))\n",
    "TP2 =  result2.apply(lambda x: x.value_counts().get('1', 0))\n",
    "FN2 =  result2.apply(lambda x: x.value_counts().get('-', 0))\n",
    "TN2 =  result2.apply(lambda x: x.value_counts().get('0', 0))\n",
    " \n",
    "summary_table = pd.concat([TP, FP, TN, FN], axis=1)\n",
    "summary_table.columns = [\"TP\", \"FP\", \"TN\", \"FN\"]\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30c11179-d075-4831-9103-0a57af3c0c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0003735</th>\n",
       "      <td>114</td>\n",
       "      <td>283</td>\n",
       "      <td>13004</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0005802</th>\n",
       "      <td>66</td>\n",
       "      <td>1468</td>\n",
       "      <td>11874</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0005840</th>\n",
       "      <td>122</td>\n",
       "      <td>264</td>\n",
       "      <td>13002</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0006412</th>\n",
       "      <td>139</td>\n",
       "      <td>628</td>\n",
       "      <td>12596</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0009451</th>\n",
       "      <td>24</td>\n",
       "      <td>642</td>\n",
       "      <td>12772</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0022625</th>\n",
       "      <td>46</td>\n",
       "      <td>333</td>\n",
       "      <td>13078</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0022626</th>\n",
       "      <td>58</td>\n",
       "      <td>243</td>\n",
       "      <td>13136</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0031640</th>\n",
       "      <td>86</td>\n",
       "      <td>1686</td>\n",
       "      <td>11659</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0042254</th>\n",
       "      <td>17</td>\n",
       "      <td>1296</td>\n",
       "      <td>12133</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TP    FP     TN   FN\n",
       "GO:0003735  114   283  13004   77\n",
       "GO:0005802   66  1468  11874   70\n",
       "GO:0005840  122   264  13002   90\n",
       "GO:0006412  139   628  12596  115\n",
       "GO:0009451   24   642  12772   40\n",
       "GO:0022625   46   333  13078   21\n",
       "GO:0022626   58   243  13136   41\n",
       "GO:0031640   86  1686  11659   47\n",
       "GO:0042254   17  1296  12133   32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table2 = pd.concat([TP2, FP2, TN2, FN2], axis=1)\n",
    "summary_table2.columns = [\"TP\", \"FP\", \"TN\", \"FN\"]\n",
    "summary_table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0572cbee-db0f-4441-bdec-688bba009421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0003735</th>\n",
       "      <td>0.115683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0005802</th>\n",
       "      <td>0.026267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0005840</th>\n",
       "      <td>0.091467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0006412</th>\n",
       "      <td>0.071874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0009451</th>\n",
       "      <td>0.012876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0022625</th>\n",
       "      <td>0.036398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0022626</th>\n",
       "      <td>0.046683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0031640</th>\n",
       "      <td>0.033372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0042254</th>\n",
       "      <td>0.009911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            precision_score\n",
       "GO:0003735         0.115683\n",
       "GO:0005802         0.026267\n",
       "GO:0005840         0.091467\n",
       "GO:0006412         0.071874\n",
       "GO:0009451         0.012876\n",
       "GO:0022625         0.036398\n",
       "GO:0022626         0.046683\n",
       "GO:0031640         0.033372\n",
       "GO:0042254         0.009911"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics = pd.DataFrame()\n",
    "eval_metrics2 = pd.DataFrame()\n",
    "eval_metrics2[\"precision_score\"] = summary_table2[\"TP\"]/(summary_table2[\"TP\"] + summary_table2[\"FP\"])\n",
    "eval_metrics[\"precision_score\"] = summary_table[\"TP\"]/(summary_table[\"TP\"] + summary_table[\"FP\"])\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ff2f7a8-0624-448c-a31c-a14d9c8fb860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0022625</th>\n",
       "      <td>0.036398</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.069810</td>\n",
       "      <td>0.887298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0003735</th>\n",
       "      <td>0.115683</td>\n",
       "      <td>0.780105</td>\n",
       "      <td>0.201487</td>\n",
       "      <td>0.912376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0005840</th>\n",
       "      <td>0.091467</td>\n",
       "      <td>0.773585</td>\n",
       "      <td>0.163591</td>\n",
       "      <td>0.875575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0022626</th>\n",
       "      <td>0.046683</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.088014</td>\n",
       "      <td>0.883143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0042254</th>\n",
       "      <td>0.009911</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.019496</td>\n",
       "      <td>0.783573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0031640</th>\n",
       "      <td>0.033372</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.064243</td>\n",
       "      <td>0.753598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0006412</th>\n",
       "      <td>0.071874</td>\n",
       "      <td>0.771654</td>\n",
       "      <td>0.131499</td>\n",
       "      <td>0.807909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0009451</th>\n",
       "      <td>0.012876</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.025342</td>\n",
       "      <td>0.708933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0005802</th>\n",
       "      <td>0.026267</td>\n",
       "      <td>0.830882</td>\n",
       "      <td>0.050924</td>\n",
       "      <td>0.687491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            precision_score  recall_score  F1_score  Accuracy\n",
       "GO:0022625         0.036398      0.850746  0.069810  0.887298\n",
       "GO:0003735         0.115683      0.780105  0.201487  0.912376\n",
       "GO:0005840         0.091467      0.773585  0.163591  0.875575\n",
       "GO:0022626         0.046683      0.767677  0.088014  0.883143\n",
       "GO:0042254         0.009911      0.591837  0.019496  0.783573\n",
       "GO:0031640         0.033372      0.857143  0.064243  0.753598\n",
       "GO:0006412         0.071874      0.771654  0.131499  0.807909\n",
       "GO:0009451         0.012876      0.796875  0.025342  0.708933\n",
       "GO:0005802         0.026267      0.830882  0.050924  0.687491"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics[\"Train Precision\"] = Root33_results_df.loc[eval_metrics.index][\"Mean Training Precision\"]\n",
    "eval_metrics[\"Validation Precision\"] = Root33_results_df.loc[eval_metrics.index][\"Mean Validation Precision\"]\n",
    "eval_metrics[\"recall_score\"] =  summary_table[\"TP\"]/(summary_table[\"TP\"] + summary_table[\"FN\"])\n",
    "eval_metrics[\"Training Recall\"] = Root33_results_df.loc[eval_metrics.index][\"Mean Training Recall\"]\n",
    "eval_metrics[\"Validation Recall\"] = Root33_results_df.loc[eval_metrics.index][\"Mean Validation Recall\"]\n",
    "eval_metrics[\"F1_score\"] = 2 * (eval_metrics[\"precision_score\"] * eval_metrics[\"recall_score\"] )/ (eval_metrics[\"precision_score\"] + eval_metrics[\"recall_score\"])\n",
    "eval_metrics[\"Training F1\"] = Root33_results_df.loc[eval_metrics.index][\"Mean Training F1 Score\"]\n",
    "eval_metrics[\"Validation F1\"] = Root33_results_df.loc[eval_metrics.index][\"Mean Validation F1 Score\"]\n",
    "eval_metrics[\"Accuracy\"] = (summary_table[\"TP\"] + summary_table[\"TN\"]) /(summary_table[\"TP\"] + summary_table[\"TN\"] + summary_table[\"FP\"] + summary_table[\"FN\"])\n",
    "eval_metrics[\"Train Accuracy\"] = Root33_results_df.loc[eval_metrics.index][\"Mean Training Accuracy\"]\n",
    "eval_metrics[\"Validation Accuracy\"] = Root33_results_df.loc[eval_metrics.index][\"Mean Validation Accuracy\"]\n",
    "eval_metrics.sort_values(by=\"Validation Precision\", ascending = False)\n",
    "eval_metrics.sort_values(by=\"Validation Precision\", ascending = False)[[\"precision_score\", \"recall_score\", \"F1_score\", \"Accuracy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1afad788-5f22-4b49-b47f-ad64bf0a701f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0022625</th>\n",
       "      <td>0.121372</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.206278</td>\n",
       "      <td>0.974551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0003735</th>\n",
       "      <td>0.287154</td>\n",
       "      <td>0.596859</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.975887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0005840</th>\n",
       "      <td>0.316062</td>\n",
       "      <td>0.575472</td>\n",
       "      <td>0.408027</td>\n",
       "      <td>0.976851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0022626</th>\n",
       "      <td>0.192691</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.980264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0042254</th>\n",
       "      <td>0.012947</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.024963</td>\n",
       "      <td>0.902359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0031640</th>\n",
       "      <td>0.048533</td>\n",
       "      <td>0.646617</td>\n",
       "      <td>0.090289</td>\n",
       "      <td>0.873498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0006412</th>\n",
       "      <td>0.181226</td>\n",
       "      <td>0.547244</td>\n",
       "      <td>0.272282</td>\n",
       "      <td>0.949102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0009451</th>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.065753</td>\n",
       "      <td>0.951402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0005802</th>\n",
       "      <td>0.043025</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.079042</td>\n",
       "      <td>0.889375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            precision_score  recall_score  F1_score  Accuracy\n",
       "GO:0022625         0.121372      0.686567  0.206278  0.974551\n",
       "GO:0003735         0.287154      0.596859  0.387755  0.975887\n",
       "GO:0005840         0.316062      0.575472  0.408027  0.976851\n",
       "GO:0022626         0.192691      0.585859  0.290000  0.980264\n",
       "GO:0042254         0.012947      0.346939  0.024963  0.902359\n",
       "GO:0031640         0.048533      0.646617  0.090289  0.873498\n",
       "GO:0006412         0.181226      0.547244  0.272282  0.949102\n",
       "GO:0009451         0.036036      0.375000  0.065753  0.951402\n",
       "GO:0005802         0.043025      0.485294  0.079042  0.889375"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics2[\"Train Precision\"] = Root33_results_df.loc[eval_metrics2.index][\"Mean Training Precision\"]\n",
    "eval_metrics2[\"Validation Precision\"] = Root33_results_df.loc[eval_metrics2.index][\"Mean Validation Precision\"]\n",
    "eval_metrics2[\"recall_score\"] =  summary_table2[\"TP\"]/(summary_table2[\"TP\"] + summary_table2[\"FN\"])\n",
    "eval_metrics2[\"Training Recall\"] = Root33_results_df.loc[eval_metrics2.index][\"Mean Training Recall\"]\n",
    "eval_metrics2[\"Validation Recall\"] = Root33_results_df.loc[eval_metrics2.index][\"Mean Validation Recall\"]\n",
    "eval_metrics2[\"F1_score\"] = 2 * (eval_metrics2[\"precision_score\"] * eval_metrics2[\"recall_score\"] )/ (eval_metrics2[\"precision_score\"] + eval_metrics2[\"recall_score\"])\n",
    "eval_metrics2[\"Training F1\"] = Root33_results_df.loc[eval_metrics2.index][\"Mean Training F1 Score\"]\n",
    "eval_metrics2[\"Validation F1\"] = Root33_results_df.loc[eval_metrics2.index][\"Mean Validation F1 Score\"]\n",
    "eval_metrics2[\"Accuracy\"] = (summary_table[\"TP\"] + summary_table2[\"TN\"]) /(summary_table2[\"TP\"] + summary_table2[\"TN\"] + summary_table2[\"FP\"] + summary_table2[\"FN\"])\n",
    "eval_metrics2[\"Train Accuracy\"] = Root33_results_df.loc[eval_metrics2.index][\"Mean Training Accuracy\"]\n",
    "eval_metrics2[\"Validation Accuracy\"] = Root33_results_df.loc[eval_metrics.index][\"Mean Validation Accuracy\"]\n",
    "eval_metrics2.sort_values(by=\"Validation Precision\", ascending = False)[[\"precision_score\", \"recall_score\", \"F1_score\", \"Accuracy\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d28037e-9124-44b8-bedb-897c9b865307",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# False Positives and False Negative Genes for each GO Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a914267e-b105-4021-a9fd-a21035399e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AT4G37660',\n",
       " 'AT5G59240',\n",
       " 'AT4G21460',\n",
       " 'AT2G47570',\n",
       " 'AT3G17626',\n",
       " 'AT5G63070',\n",
       " 'AT5G14290',\n",
       " 'AT1G12960',\n",
       " 'AT1G66580',\n",
       " 'AT5G24510',\n",
       " 'AT3G27840',\n",
       " 'AT3G09680',\n",
       " 'AT2G34210',\n",
       " 'AT1G52370',\n",
       " 'AT3G15190',\n",
       " 'AT5G51610',\n",
       " 'AT5G15650',\n",
       " 'AT3G52300',\n",
       " 'AT1G79850',\n",
       " 'AT3G44890',\n",
       " 'AT2G03130',\n",
       " 'AT1G77750',\n",
       " 'AT2G43310',\n",
       " 'AT1G29970',\n",
       " 'AT1G69485',\n",
       " 'AT3G04770',\n",
       " 'AT5G40040',\n",
       " 'AT1G36240',\n",
       " 'AT4G17560',\n",
       " 'AT3G17820',\n",
       " 'AT3G61111',\n",
       " 'AT5G63300',\n",
       " 'AT3G27160',\n",
       " 'AT3G08520',\n",
       " 'AT4G09012',\n",
       " 'AT5G15760',\n",
       " 'AT3G22450']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SONG:\n",
    "# Need a table for each GO term, what are the genes that are FP / FN \n",
    "# FN genes can be used to understand why the ML failed on these genes\n",
    "# FP genes are useful predictions that were not annotated by prior research but can be â€œnovelâ€ predictions by the ML method and \n",
    "# can potentially be validated, say via AlphaFold type of ML method followed by bench work. \n",
    "\n",
    "result_dict = {}\n",
    "for go_term in result.columns:\n",
    "    fp_genes = []\n",
    "    fn_genes = []\n",
    "    for index, value in result[go_term].items():\n",
    "        gene = index  \n",
    "        if value == '+':\n",
    "            fp_genes.append(gene)\n",
    "        elif value == '-':\n",
    "            fn_genes.append(gene)\n",
    "\n",
    "    result_dict[go_term] = {'FP': fp_genes, 'FN': fn_genes}\n",
    "\n",
    "# To access the FP genes for each GO term, use result_dict['go_term']['FP' / 'FN']\n",
    "result_dict['GO:0003735']['FN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63058c5-07cd-42d2-ac15-1c59984c9dfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Balanced Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b648cc-160f-42cd-842f-f35d1fbacf05",
   "metadata": {},
   "source": [
    "### 0.5 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ac148b-88b3-4a1e-a68e-d84f54644ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this GO Term, Reduced Majority Size:  1281 Minority Size:  1281\n",
      "For this GO Term, Reduced Majority Size:  80 Minority Size:  80\n",
      "For this GO Term, Reduced Majority Size:  345 Minority Size:  345\n",
      "For this GO Term, Reduced Majority Size:  83 Minority Size:  83\n",
      "For this GO Term, Reduced Majority Size:  372 Minority Size:  372\n",
      "For this GO Term, Reduced Majority Size:  122 Minority Size:  122\n",
      "For this GO Term, Reduced Majority Size:  140 Minority Size:  140\n",
      "For this GO Term, Reduced Majority Size:  2244 Minority Size:  2244\n",
      "For this GO Term, Reduced Majority Size:  344 Minority Size:  344\n",
      "For this GO Term, Reduced Majority Size:  1066 Minority Size:  1066\n",
      "For this GO Term, Reduced Majority Size:  63 Minority Size:  63\n",
      "For this GO Term, Reduced Majority Size:  55 Minority Size:  55\n",
      "For this GO Term, Reduced Majority Size:  822 Minority Size:  822\n",
      "For this GO Term, Reduced Majority Size:  539 Minority Size:  539\n",
      "For this GO Term, Reduced Majority Size:  478 Minority Size:  478\n",
      "For this GO Term, Reduced Majority Size:  182 Minority Size:  182\n",
      "For this GO Term, Reduced Majority Size:  54 Minority Size:  54\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  305 Minority Size:  305\n",
      "For this GO Term, Reduced Majority Size:  107 Minority Size:  107\n",
      "For this GO Term, Reduced Majority Size:  64 Minority Size:  64\n",
      "For this GO Term, Reduced Majority Size:  117 Minority Size:  117\n",
      "For this GO Term, Reduced Majority Size:  141 Minority Size:  141\n",
      "For this GO Term, Reduced Majority Size:  67 Minority Size:  67\n",
      "For this GO Term, Reduced Majority Size:  56 Minority Size:  56\n",
      "For this GO Term, Reduced Majority Size:  129 Minority Size:  129\n",
      "For this GO Term, Reduced Majority Size:  63 Minority Size:  63\n",
      "For this GO Term, Reduced Majority Size:  542 Minority Size:  542\n",
      "For this GO Term, Reduced Majority Size:  442 Minority Size:  442\n",
      "For this GO Term, Reduced Majority Size:  356 Minority Size:  356\n",
      "For this GO Term, Reduced Majority Size:  76 Minority Size:  76\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  140 Minority Size:  140\n",
      "For this GO Term, Reduced Majority Size:  68 Minority Size:  68\n",
      "For this GO Term, Reduced Majority Size:  176 Minority Size:  176\n",
      "For this GO Term, Reduced Majority Size:  105 Minority Size:  105\n",
      "For this GO Term, Reduced Majority Size:  136 Minority Size:  136\n",
      "For this GO Term, Reduced Majority Size:  3302 Minority Size:  3302\n",
      "For this GO Term, Reduced Majority Size:  114 Minority Size:  114\n",
      "For this GO Term, Reduced Majority Size:  1222 Minority Size:  1222\n",
      "For this GO Term, Reduced Majority Size:  148 Minority Size:  148\n",
      "For this GO Term, Reduced Majority Size:  933 Minority Size:  933\n",
      "For this GO Term, Reduced Majority Size:  787 Minority Size:  787\n",
      "For this GO Term, Reduced Majority Size:  83 Minority Size:  83\n",
      "For this GO Term, Reduced Majority Size:  2828 Minority Size:  2828\n",
      "For this GO Term, Reduced Majority Size:  84 Minority Size:  84\n",
      "For this GO Term, Reduced Majority Size:  62 Minority Size:  62\n",
      "For this GO Term, Reduced Majority Size:  78 Minority Size:  78\n",
      "For this GO Term, Reduced Majority Size:  279 Minority Size:  279\n",
      "For this GO Term, Reduced Majority Size:  1830 Minority Size:  1830\n",
      "For this GO Term, Reduced Majority Size:  811 Minority Size:  811\n",
      "For this GO Term, Reduced Majority Size:  105 Minority Size:  105\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  231 Minority Size:  231\n",
      "For this GO Term, Reduced Majority Size:  245 Minority Size:  245\n",
      "For this GO Term, Reduced Majority Size:  106 Minority Size:  106\n",
      "For this GO Term, Reduced Majority Size:  175 Minority Size:  175\n",
      "For this GO Term, Reduced Majority Size:  590 Minority Size:  590\n",
      "For this GO Term, Reduced Majority Size:  277 Minority Size:  277\n",
      "For this GO Term, Reduced Majority Size:  579 Minority Size:  579\n",
      "For this GO Term, Reduced Majority Size:  141 Minority Size:  141\n",
      "For this GO Term, Reduced Majority Size:  1300 Minority Size:  1300\n",
      "For this GO Term, Reduced Majority Size:  194 Minority Size:  194\n",
      "For this GO Term, Reduced Majority Size:  138 Minority Size:  138\n",
      "For this GO Term, Reduced Majority Size:  94 Minority Size:  94\n",
      "For this GO Term, Reduced Majority Size:  1402 Minority Size:  1402\n",
      "For this GO Term, Reduced Majority Size:  58 Minority Size:  58\n",
      "For this GO Term, Reduced Majority Size:  286 Minority Size:  286\n",
      "For this GO Term, Reduced Majority Size:  59 Minority Size:  59\n",
      "For this GO Term, Reduced Majority Size:  131 Minority Size:  131\n",
      "For this GO Term, Reduced Majority Size:  101 Minority Size:  101\n",
      "For this GO Term, Reduced Majority Size:  55 Minority Size:  55\n",
      "For this GO Term, Reduced Majority Size:  950 Minority Size:  950\n",
      "For this GO Term, Reduced Majority Size:  199 Minority Size:  199\n",
      "For this GO Term, Reduced Majority Size:  86 Minority Size:  86\n",
      "For this GO Term, Reduced Majority Size:  47 Minority Size:  47\n",
      "For this GO Term, Reduced Majority Size:  152 Minority Size:  152\n",
      "For this GO Term, Reduced Majority Size:  235 Minority Size:  235\n",
      "For this GO Term, Reduced Majority Size:  59 Minority Size:  59\n",
      "For this GO Term, Reduced Majority Size:  97 Minority Size:  97\n",
      "For this GO Term, Reduced Majority Size:  530 Minority Size:  530\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  250 Minority Size:  250\n",
      "For this GO Term, Reduced Majority Size:  127 Minority Size:  127\n",
      "For this GO Term, Reduced Majority Size:  268 Minority Size:  268\n",
      "For this GO Term, Reduced Majority Size:  80 Minority Size:  80\n",
      "For this GO Term, Reduced Majority Size:  67 Minority Size:  67\n",
      "For this GO Term, Reduced Majority Size:  197 Minority Size:  197\n",
      "For this GO Term, Reduced Majority Size:  56 Minority Size:  56\n",
      "For this GO Term, Reduced Majority Size:  111 Minority Size:  111\n",
      "For this GO Term, Reduced Majority Size:  590 Minority Size:  590\n",
      "For this GO Term, Reduced Majority Size:  71 Minority Size:  71\n",
      "For this GO Term, Reduced Majority Size:  108 Minority Size:  108\n",
      "For this GO Term, Reduced Majority Size:  134 Minority Size:  134\n",
      "For this GO Term, Reduced Majority Size:  130 Minority Size:  130\n",
      "For this GO Term, Reduced Majority Size:  198 Minority Size:  198\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  79 Minority Size:  79\n",
      "For this GO Term, Reduced Majority Size:  2242 Minority Size:  2242\n",
      "For this GO Term, Reduced Majority Size:  248 Minority Size:  248\n",
      "For this GO Term, Reduced Majority Size:  186 Minority Size:  186\n",
      "For this GO Term, Reduced Majority Size:  65 Minority Size:  65\n",
      "For this GO Term, Reduced Majority Size:  242 Minority Size:  242\n",
      "For this GO Term, Reduced Majority Size:  52 Minority Size:  52\n",
      "For this GO Term, Reduced Majority Size:  59 Minority Size:  59\n",
      "For this GO Term, Reduced Majority Size:  309 Minority Size:  309\n",
      "For this GO Term, Reduced Majority Size:  85 Minority Size:  85\n",
      "For this GO Term, Reduced Majority Size:  94 Minority Size:  94\n",
      "For this GO Term, Reduced Majority Size:  67 Minority Size:  67\n",
      "For this GO Term, Reduced Majority Size:  91 Minority Size:  91\n",
      "For this GO Term, Reduced Majority Size:  59 Minority Size:  59\n",
      "For this GO Term, Reduced Majority Size:  90 Minority Size:  90\n",
      "For this GO Term, Reduced Majority Size:  154 Minority Size:  154\n",
      "For this GO Term, Reduced Majority Size:  184 Minority Size:  184\n",
      "For this GO Term, Reduced Majority Size:  89 Minority Size:  89\n",
      "For this GO Term, Reduced Majority Size:  53 Minority Size:  53\n",
      "For this GO Term, Reduced Majority Size:  185 Minority Size:  185\n",
      "For this GO Term, Reduced Majority Size:  434 Minority Size:  434\n",
      "For this GO Term, Reduced Majority Size:  1065 Minority Size:  1065\n",
      "For this GO Term, Reduced Majority Size:  43 Minority Size:  43\n",
      "For this GO Term, Reduced Majority Size:  101 Minority Size:  101\n",
      "For this GO Term, Reduced Majority Size:  178 Minority Size:  178\n",
      "For this GO Term, Reduced Majority Size:  1011 Minority Size:  1011\n",
      "For this GO Term, Reduced Majority Size:  127 Minority Size:  127\n",
      "For this GO Term, Reduced Majority Size:  337 Minority Size:  337\n",
      "For this GO Term, Reduced Majority Size:  173 Minority Size:  173\n",
      "For this GO Term, Reduced Majority Size:  101 Minority Size:  101\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  188 Minority Size:  188\n",
      "For this GO Term, Reduced Majority Size:  94 Minority Size:  94\n",
      "For this GO Term, Reduced Majority Size:  63 Minority Size:  63\n",
      "For this GO Term, Reduced Majority Size:  151 Minority Size:  151\n",
      "For this GO Term, Reduced Majority Size:  131 Minority Size:  131\n",
      "For this GO Term, Reduced Majority Size:  200 Minority Size:  200\n",
      "For this GO Term, Reduced Majority Size:  111 Minority Size:  111\n",
      "For this GO Term, Reduced Majority Size:  50 Minority Size:  50\n",
      "For this GO Term, Reduced Majority Size:  69 Minority Size:  69\n",
      "For this GO Term, Reduced Majority Size:  161 Minority Size:  161\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  107 Minority Size:  107\n",
      "For this GO Term, Reduced Majority Size:  118 Minority Size:  118\n",
      "For this GO Term, Reduced Majority Size:  247 Minority Size:  247\n",
      "For this GO Term, Reduced Majority Size:  60 Minority Size:  60\n",
      "For this GO Term, Reduced Majority Size:  56 Minority Size:  56\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  90 Minority Size:  90\n",
      "For this GO Term, Reduced Majority Size:  66 Minority Size:  66\n",
      "For this GO Term, Reduced Majority Size:  313 Minority Size:  313\n",
      "For this GO Term, Reduced Majority Size:  88 Minority Size:  88\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  3472 Minority Size:  3472\n",
      "For this GO Term, Reduced Majority Size:  2917 Minority Size:  2917\n",
      "For this GO Term, Reduced Majority Size:  72 Minority Size:  72\n",
      "For this GO Term, Reduced Majority Size:  134 Minority Size:  134\n",
      "For this GO Term, Reduced Majority Size:  695 Minority Size:  695\n",
      "For this GO Term, Reduced Majority Size:  694 Minority Size:  694\n",
      "For this GO Term, Reduced Majority Size:  632 Minority Size:  632\n",
      "For this GO Term, Reduced Majority Size:  385 Minority Size:  385\n",
      "For this GO Term, Reduced Majority Size:  53 Minority Size:  53\n",
      "For this GO Term, Reduced Majority Size:  133 Minority Size:  133\n",
      "For this GO Term, Reduced Majority Size:  1691 Minority Size:  1691\n",
      "For this GO Term, Reduced Majority Size:  132 Minority Size:  132\n",
      "For this GO Term, Reduced Majority Size:  269 Minority Size:  269\n",
      "For this GO Term, Reduced Majority Size:  56 Minority Size:  56\n",
      "For this GO Term, Reduced Majority Size:  1237 Minority Size:  1237\n",
      "For this GO Term, Reduced Majority Size:  71 Minority Size:  71\n",
      "For this GO Term, Reduced Majority Size:  192 Minority Size:  192\n",
      "For this GO Term, Reduced Majority Size:  184 Minority Size:  184\n",
      "For this GO Term, Reduced Majority Size:  119 Minority Size:  119\n",
      "For this GO Term, Reduced Majority Size:  123 Minority Size:  123\n",
      "For this GO Term, Reduced Majority Size:  197 Minority Size:  197\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  49 Minority Size:  49\n",
      "For this GO Term, Reduced Majority Size:  182 Minority Size:  182\n",
      "For this GO Term, Reduced Majority Size:  73 Minority Size:  73\n",
      "For this GO Term, Reduced Majority Size:  102 Minority Size:  102\n",
      "For this GO Term, Reduced Majority Size:  176 Minority Size:  176\n",
      "For this GO Term, Reduced Majority Size:  116 Minority Size:  116\n",
      "For this GO Term, Reduced Majority Size:  112 Minority Size:  112\n",
      "For this GO Term, Reduced Majority Size:  137 Minority Size:  137\n",
      "For this GO Term, Reduced Majority Size:  73 Minority Size:  73\n",
      "For this GO Term, Reduced Majority Size:  141 Minority Size:  141\n",
      "For this GO Term, Reduced Majority Size:  83 Minority Size:  83\n",
      "For this GO Term, Reduced Majority Size:  185 Minority Size:  185\n",
      "For this GO Term, Reduced Majority Size:  72 Minority Size:  72\n",
      "For this GO Term, Reduced Majority Size:  78 Minority Size:  78\n",
      "For this GO Term, Reduced Majority Size:  74 Minority Size:  74\n",
      "For this GO Term, Reduced Majority Size:  52 Minority Size:  52\n",
      "For this GO Term, Reduced Majority Size:  162 Minority Size:  162\n",
      "For this GO Term, Reduced Majority Size:  106 Minority Size:  106\n",
      "For this GO Term, Reduced Majority Size:  112 Minority Size:  112\n",
      "For this GO Term, Reduced Majority Size:  85 Minority Size:  85\n",
      "For this GO Term, Reduced Majority Size:  187 Minority Size:  187\n",
      "For this GO Term, Reduced Majority Size:  87 Minority Size:  87\n",
      "For this GO Term, Reduced Majority Size:  242 Minority Size:  242\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  60 Minority Size:  60\n",
      "For this GO Term, Reduced Majority Size:  96 Minority Size:  96\n",
      "For this GO Term, Reduced Majority Size:  169 Minority Size:  169\n",
      "For this GO Term, Reduced Majority Size:  91 Minority Size:  91\n",
      "For this GO Term, Reduced Majority Size:  79 Minority Size:  79\n",
      "For this GO Term, Reduced Majority Size:  97 Minority Size:  97\n",
      "For this GO Term, Reduced Majority Size:  1632 Minority Size:  1632\n",
      "For this GO Term, Reduced Majority Size:  64 Minority Size:  64\n",
      "For this GO Term, Reduced Majority Size:  162 Minority Size:  162\n",
      "For this GO Term, Reduced Majority Size:  220 Minority Size:  220\n",
      "For this GO Term, Reduced Majority Size:  97 Minority Size:  97\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  66 Minority Size:  66\n",
      "For this GO Term, Reduced Majority Size:  93 Minority Size:  93\n",
      "For this GO Term, Reduced Majority Size:  258 Minority Size:  258\n",
      "For this GO Term, Reduced Majority Size:  62 Minority Size:  62\n",
      "For this GO Term, Reduced Majority Size:  56 Minority Size:  56\n",
      "For this GO Term, Reduced Majority Size:  76 Minority Size:  76\n",
      "For this GO Term, Reduced Majority Size:  132 Minority Size:  132\n",
      "For this GO Term, Reduced Majority Size:  72 Minority Size:  72\n",
      "For this GO Term, Reduced Majority Size:  400 Minority Size:  400\n",
      "For this GO Term, Reduced Majority Size:  132 Minority Size:  132\n",
      "For this GO Term, Reduced Majority Size:  121 Minority Size:  121\n",
      "For this GO Term, Reduced Majority Size:  195 Minority Size:  195\n",
      "For this GO Term, Reduced Majority Size:  49 Minority Size:  49\n",
      "For this GO Term, Reduced Majority Size:  105 Minority Size:  105\n",
      "For this GO Term, Reduced Majority Size:  46 Minority Size:  46\n",
      "For this GO Term, Reduced Majority Size:  77 Minority Size:  77\n",
      "For this GO Term, Reduced Majority Size:  75 Minority Size:  75\n",
      "For this GO Term, Reduced Majority Size:  337 Minority Size:  337\n",
      "For this GO Term, Reduced Majority Size:  139 Minority Size:  139\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "CB_Root33_Go_Mean_Accuracy = []\n",
    "CB_Root33_Go_Std_Accuracy = []\n",
    "CB_Root33_Go_10cross_Accuracies = []\n",
    "CB_Root33_Go_10cross_Mean_F1=[]\n",
    "CB_Root33_Go_10cross_Mean_Precision=[]\n",
    "CB_Root33_Go_10cross_Mean_Recall=[]\n",
    "CB_Root33_results_dict = {}\n",
    "column_headers = list(Root33_data_setA.columns)\n",
    "\n",
    "\n",
    "for i in range(len(Feature_columns), Root33_data_setA.shape[1]):\n",
    "    X = Root33_data_setA.iloc[:, :len(Feature_columns) ]\n",
    "    y = Root33_data_setA.iloc[:,i]\n",
    "\n",
    "    # Determin the minority class\n",
    "    minority_class = 1 if sum(y == 1) < sum(y == 0) else 0\n",
    "    minority_indices = np.where(y == minority_class)[0]\n",
    "    majority_indices = np.where(y != minority_class)[0]\n",
    "    majority_indices_downsampled = resample(majority_indices, replace=False, n_samples=len(minority_indices), random_state=0)\n",
    "\n",
    "    print(\"For this GO Term, Reduced Majority Size: \", len(majority_indices_downsampled), \"Minority Size: \", len(minority_indices))\n",
    "    \n",
    "    # Combine minority and downsampled majority indices\n",
    "    indices_combined = np.concatenate([minority_indices, majority_indices_downsampled])\n",
    "\n",
    "    # Subset the data based on the selected indices\n",
    "    X_balanced = X.iloc[indices_combined, :]\n",
    "    y_balanced = y.iloc[indices_combined]\n",
    "\n",
    "    # Train the random forest model\n",
    "    goterm = column_headers[i]\n",
    "    CB_Root33_results_dict[goterm] = cross_validation(pipe, X_balanced, y_balanced, 10)\n",
    "\n",
    "    # CB_Root33_Go_Mean_Accuracy.append(Random_forest_result[\"Mean Validation Accuracy\"])\n",
    "    # CB_Root33_Go_Std_Accuracy.append(Random_forest_result[\"Std Validation Accuracy\"])\n",
    "    # CB_Root33_Go_10cross_Accuracies.append(Random_forest_result[\"Validation Accuracy scores\"])\n",
    "    # CB_Root33_Go_10cross_Mean_F1.append(Random_forest_result[\"Mean Validation F1 Score\"])\n",
    "    # CB_Root33_Go_10cross_Mean_Precision.append(Random_forest_result[\"Mean Validation Precision\"])\n",
    "    # CB_Root33_Go_10cross_Mean_Recall.append(Random_forest_result[\"Mean Validation Recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52bb9a0f-f3aa-4c45-929b-ea73d40483fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy scores</th>\n",
       "      <th>Mean Training Accuracy</th>\n",
       "      <th>Training Precision scores</th>\n",
       "      <th>Mean Training Precision</th>\n",
       "      <th>Training Recall scores</th>\n",
       "      <th>Mean Training Recall</th>\n",
       "      <th>Training F1 scores</th>\n",
       "      <th>Mean Training F1 Score</th>\n",
       "      <th>Validation Accuracy scores</th>\n",
       "      <th>Mean Validation Accuracy</th>\n",
       "      <th>Std Validation Accuracy</th>\n",
       "      <th>Validation Precision scores</th>\n",
       "      <th>Mean Validation Precision</th>\n",
       "      <th>Validation Recall scores</th>\n",
       "      <th>Mean Validation Recall</th>\n",
       "      <th>Validation F1 scores</th>\n",
       "      <th>Mean Validation F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0000166</th>\n",
       "      <td>[0.9509761388286334, 0.9596529284164859, 0.9648742411101474, 0.9570685169124025, 0.9570685169124025, 0.9549002601908065, 0.9522983521248916, 0.9588031222896791, 0.9549002601908065, 0.95836947094536]</td>\n",
       "      <td>95.689118</td>\n",
       "      <td>[0.9545056867891514, 0.9715302491103203, 0.972663139329806, 0.9647266313932981, 0.9639084507042254, 0.961301671064204, 0.9461077844311377, 0.9640350877192982, 0.9704035874439462, 0.9714540588760036]</td>\n",
       "      <td>0.964064</td>\n",
       "      <td>[0.9470486111111112, 0.9470945359930616, 0.9566348655680833, 0.9488291413703382, 0.9496964440589766, 0.9479618386816999, 0.9592367736339983, 0.9531656548135299, 0.9384215091066782, 0.9444926279271...</td>\n",
       "      <td>0.949258</td>\n",
       "      <td>[0.9507625272331154, 0.9591567852437418, 0.964582422387407, 0.9567118495846088, 0.9567496723460026, 0.9545851528384279, 0.9526270456503014, 0.9585695595290012, 0.9541446208112875, 0.9577836411609498]</td>\n",
       "      <td>0.956567</td>\n",
       "      <td>[0.556420233463035, 0.5603112840466926, 0.62109375, 0.5625, 0.625, 0.54296875, 0.5546875, 0.5625, 0.546875, 0.55859375]</td>\n",
       "      <td>56.909503</td>\n",
       "      <td>2.765536</td>\n",
       "      <td>[0.556390977443609, 0.556390977443609, 0.6165413533834586, 0.5563380281690141, 0.6194029850746269, 0.5426356589147286, 0.547945205479452, 0.5526315789473685, 0.5441176470588235, 0.5555555555555556]</td>\n",
       "      <td>0.564795</td>\n",
       "      <td>[0.5736434108527132, 0.578125, 0.640625, 0.6171875, 0.6484375, 0.546875, 0.625, 0.65625, 0.578125, 0.5859375]</td>\n",
       "      <td>0.605021</td>\n",
       "      <td>[0.564885496183206, 0.5670498084291187, 0.6283524904214559, 0.5851851851851853, 0.633587786259542, 0.5447470817120622, 0.583941605839416, 0.6, 0.5606060606060606, 0.570342205323194]</td>\n",
       "      <td>0.58387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000287</th>\n",
       "      <td>[1.0, 1.0, 0.9930555555555556, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.930556</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0, 1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998611</td>\n",
       "      <td>[1.0, 1.0, 0.993006993006993, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.999301</td>\n",
       "      <td>[0.625, 0.4375, 0.6875, 0.625, 0.4375, 0.6875, 0.5, 0.5625, 0.625, 0.5625]</td>\n",
       "      <td>57.5</td>\n",
       "      <td>8.75</td>\n",
       "      <td>[0.625, 0.4, 0.6666666666666666, 0.625, 0.42857142857142855, 0.7142857142857143, 0.5, 0.5555555555555556, 0.6666666666666666, 0.5714285714285714]</td>\n",
       "      <td>0.575317</td>\n",
       "      <td>[0.625, 0.25, 0.75, 0.625, 0.375, 0.625, 0.5, 0.625, 0.5, 0.5]</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>[0.625, 0.3076923076923077, 0.7058823529411765, 0.625, 0.39999999999999997, 0.6666666666666666, 0.5, 0.5882352941176471, 0.5714285714285715, 0.5333333333333333]</td>\n",
       "      <td>0.552324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000325</th>\n",
       "      <td>[0.9871175523349437, 0.9871175523349437, 0.9871175523349437, 0.9855072463768116, 0.9903381642512077, 0.9806763285024155, 0.9919484702093397, 0.9887278582930756, 0.9951690821256038, 0.9903381642512...</td>\n",
       "      <td>98.84058</td>\n",
       "      <td>[0.9934640522875817, 0.9902597402597403, 0.9934640522875817, 0.990228013029316, 0.9967320261437909, 0.9869706840390879, 0.9903846153846154, 0.9871794871794872, 0.9967741935483871, 0.987220447284345]</td>\n",
       "      <td>0.991268</td>\n",
       "      <td>[0.9806451612903225, 0.9838709677419355, 0.9806451612903225, 0.9806451612903225, 0.9838709677419355, 0.9742765273311897, 0.9935691318327974, 0.9903536977491961, 0.9935691318327974, 0.9935691318327...</td>\n",
       "      <td>0.985502</td>\n",
       "      <td>[0.987012987012987, 0.9870550161812298, 0.987012987012987, 0.9854132901134521, 0.9902597402597403, 0.9805825242718447, 0.9919743178170144, 0.9887640449438203, 0.9951690821256038, 0.9903846153846154]</td>\n",
       "      <td>0.988363</td>\n",
       "      <td>[0.7391304347826086, 0.7101449275362319, 0.7246376811594203, 0.7101449275362319, 0.7971014492753623, 0.6811594202898551, 0.7681159420289855, 0.7246376811594203, 0.6956521739130435, 0.7391304347826...</td>\n",
       "      <td>72.898551</td>\n",
       "      <td>3.243917</td>\n",
       "      <td>[0.7741935483870968, 0.7142857142857143, 0.75, 0.7027027027027027, 0.7837837837837838, 0.6666666666666666, 0.78125, 0.7419354838709677, 0.7096774193548387, 0.7]</td>\n",
       "      <td>0.73245</td>\n",
       "      <td>[0.6857142857142857, 0.7142857142857143, 0.6857142857142857, 0.7428571428571429, 0.8285714285714286, 0.7058823529411765, 0.7352941176470589, 0.6764705882352942, 0.6470588235294118, 0.8235294117647...</td>\n",
       "      <td>0.724538</td>\n",
       "      <td>[0.7272727272727272, 0.7142857142857143, 0.7164179104477612, 0.7222222222222223, 0.8055555555555555, 0.6857142857142857, 0.7575757575757576, 0.7076923076923077, 0.6769230769230768, 0.7567567567567...</td>\n",
       "      <td>0.727042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000398</th>\n",
       "      <td>[0.9932885906040269, 0.9932885906040269, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 0.9933333333333333, 1.0, 1.0]</td>\n",
       "      <td>99.731991</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.9864864864864865, 0.9864864864864865, 1.0, 1.0, 1.0, 0.9866666666666667, 1.0, 0.9866666666666667, 1.0, 1.0]</td>\n",
       "      <td>0.994631</td>\n",
       "      <td>[0.9931972789115647, 0.9931972789115647, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 0.9932885906040269, 1.0, 1.0]</td>\n",
       "      <td>0.997297</td>\n",
       "      <td>[0.7647058823529411, 0.6470588235294118, 0.7647058823529411, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.6875, 0.6875, 0.5625, 0.8125]</td>\n",
       "      <td>70.441176</td>\n",
       "      <td>7.062652</td>\n",
       "      <td>[0.7777777777777778, 0.6666666666666666, 1.0, 0.625, 0.8333333333333334, 0.6153846153846154, 0.6363636363636364, 0.6666666666666666, 0.5714285714285714, 1.0]</td>\n",
       "      <td>0.739262</td>\n",
       "      <td>[0.7777777777777778, 0.6666666666666666, 0.5555555555555556, 0.625, 0.625, 1.0, 0.875, 0.75, 0.5, 0.625]</td>\n",
       "      <td>0.7</td>\n",
       "      <td>[0.7777777777777778, 0.6666666666666666, 0.7142857142857143, 0.625, 0.7142857142857143, 0.761904761904762, 0.7368421052631579, 0.7058823529411765, 0.5333333333333333, 0.7692307692307693]</td>\n",
       "      <td>0.700521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000976</th>\n",
       "      <td>[0.992526158445441, 0.9910313901345291, 0.9940209267563528, 0.9970104633781763, 0.9940298507462687, 0.9925373134328358, 0.9895522388059701, 0.991044776119403, 0.9925373134328358, 0.9940298507462687]</td>\n",
       "      <td>99.283203</td>\n",
       "      <td>[0.9969788519637462, 0.9910179640718563, 0.9940298507462687, 1.0, 1.0, 0.9940119760479041, 0.996969696969697, 0.993993993993994, 0.9940119760479041, 0.996996996996997]</td>\n",
       "      <td>0.995801</td>\n",
       "      <td>[0.9880239520958084, 0.9910179640718563, 0.9940298507462687, 0.9940298507462687, 0.9880597014925373, 0.991044776119403, 0.982089552238806, 0.9880597014925373, 0.991044776119403, 0.991044776119403]</td>\n",
       "      <td>0.989844</td>\n",
       "      <td>[0.9924812030075189, 0.9910179640718563, 0.9940298507462687, 0.997005988023952, 0.993993993993994, 0.9925261584454409, 0.9894736842105263, 0.9910179640718564, 0.9925261584454409, 0.9940119760479041]</td>\n",
       "      <td>0.992808</td>\n",
       "      <td>[0.72, 0.68, 0.5866666666666667, 0.7333333333333333, 0.7162162162162162, 0.7027027027027027, 0.7432432432432432, 0.7297297297297297, 0.6621621621621622, 0.6756756756756757]</td>\n",
       "      <td>69.497297</td>\n",
       "      <td>4.421575</td>\n",
       "      <td>[0.6976744186046512, 0.6666666666666666, 0.5652173913043478, 0.7428571428571429, 0.6904761904761905, 0.7027027027027027, 0.8, 0.717948717948718, 0.6875, 0.6666666666666666]</td>\n",
       "      <td>0.693771</td>\n",
       "      <td>[0.7894736842105263, 0.7368421052631579, 0.7027027027027027, 0.7027027027027027, 0.7837837837837838, 0.7027027027027027, 0.6486486486486487, 0.7567567567567568, 0.5945945945945946, 0.7027027027027...</td>\n",
       "      <td>0.712091</td>\n",
       "      <td>[0.7407407407407408, 0.7, 0.6265060240963856, 0.7222222222222223, 0.7341772151898734, 0.7027027027027027, 0.7164179104477612, 0.736842105263158, 0.6376811594202898, 0.6842105263157895]</td>\n",
       "      <td>0.70015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0090502</th>\n",
       "      <td>[1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0]</td>\n",
       "      <td>99.759036</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767441860465116, 1.0, 1.0]</td>\n",
       "      <td>0.997674</td>\n",
       "      <td>[1.0, 1.0, 0.975609756097561, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.997561</td>\n",
       "      <td>[1.0, 1.0, 0.9876543209876543, 1.0, 1.0, 1.0, 1.0, 0.988235294117647, 1.0, 1.0]</td>\n",
       "      <td>0.997589</td>\n",
       "      <td>[0.8, 0.6, 0.8888888888888888, 0.7777777777777778, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.5555555555555556, 0.7777777777777778]</td>\n",
       "      <td>65.111111</td>\n",
       "      <td>13.879552</td>\n",
       "      <td>[0.8, 0.6, 1.0, 0.7142857142857143, 0.6666666666666666, 0.6, 0.5, 0.4, 0.5, 0.6666666666666666]</td>\n",
       "      <td>0.644762</td>\n",
       "      <td>[0.8, 0.6, 0.8, 1.0, 0.4, 0.6, 0.5, 0.5, 0.25, 1.0]</td>\n",
       "      <td>0.645</td>\n",
       "      <td>[0.8000000000000002, 0.6, 0.888888888888889, 0.8333333333333333, 0.5, 0.6, 0.5, 0.4444444444444445, 0.3333333333333333, 0.8]</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0098869</th>\n",
       "      <td>[0.9855072463768116, 1.0, 1.0, 1.0, 0.9856115107913669, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.711188</td>\n",
       "      <td>[0.9855072463768116, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>[0.9855072463768116, 1.0, 1.0, 1.0, 0.9710144927536232, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.995652</td>\n",
       "      <td>[0.9855072463768116, 1.0, 1.0, 1.0, 0.9852941176470589, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.99708</td>\n",
       "      <td>[0.5, 0.5, 0.5625, 0.5625, 0.5333333333333333, 0.5333333333333333, 0.4666666666666667, 0.5333333333333333, 0.7333333333333333, 0.7333333333333333]</td>\n",
       "      <td>56.583333</td>\n",
       "      <td>8.820746</td>\n",
       "      <td>[0.5, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6, 0.5714285714285714, 0.5, 0.5, 0.7142857142857143, 0.6666666666666666]</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>[0.625, 0.5, 0.25, 0.25, 0.375, 0.5, 0.5, 0.42857142857142855, 0.7142857142857143, 0.8571428571428571]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.5555555555555556, 0.5, 0.36363636363636365, 0.36363636363636365, 0.4615384615384615, 0.5333333333333333, 0.5, 0.4615384615384615, 0.7142857142857143, 0.75]</td>\n",
       "      <td>0.520352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0099503</th>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9925925925925926, 0.9925925925925926, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.851852</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9850746268656716, 0.9850746268656716, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.997015</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9924812030075187, 0.9924812030075187, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998496</td>\n",
       "      <td>[0.5333333333333333, 0.6, 0.5333333333333333, 0.6, 0.4, 0.5333333333333333, 0.4666666666666667, 0.6, 0.6, 0.3333333333333333]</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.844333</td>\n",
       "      <td>[0.5555555555555556, 0.6666666666666666, 0.6666666666666666, 0.625, 0.42857142857142855, 0.5, 0.4, 0.5555555555555556, 0.6, 0.3333333333333333]</td>\n",
       "      <td>0.533135</td>\n",
       "      <td>[0.625, 0.5, 0.25, 0.625, 0.375, 0.42857142857142855, 0.2857142857142857, 0.7142857142857143, 0.42857142857142855, 0.42857142857142855]</td>\n",
       "      <td>0.466071</td>\n",
       "      <td>[0.5882352941176471, 0.5714285714285715, 0.36363636363636365, 0.625, 0.39999999999999997, 0.4615384615384615, 0.3333333333333333, 0.6250000000000001, 0.5, 0.375]</td>\n",
       "      <td>0.484317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0106310</th>\n",
       "      <td>[0.9933993399339934, 0.9966996699669967, 0.9933993399339934, 0.9983498349834984, 0.9950576606260296, 0.9950576606260296, 0.9917627677100495, 0.9934102141680395, 0.9967051070840197, 0.9917627677100...</td>\n",
       "      <td>99.456044</td>\n",
       "      <td>[0.9933993399339934, 0.9966996699669967, 1.0, 1.0, 1.0, 1.0, 0.9901315789473685, 0.9966887417218543, 1.0, 0.9966777408637874]</td>\n",
       "      <td>0.99736</td>\n",
       "      <td>[0.9933993399339934, 0.9966996699669967, 0.9867986798679867, 0.9966996699669967, 0.9900990099009901, 0.9900990099009901, 0.9933993399339934, 0.9901315789473685, 0.993421052631579, 0.9868421052631579]</td>\n",
       "      <td>0.991759</td>\n",
       "      <td>[0.9933993399339934, 0.9966996699669967, 0.9933554817275748, 0.9983471074380166, 0.9950248756218906, 0.9950248756218906, 0.9917627677100495, 0.9933993399339934, 0.9966996699669968, 0.9917355371900...</td>\n",
       "      <td>0.994545</td>\n",
       "      <td>[0.6323529411764706, 0.6911764705882353, 0.5588235294117647, 0.6470588235294118, 0.6268656716417911, 0.5074626865671642, 0.5223880597014925, 0.6119402985074627, 0.5970149253731343, 0.6716417910447...</td>\n",
       "      <td>60.667252</td>\n",
       "      <td>5.780764</td>\n",
       "      <td>[0.6363636363636364, 0.6666666666666666, 0.5526315789473685, 0.6785714285714286, 0.6363636363636364, 0.5142857142857142, 0.5238095238095238, 0.6129032258064516, 0.5882352941176471, 0.6666666666666...</td>\n",
       "      <td>0.60765</td>\n",
       "      <td>[0.6176470588235294, 0.7647058823529411, 0.6176470588235294, 0.5588235294117647, 0.6176470588235294, 0.5294117647058824, 0.6470588235294118, 0.5757575757575758, 0.6060606060606061, 0.6666666666666...</td>\n",
       "      <td>0.620143</td>\n",
       "      <td>[0.6268656716417911, 0.7123287671232877, 0.5833333333333334, 0.6129032258064516, 0.6268656716417911, 0.5217391304347826, 0.5789473684210527, 0.59375, 0.5970149253731343, 0.6666666666666666]</td>\n",
       "      <td>0.612041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0110165</th>\n",
       "      <td>[0.976, 0.98, 0.988, 0.984, 0.988, 0.984, 1.0, 0.988, 0.9840637450199203, 0.9880478087649402]</td>\n",
       "      <td>98.601116</td>\n",
       "      <td>[0.9541984732824428, 0.9615384615384616, 0.9919354838709677, 0.9763779527559056, 0.9765625, 0.9689922480620154, 1.0, 0.9765625, 0.9763779527559056, 0.984251968503937]</td>\n",
       "      <td>0.97668</td>\n",
       "      <td>[1.0, 1.0, 0.984, 0.992, 1.0, 1.0, 1.0, 1.0, 0.992, 0.9920634920634921]</td>\n",
       "      <td>0.996006</td>\n",
       "      <td>[0.9765625, 0.9803921568627451, 0.9879518072289156, 0.9841269841269842, 0.9881422924901185, 0.9842519685039369, 1.0, 0.9881422924901185, 0.9841269841269842, 0.9881422924901185]</td>\n",
       "      <td>0.986184</td>\n",
       "      <td>[0.6428571428571429, 0.42857142857142855, 0.5357142857142857, 0.5, 0.4642857142857143, 0.5, 0.5714285714285714, 0.4642857142857143, 0.37037037037037035, 0.5555555555555556]</td>\n",
       "      <td>50.330688</td>\n",
       "      <td>7.363402</td>\n",
       "      <td>[0.625, 0.4166666666666667, 0.5555555555555556, 0.5, 0.47058823529411764, 0.5, 0.5714285714285714, 0.4666666666666667, 0.4117647058823529, 0.5454545454545454]</td>\n",
       "      <td>0.506312</td>\n",
       "      <td>[0.7142857142857143, 0.35714285714285715, 0.35714285714285715, 0.35714285714285715, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5, 0.5, 0.46153846153846156]</td>\n",
       "      <td>0.496154</td>\n",
       "      <td>[0.6666666666666666, 0.3846153846153846, 0.43478260869565216, 0.41666666666666663, 0.5161290322580646, 0.5333333333333333, 0.5714285714285714, 0.4827586206896552, 0.45161290322580644, 0.4999999999...</td>\n",
       "      <td>0.495799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                           Training Accuracy scores   \n",
       "GO:0000166   [0.9509761388286334, 0.9596529284164859, 0.9648742411101474, 0.9570685169124025, 0.9570685169124025, 0.9549002601908065, 0.9522983521248916, 0.9588031222896791, 0.9549002601908065, 0.95836947094536]  \\\n",
       "GO:0000287                                                                                                                                        [1.0, 1.0, 0.9930555555555556, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000325  [0.9871175523349437, 0.9871175523349437, 0.9871175523349437, 0.9855072463768116, 0.9903381642512077, 0.9806763285024155, 0.9919484702093397, 0.9887278582930756, 0.9951690821256038, 0.9903381642512...   \n",
       "GO:0000398                                                                                           [0.9932885906040269, 0.9932885906040269, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 0.9933333333333333, 1.0, 1.0]   \n",
       "GO:0000976   [0.992526158445441, 0.9910313901345291, 0.9940209267563528, 0.9970104633781763, 0.9940298507462687, 0.9925373134328358, 0.9895522388059701, 0.991044776119403, 0.9925373134328358, 0.9940298507462687]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                         [1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                         [0.9855072463768116, 1.0, 1.0, 1.0, 0.9856115107913669, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                         [1.0, 1.0, 1.0, 0.9925925925925926, 0.9925925925925926, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.9933993399339934, 0.9966996699669967, 0.9933993399339934, 0.9983498349834984, 0.9950576606260296, 0.9950576606260296, 0.9917627677100495, 0.9934102141680395, 0.9967051070840197, 0.9917627677100...   \n",
       "GO:0110165                                                                                                            [0.976, 0.98, 0.988, 0.984, 0.988, 0.984, 1.0, 0.988, 0.9840637450199203, 0.9880478087649402]   \n",
       "\n",
       "           Mean Training Accuracy   \n",
       "GO:0000166              95.689118  \\\n",
       "GO:0000287              99.930556   \n",
       "GO:0000325               98.84058   \n",
       "GO:0000398              99.731991   \n",
       "GO:0000976              99.283203   \n",
       "...                           ...   \n",
       "GO:0090502              99.759036   \n",
       "GO:0098869              99.711188   \n",
       "GO:0099503              99.851852   \n",
       "GO:0106310              99.456044   \n",
       "GO:0110165              98.601116   \n",
       "\n",
       "                                                                                                                                                                                         Training Precision scores   \n",
       "GO:0000166  [0.9545056867891514, 0.9715302491103203, 0.972663139329806, 0.9647266313932981, 0.9639084507042254, 0.961301671064204, 0.9461077844311377, 0.9640350877192982, 0.9704035874439462, 0.9714540588760036]  \\\n",
       "GO:0000287                                                                                                                                                      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000325  [0.9934640522875817, 0.9902597402597403, 0.9934640522875817, 0.990228013029316, 0.9967320261437909, 0.9869706840390879, 0.9903846153846154, 0.9871794871794872, 0.9967741935483871, 0.987220447284345]   \n",
       "GO:0000398                                                                                                                                                      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000976                                 [0.9969788519637462, 0.9910179640718563, 0.9940298507462687, 1.0, 1.0, 0.9940119760479041, 0.996969696969697, 0.993993993993994, 0.9940119760479041, 0.996996996996997]   \n",
       "...                                                                                                                                                                                                            ...   \n",
       "GO:0090502                                                                                                                                       [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767441860465116, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                                       [0.9855072463768116, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                                                      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310                                                                           [0.9933993399339934, 0.9966996699669967, 1.0, 1.0, 1.0, 1.0, 0.9901315789473685, 0.9966887417218543, 1.0, 0.9966777408637874]   \n",
       "GO:0110165                                  [0.9541984732824428, 0.9615384615384616, 0.9919354838709677, 0.9763779527559056, 0.9765625, 0.9689922480620154, 1.0, 0.9765625, 0.9763779527559056, 0.984251968503937]   \n",
       "\n",
       "           Mean Training Precision   \n",
       "GO:0000166                0.964064  \\\n",
       "GO:0000287                     1.0   \n",
       "GO:0000325                0.991268   \n",
       "GO:0000398                     1.0   \n",
       "GO:0000976                0.995801   \n",
       "...                            ...   \n",
       "GO:0090502                0.997674   \n",
       "GO:0098869                0.998551   \n",
       "GO:0099503                     1.0   \n",
       "GO:0106310                 0.99736   \n",
       "GO:0110165                 0.97668   \n",
       "\n",
       "                                                                                                                                                                                             Training Recall scores   \n",
       "GO:0000166  [0.9470486111111112, 0.9470945359930616, 0.9566348655680833, 0.9488291413703382, 0.9496964440589766, 0.9479618386816999, 0.9592367736339983, 0.9531656548135299, 0.9384215091066782, 0.9444926279271...  \\\n",
       "GO:0000287                                                                                                                                        [1.0, 1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000325  [0.9806451612903225, 0.9838709677419355, 0.9806451612903225, 0.9806451612903225, 0.9838709677419355, 0.9742765273311897, 0.9935691318327974, 0.9903536977491961, 0.9935691318327974, 0.9935691318327...   \n",
       "GO:0000398                                                                                           [0.9864864864864865, 0.9864864864864865, 1.0, 1.0, 1.0, 0.9866666666666667, 1.0, 0.9866666666666667, 1.0, 1.0]   \n",
       "GO:0000976     [0.9880239520958084, 0.9910179640718563, 0.9940298507462687, 0.9940298507462687, 0.9880597014925373, 0.991044776119403, 0.982089552238806, 0.9880597014925373, 0.991044776119403, 0.991044776119403]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                         [1.0, 1.0, 0.975609756097561, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                         [0.9855072463768116, 1.0, 1.0, 1.0, 0.9710144927536232, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                         [1.0, 1.0, 1.0, 0.9850746268656716, 0.9850746268656716, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.9933993399339934, 0.9966996699669967, 0.9867986798679867, 0.9966996699669967, 0.9900990099009901, 0.9900990099009901, 0.9933993399339934, 0.9901315789473685, 0.993421052631579, 0.9868421052631579]   \n",
       "GO:0110165                                                                                                                                  [1.0, 1.0, 0.984, 0.992, 1.0, 1.0, 1.0, 1.0, 0.992, 0.9920634920634921]   \n",
       "\n",
       "           Mean Training Recall   \n",
       "GO:0000166             0.949258  \\\n",
       "GO:0000287             0.998611   \n",
       "GO:0000325             0.985502   \n",
       "GO:0000398             0.994631   \n",
       "GO:0000976             0.989844   \n",
       "...                         ...   \n",
       "GO:0090502             0.997561   \n",
       "GO:0098869             0.995652   \n",
       "GO:0099503             0.997015   \n",
       "GO:0106310             0.991759   \n",
       "GO:0110165             0.996006   \n",
       "\n",
       "                                                                                                                                                                                                 Training F1 scores   \n",
       "GO:0000166  [0.9507625272331154, 0.9591567852437418, 0.964582422387407, 0.9567118495846088, 0.9567496723460026, 0.9545851528384279, 0.9526270456503014, 0.9585695595290012, 0.9541446208112875, 0.9577836411609498]  \\\n",
       "GO:0000287                                                                                                                                         [1.0, 1.0, 0.993006993006993, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000325   [0.987012987012987, 0.9870550161812298, 0.987012987012987, 0.9854132901134521, 0.9902597402597403, 0.9805825242718447, 0.9919743178170144, 0.9887640449438203, 0.9951690821256038, 0.9903846153846154]   \n",
       "GO:0000398                                                                                           [0.9931972789115647, 0.9931972789115647, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 0.9932885906040269, 1.0, 1.0]   \n",
       "GO:0000976   [0.9924812030075189, 0.9910179640718563, 0.9940298507462687, 0.997005988023952, 0.993993993993994, 0.9925261584454409, 0.9894736842105263, 0.9910179640718564, 0.9925261584454409, 0.9940119760479041]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                          [1.0, 1.0, 0.9876543209876543, 1.0, 1.0, 1.0, 1.0, 0.988235294117647, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                         [0.9855072463768116, 1.0, 1.0, 1.0, 0.9852941176470589, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                         [1.0, 1.0, 1.0, 0.9924812030075187, 0.9924812030075187, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.9933993399339934, 0.9966996699669967, 0.9933554817275748, 0.9983471074380166, 0.9950248756218906, 0.9950248756218906, 0.9917627677100495, 0.9933993399339934, 0.9966996699669968, 0.9917355371900...   \n",
       "GO:0110165                         [0.9765625, 0.9803921568627451, 0.9879518072289156, 0.9841269841269842, 0.9881422924901185, 0.9842519685039369, 1.0, 0.9881422924901185, 0.9841269841269842, 0.9881422924901185]   \n",
       "\n",
       "           Mean Training F1 Score   \n",
       "GO:0000166               0.956567  \\\n",
       "GO:0000287               0.999301   \n",
       "GO:0000325               0.988363   \n",
       "GO:0000398               0.997297   \n",
       "GO:0000976               0.992808   \n",
       "...                           ...   \n",
       "GO:0090502               0.997589   \n",
       "GO:0098869                0.99708   \n",
       "GO:0099503               0.998496   \n",
       "GO:0106310               0.994545   \n",
       "GO:0110165               0.986184   \n",
       "\n",
       "                                                                                                                                                                                         Validation Accuracy scores   \n",
       "GO:0000166                                                                                  [0.556420233463035, 0.5603112840466926, 0.62109375, 0.5625, 0.625, 0.54296875, 0.5546875, 0.5625, 0.546875, 0.55859375]  \\\n",
       "GO:0000287                                                                                                                               [0.625, 0.4375, 0.6875, 0.625, 0.4375, 0.6875, 0.5, 0.5625, 0.625, 0.5625]   \n",
       "GO:0000325  [0.7391304347826086, 0.7101449275362319, 0.7246376811594203, 0.7101449275362319, 0.7971014492753623, 0.6811594202898551, 0.7681159420289855, 0.7246376811594203, 0.6956521739130435, 0.7391304347826...   \n",
       "GO:0000398                                                 [0.7647058823529411, 0.6470588235294118, 0.7647058823529411, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.6875, 0.6875, 0.5625, 0.8125]   \n",
       "GO:0000976                             [0.72, 0.68, 0.5866666666666667, 0.7333333333333333, 0.7162162162162162, 0.7027027027027027, 0.7432432432432432, 0.7297297297297297, 0.6621621621621622, 0.6756756756756757]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                               [0.8, 0.6, 0.8888888888888888, 0.7777777777777778, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.5555555555555556, 0.7777777777777778]   \n",
       "GO:0098869                                                       [0.5, 0.5, 0.5625, 0.5625, 0.5333333333333333, 0.5333333333333333, 0.4666666666666667, 0.5333333333333333, 0.7333333333333333, 0.7333333333333333]   \n",
       "GO:0099503                                                                            [0.5333333333333333, 0.6, 0.5333333333333333, 0.6, 0.4, 0.5333333333333333, 0.4666666666666667, 0.6, 0.6, 0.3333333333333333]   \n",
       "GO:0106310  [0.6323529411764706, 0.6911764705882353, 0.5588235294117647, 0.6470588235294118, 0.6268656716417911, 0.5074626865671642, 0.5223880597014925, 0.6119402985074627, 0.5970149253731343, 0.6716417910447...   \n",
       "GO:0110165                             [0.6428571428571429, 0.42857142857142855, 0.5357142857142857, 0.5, 0.4642857142857143, 0.5, 0.5714285714285714, 0.4642857142857143, 0.37037037037037035, 0.5555555555555556]   \n",
       "\n",
       "           Mean Validation Accuracy Std Validation Accuracy   \n",
       "GO:0000166                56.909503                2.765536  \\\n",
       "GO:0000287                     57.5                    8.75   \n",
       "GO:0000325                72.898551                3.243917   \n",
       "GO:0000398                70.441176                7.062652   \n",
       "GO:0000976                69.497297                4.421575   \n",
       "...                             ...                     ...   \n",
       "GO:0090502                65.111111               13.879552   \n",
       "GO:0098869                56.583333                8.820746   \n",
       "GO:0099503                     52.0                8.844333   \n",
       "GO:0106310                60.667252                5.780764   \n",
       "GO:0110165                50.330688                7.363402   \n",
       "\n",
       "                                                                                                                                                                                        Validation Precision scores   \n",
       "GO:0000166    [0.556390977443609, 0.556390977443609, 0.6165413533834586, 0.5563380281690141, 0.6194029850746269, 0.5426356589147286, 0.547945205479452, 0.5526315789473685, 0.5441176470588235, 0.5555555555555556]  \\\n",
       "GO:0000287                                                        [0.625, 0.4, 0.6666666666666666, 0.625, 0.42857142857142855, 0.7142857142857143, 0.5, 0.5555555555555556, 0.6666666666666666, 0.5714285714285714]   \n",
       "GO:0000325                                         [0.7741935483870968, 0.7142857142857143, 0.75, 0.7027027027027027, 0.7837837837837838, 0.6666666666666666, 0.78125, 0.7419354838709677, 0.7096774193548387, 0.7]   \n",
       "GO:0000398                                            [0.7777777777777778, 0.6666666666666666, 1.0, 0.625, 0.8333333333333334, 0.6153846153846154, 0.6363636363636364, 0.6666666666666666, 0.5714285714285714, 1.0]   \n",
       "GO:0000976                             [0.6976744186046512, 0.6666666666666666, 0.5652173913043478, 0.7428571428571429, 0.6904761904761905, 0.7027027027027027, 0.8, 0.717948717948718, 0.6875, 0.6666666666666666]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                          [0.8, 0.6, 1.0, 0.7142857142857143, 0.6666666666666666, 0.6, 0.5, 0.4, 0.5, 0.6666666666666666]   \n",
       "GO:0098869                                                                            [0.5, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6, 0.5714285714285714, 0.5, 0.5, 0.7142857142857143, 0.6666666666666666]   \n",
       "GO:0099503                                                          [0.5555555555555556, 0.6666666666666666, 0.6666666666666666, 0.625, 0.42857142857142855, 0.5, 0.4, 0.5555555555555556, 0.6, 0.3333333333333333]   \n",
       "GO:0106310  [0.6363636363636364, 0.6666666666666666, 0.5526315789473685, 0.6785714285714286, 0.6363636363636364, 0.5142857142857142, 0.5238095238095238, 0.6129032258064516, 0.5882352941176471, 0.6666666666666...   \n",
       "GO:0110165                                           [0.625, 0.4166666666666667, 0.5555555555555556, 0.5, 0.47058823529411764, 0.5, 0.5714285714285714, 0.4666666666666667, 0.4117647058823529, 0.5454545454545454]   \n",
       "\n",
       "           Mean Validation Precision   \n",
       "GO:0000166                  0.564795  \\\n",
       "GO:0000287                  0.575317   \n",
       "GO:0000325                   0.73245   \n",
       "GO:0000398                  0.739262   \n",
       "GO:0000976                  0.693771   \n",
       "...                              ...   \n",
       "GO:0090502                  0.644762   \n",
       "GO:0098869                  0.588571   \n",
       "GO:0099503                  0.533135   \n",
       "GO:0106310                   0.60765   \n",
       "GO:0110165                  0.506312   \n",
       "\n",
       "                                                                                                                                                                                           Validation Recall scores   \n",
       "GO:0000166                                                                                            [0.5736434108527132, 0.578125, 0.640625, 0.6171875, 0.6484375, 0.546875, 0.625, 0.65625, 0.578125, 0.5859375]  \\\n",
       "GO:0000287                                                                                                                                           [0.625, 0.25, 0.75, 0.625, 0.375, 0.625, 0.5, 0.625, 0.5, 0.5]   \n",
       "GO:0000325  [0.6857142857142857, 0.7142857142857143, 0.6857142857142857, 0.7428571428571429, 0.8285714285714286, 0.7058823529411765, 0.7352941176470589, 0.6764705882352942, 0.6470588235294118, 0.8235294117647...   \n",
       "GO:0000398                                                                                                 [0.7777777777777778, 0.6666666666666666, 0.5555555555555556, 0.625, 0.625, 1.0, 0.875, 0.75, 0.5, 0.625]   \n",
       "GO:0000976  [0.7894736842105263, 0.7368421052631579, 0.7027027027027027, 0.7027027027027027, 0.7837837837837838, 0.7027027027027027, 0.6486486486486487, 0.7567567567567568, 0.5945945945945946, 0.7027027027027...   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                                      [0.8, 0.6, 0.8, 1.0, 0.4, 0.6, 0.5, 0.5, 0.25, 1.0]   \n",
       "GO:0098869                                                                                                   [0.625, 0.5, 0.25, 0.25, 0.375, 0.5, 0.5, 0.42857142857142855, 0.7142857142857143, 0.8571428571428571]   \n",
       "GO:0099503                                                                  [0.625, 0.5, 0.25, 0.625, 0.375, 0.42857142857142855, 0.2857142857142857, 0.7142857142857143, 0.42857142857142855, 0.42857142857142855]   \n",
       "GO:0106310  [0.6176470588235294, 0.7647058823529411, 0.6176470588235294, 0.5588235294117647, 0.6176470588235294, 0.5294117647058824, 0.6470588235294118, 0.5757575757575758, 0.6060606060606061, 0.6666666666666...   \n",
       "GO:0110165                           [0.7142857142857143, 0.35714285714285715, 0.35714285714285715, 0.35714285714285715, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5, 0.5, 0.46153846153846156]   \n",
       "\n",
       "           Mean Validation Recall   \n",
       "GO:0000166               0.605021  \\\n",
       "GO:0000287                 0.5375   \n",
       "GO:0000325               0.724538   \n",
       "GO:0000398                    0.7   \n",
       "GO:0000976               0.712091   \n",
       "...                           ...   \n",
       "GO:0090502                  0.645   \n",
       "GO:0098869                    0.5   \n",
       "GO:0099503               0.466071   \n",
       "GO:0106310               0.620143   \n",
       "GO:0110165               0.496154   \n",
       "\n",
       "                                                                                                                                                                                               Validation F1 scores   \n",
       "GO:0000166                    [0.564885496183206, 0.5670498084291187, 0.6283524904214559, 0.5851851851851853, 0.633587786259542, 0.5447470817120622, 0.583941605839416, 0.6, 0.5606060606060606, 0.570342205323194]  \\\n",
       "GO:0000287                                         [0.625, 0.3076923076923077, 0.7058823529411765, 0.625, 0.39999999999999997, 0.6666666666666666, 0.5, 0.5882352941176471, 0.5714285714285715, 0.5333333333333333]   \n",
       "GO:0000325  [0.7272727272727272, 0.7142857142857143, 0.7164179104477612, 0.7222222222222223, 0.8055555555555555, 0.6857142857142857, 0.7575757575757576, 0.7076923076923077, 0.6769230769230768, 0.7567567567567...   \n",
       "GO:0000398               [0.7777777777777778, 0.6666666666666666, 0.7142857142857143, 0.625, 0.7142857142857143, 0.761904761904762, 0.7368421052631579, 0.7058823529411765, 0.5333333333333333, 0.7692307692307693]   \n",
       "GO:0000976                 [0.7407407407407408, 0.7, 0.6265060240963856, 0.7222222222222223, 0.7341772151898734, 0.7027027027027027, 0.7164179104477612, 0.736842105263158, 0.6376811594202898, 0.6842105263157895]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                             [0.8000000000000002, 0.6, 0.888888888888889, 0.8333333333333333, 0.5, 0.6, 0.5, 0.4444444444444445, 0.3333333333333333, 0.8]   \n",
       "GO:0098869                                           [0.5555555555555556, 0.5, 0.36363636363636365, 0.36363636363636365, 0.4615384615384615, 0.5333333333333333, 0.5, 0.4615384615384615, 0.7142857142857143, 0.75]   \n",
       "GO:0099503                                        [0.5882352941176471, 0.5714285714285715, 0.36363636363636365, 0.625, 0.39999999999999997, 0.4615384615384615, 0.3333333333333333, 0.6250000000000001, 0.5, 0.375]   \n",
       "GO:0106310            [0.6268656716417911, 0.7123287671232877, 0.5833333333333334, 0.6129032258064516, 0.6268656716417911, 0.5217391304347826, 0.5789473684210527, 0.59375, 0.5970149253731343, 0.6666666666666666]   \n",
       "GO:0110165  [0.6666666666666666, 0.3846153846153846, 0.43478260869565216, 0.41666666666666663, 0.5161290322580646, 0.5333333333333333, 0.5714285714285714, 0.4827586206896552, 0.45161290322580644, 0.4999999999...   \n",
       "\n",
       "           Mean Validation F1 Score  \n",
       "GO:0000166                  0.58387  \n",
       "GO:0000287                 0.552324  \n",
       "GO:0000325                 0.727042  \n",
       "GO:0000398                 0.700521  \n",
       "GO:0000976                  0.70015  \n",
       "...                             ...  \n",
       "GO:0090502                     0.63  \n",
       "GO:0098869                 0.520352  \n",
       "GO:0099503                 0.484317  \n",
       "GO:0106310                 0.612041  \n",
       "GO:0110165                 0.495799  \n",
       "\n",
       "[227 rows x 17 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CB_Root33_results_df = pd.DataFrame(CB_Root33_results_dict).T\n",
    "csv_file_path = '22Jan2024_DAPSeq0.5_Root33_results_df.csv'\n",
    "CB_Root33_results_df.to_csv(csv_file_path, index_label='GO')\n",
    "CB_Root33_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c693cb-2c18-45f6-87fa-fd092d27f87e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy scores</th>\n",
       "      <th>Mean Training Accuracy</th>\n",
       "      <th>Training Precision scores</th>\n",
       "      <th>Mean Training Precision</th>\n",
       "      <th>Training Recall scores</th>\n",
       "      <th>Mean Training Recall</th>\n",
       "      <th>Training F1 scores</th>\n",
       "      <th>Mean Training F1 Score</th>\n",
       "      <th>Validation Accuracy scores</th>\n",
       "      <th>Mean Validation Accuracy</th>\n",
       "      <th>Std Validation Accuracy</th>\n",
       "      <th>Validation Precision scores</th>\n",
       "      <th>Mean Validation Precision</th>\n",
       "      <th>Validation Recall scores</th>\n",
       "      <th>Mean Validation Recall</th>\n",
       "      <th>Validation F1 scores</th>\n",
       "      <th>Mean Validation F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0000166</th>\n",
       "      <td>[0.9509761388286334, 0.9596529284164859, 0.9648742411101474, 0.9570685169124025, 0.9570685169124025, 0.9549002601908065, 0.9522983521248916, 0.9588031222896791, 0.9549002601908065, 0.95836947094536]</td>\n",
       "      <td>95.689118</td>\n",
       "      <td>[0.9545056867891514, 0.9715302491103203, 0.972663139329806, 0.9647266313932981, 0.9639084507042254, 0.961301671064204, 0.9461077844311377, 0.9640350877192982, 0.9704035874439462, 0.9714540588760036]</td>\n",
       "      <td>0.964064</td>\n",
       "      <td>[0.9470486111111112, 0.9470945359930616, 0.9566348655680833, 0.9488291413703382, 0.9496964440589766, 0.9479618386816999, 0.9592367736339983, 0.9531656548135299, 0.9384215091066782, 0.9444926279271...</td>\n",
       "      <td>0.949258</td>\n",
       "      <td>[0.9507625272331154, 0.9591567852437418, 0.964582422387407, 0.9567118495846088, 0.9567496723460026, 0.9545851528384279, 0.9526270456503014, 0.9585695595290012, 0.9541446208112875, 0.9577836411609498]</td>\n",
       "      <td>0.956567</td>\n",
       "      <td>[0.556420233463035, 0.5603112840466926, 0.62109375, 0.5625, 0.625, 0.54296875, 0.5546875, 0.5625, 0.546875, 0.55859375]</td>\n",
       "      <td>56.909503</td>\n",
       "      <td>2.765536</td>\n",
       "      <td>[0.556390977443609, 0.556390977443609, 0.6165413533834586, 0.5563380281690141, 0.6194029850746269, 0.5426356589147286, 0.547945205479452, 0.5526315789473685, 0.5441176470588235, 0.5555555555555556]</td>\n",
       "      <td>0.564795</td>\n",
       "      <td>[0.5736434108527132, 0.578125, 0.640625, 0.6171875, 0.6484375, 0.546875, 0.625, 0.65625, 0.578125, 0.5859375]</td>\n",
       "      <td>0.605021</td>\n",
       "      <td>[0.564885496183206, 0.5670498084291187, 0.6283524904214559, 0.5851851851851853, 0.633587786259542, 0.5447470817120622, 0.583941605839416, 0.6, 0.5606060606060606, 0.570342205323194]</td>\n",
       "      <td>0.58387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000287</th>\n",
       "      <td>[1.0, 1.0, 0.9930555555555556, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.930556</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0, 1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998611</td>\n",
       "      <td>[1.0, 1.0, 0.993006993006993, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.999301</td>\n",
       "      <td>[0.625, 0.4375, 0.6875, 0.625, 0.4375, 0.6875, 0.5, 0.5625, 0.625, 0.5625]</td>\n",
       "      <td>57.5</td>\n",
       "      <td>8.75</td>\n",
       "      <td>[0.625, 0.4, 0.6666666666666666, 0.625, 0.42857142857142855, 0.7142857142857143, 0.5, 0.5555555555555556, 0.6666666666666666, 0.5714285714285714]</td>\n",
       "      <td>0.575317</td>\n",
       "      <td>[0.625, 0.25, 0.75, 0.625, 0.375, 0.625, 0.5, 0.625, 0.5, 0.5]</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>[0.625, 0.3076923076923077, 0.7058823529411765, 0.625, 0.39999999999999997, 0.6666666666666666, 0.5, 0.5882352941176471, 0.5714285714285715, 0.5333333333333333]</td>\n",
       "      <td>0.552324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000325</th>\n",
       "      <td>[0.9871175523349437, 0.9871175523349437, 0.9871175523349437, 0.9855072463768116, 0.9903381642512077, 0.9806763285024155, 0.9919484702093397, 0.9887278582930756, 0.9951690821256038, 0.9903381642512...</td>\n",
       "      <td>98.84058</td>\n",
       "      <td>[0.9934640522875817, 0.9902597402597403, 0.9934640522875817, 0.990228013029316, 0.9967320261437909, 0.9869706840390879, 0.9903846153846154, 0.9871794871794872, 0.9967741935483871, 0.987220447284345]</td>\n",
       "      <td>0.991268</td>\n",
       "      <td>[0.9806451612903225, 0.9838709677419355, 0.9806451612903225, 0.9806451612903225, 0.9838709677419355, 0.9742765273311897, 0.9935691318327974, 0.9903536977491961, 0.9935691318327974, 0.9935691318327...</td>\n",
       "      <td>0.985502</td>\n",
       "      <td>[0.987012987012987, 0.9870550161812298, 0.987012987012987, 0.9854132901134521, 0.9902597402597403, 0.9805825242718447, 0.9919743178170144, 0.9887640449438203, 0.9951690821256038, 0.9903846153846154]</td>\n",
       "      <td>0.988363</td>\n",
       "      <td>[0.7391304347826086, 0.7101449275362319, 0.7246376811594203, 0.7101449275362319, 0.7971014492753623, 0.6811594202898551, 0.7681159420289855, 0.7246376811594203, 0.6956521739130435, 0.7391304347826...</td>\n",
       "      <td>72.898551</td>\n",
       "      <td>3.243917</td>\n",
       "      <td>[0.7741935483870968, 0.7142857142857143, 0.75, 0.7027027027027027, 0.7837837837837838, 0.6666666666666666, 0.78125, 0.7419354838709677, 0.7096774193548387, 0.7]</td>\n",
       "      <td>0.73245</td>\n",
       "      <td>[0.6857142857142857, 0.7142857142857143, 0.6857142857142857, 0.7428571428571429, 0.8285714285714286, 0.7058823529411765, 0.7352941176470589, 0.6764705882352942, 0.6470588235294118, 0.8235294117647...</td>\n",
       "      <td>0.724538</td>\n",
       "      <td>[0.7272727272727272, 0.7142857142857143, 0.7164179104477612, 0.7222222222222223, 0.8055555555555555, 0.6857142857142857, 0.7575757575757576, 0.7076923076923077, 0.6769230769230768, 0.7567567567567...</td>\n",
       "      <td>0.727042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000398</th>\n",
       "      <td>[0.9932885906040269, 0.9932885906040269, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 0.9933333333333333, 1.0, 1.0]</td>\n",
       "      <td>99.731991</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.9864864864864865, 0.9864864864864865, 1.0, 1.0, 1.0, 0.9866666666666667, 1.0, 0.9866666666666667, 1.0, 1.0]</td>\n",
       "      <td>0.994631</td>\n",
       "      <td>[0.9931972789115647, 0.9931972789115647, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 0.9932885906040269, 1.0, 1.0]</td>\n",
       "      <td>0.997297</td>\n",
       "      <td>[0.7647058823529411, 0.6470588235294118, 0.7647058823529411, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.6875, 0.6875, 0.5625, 0.8125]</td>\n",
       "      <td>70.441176</td>\n",
       "      <td>7.062652</td>\n",
       "      <td>[0.7777777777777778, 0.6666666666666666, 1.0, 0.625, 0.8333333333333334, 0.6153846153846154, 0.6363636363636364, 0.6666666666666666, 0.5714285714285714, 1.0]</td>\n",
       "      <td>0.739262</td>\n",
       "      <td>[0.7777777777777778, 0.6666666666666666, 0.5555555555555556, 0.625, 0.625, 1.0, 0.875, 0.75, 0.5, 0.625]</td>\n",
       "      <td>0.7</td>\n",
       "      <td>[0.7777777777777778, 0.6666666666666666, 0.7142857142857143, 0.625, 0.7142857142857143, 0.761904761904762, 0.7368421052631579, 0.7058823529411765, 0.5333333333333333, 0.7692307692307693]</td>\n",
       "      <td>0.700521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000976</th>\n",
       "      <td>[0.992526158445441, 0.9910313901345291, 0.9940209267563528, 0.9970104633781763, 0.9940298507462687, 0.9925373134328358, 0.9895522388059701, 0.991044776119403, 0.9925373134328358, 0.9940298507462687]</td>\n",
       "      <td>99.283203</td>\n",
       "      <td>[0.9969788519637462, 0.9910179640718563, 0.9940298507462687, 1.0, 1.0, 0.9940119760479041, 0.996969696969697, 0.993993993993994, 0.9940119760479041, 0.996996996996997]</td>\n",
       "      <td>0.995801</td>\n",
       "      <td>[0.9880239520958084, 0.9910179640718563, 0.9940298507462687, 0.9940298507462687, 0.9880597014925373, 0.991044776119403, 0.982089552238806, 0.9880597014925373, 0.991044776119403, 0.991044776119403]</td>\n",
       "      <td>0.989844</td>\n",
       "      <td>[0.9924812030075189, 0.9910179640718563, 0.9940298507462687, 0.997005988023952, 0.993993993993994, 0.9925261584454409, 0.9894736842105263, 0.9910179640718564, 0.9925261584454409, 0.9940119760479041]</td>\n",
       "      <td>0.992808</td>\n",
       "      <td>[0.72, 0.68, 0.5866666666666667, 0.7333333333333333, 0.7162162162162162, 0.7027027027027027, 0.7432432432432432, 0.7297297297297297, 0.6621621621621622, 0.6756756756756757]</td>\n",
       "      <td>69.497297</td>\n",
       "      <td>4.421575</td>\n",
       "      <td>[0.6976744186046512, 0.6666666666666666, 0.5652173913043478, 0.7428571428571429, 0.6904761904761905, 0.7027027027027027, 0.8, 0.717948717948718, 0.6875, 0.6666666666666666]</td>\n",
       "      <td>0.693771</td>\n",
       "      <td>[0.7894736842105263, 0.7368421052631579, 0.7027027027027027, 0.7027027027027027, 0.7837837837837838, 0.7027027027027027, 0.6486486486486487, 0.7567567567567568, 0.5945945945945946, 0.7027027027027...</td>\n",
       "      <td>0.712091</td>\n",
       "      <td>[0.7407407407407408, 0.7, 0.6265060240963856, 0.7222222222222223, 0.7341772151898734, 0.7027027027027027, 0.7164179104477612, 0.736842105263158, 0.6376811594202898, 0.6842105263157895]</td>\n",
       "      <td>0.70015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0090502</th>\n",
       "      <td>[1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0]</td>\n",
       "      <td>99.759036</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767441860465116, 1.0, 1.0]</td>\n",
       "      <td>0.997674</td>\n",
       "      <td>[1.0, 1.0, 0.975609756097561, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.997561</td>\n",
       "      <td>[1.0, 1.0, 0.9876543209876543, 1.0, 1.0, 1.0, 1.0, 0.988235294117647, 1.0, 1.0]</td>\n",
       "      <td>0.997589</td>\n",
       "      <td>[0.8, 0.6, 0.8888888888888888, 0.7777777777777778, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.5555555555555556, 0.7777777777777778]</td>\n",
       "      <td>65.111111</td>\n",
       "      <td>13.879552</td>\n",
       "      <td>[0.8, 0.6, 1.0, 0.7142857142857143, 0.6666666666666666, 0.6, 0.5, 0.4, 0.5, 0.6666666666666666]</td>\n",
       "      <td>0.644762</td>\n",
       "      <td>[0.8, 0.6, 0.8, 1.0, 0.4, 0.6, 0.5, 0.5, 0.25, 1.0]</td>\n",
       "      <td>0.645</td>\n",
       "      <td>[0.8000000000000002, 0.6, 0.888888888888889, 0.8333333333333333, 0.5, 0.6, 0.5, 0.4444444444444445, 0.3333333333333333, 0.8]</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0098869</th>\n",
       "      <td>[0.9855072463768116, 1.0, 1.0, 1.0, 0.9856115107913669, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.711188</td>\n",
       "      <td>[0.9855072463768116, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>[0.9855072463768116, 1.0, 1.0, 1.0, 0.9710144927536232, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.995652</td>\n",
       "      <td>[0.9855072463768116, 1.0, 1.0, 1.0, 0.9852941176470589, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.99708</td>\n",
       "      <td>[0.5, 0.5, 0.5625, 0.5625, 0.5333333333333333, 0.5333333333333333, 0.4666666666666667, 0.5333333333333333, 0.7333333333333333, 0.7333333333333333]</td>\n",
       "      <td>56.583333</td>\n",
       "      <td>8.820746</td>\n",
       "      <td>[0.5, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6, 0.5714285714285714, 0.5, 0.5, 0.7142857142857143, 0.6666666666666666]</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>[0.625, 0.5, 0.25, 0.25, 0.375, 0.5, 0.5, 0.42857142857142855, 0.7142857142857143, 0.8571428571428571]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.5555555555555556, 0.5, 0.36363636363636365, 0.36363636363636365, 0.4615384615384615, 0.5333333333333333, 0.5, 0.4615384615384615, 0.7142857142857143, 0.75]</td>\n",
       "      <td>0.520352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0099503</th>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9925925925925926, 0.9925925925925926, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.851852</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9850746268656716, 0.9850746268656716, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.997015</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9924812030075187, 0.9924812030075187, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998496</td>\n",
       "      <td>[0.5333333333333333, 0.6, 0.5333333333333333, 0.6, 0.4, 0.5333333333333333, 0.4666666666666667, 0.6, 0.6, 0.3333333333333333]</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.844333</td>\n",
       "      <td>[0.5555555555555556, 0.6666666666666666, 0.6666666666666666, 0.625, 0.42857142857142855, 0.5, 0.4, 0.5555555555555556, 0.6, 0.3333333333333333]</td>\n",
       "      <td>0.533135</td>\n",
       "      <td>[0.625, 0.5, 0.25, 0.625, 0.375, 0.42857142857142855, 0.2857142857142857, 0.7142857142857143, 0.42857142857142855, 0.42857142857142855]</td>\n",
       "      <td>0.466071</td>\n",
       "      <td>[0.5882352941176471, 0.5714285714285715, 0.36363636363636365, 0.625, 0.39999999999999997, 0.4615384615384615, 0.3333333333333333, 0.6250000000000001, 0.5, 0.375]</td>\n",
       "      <td>0.484317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0106310</th>\n",
       "      <td>[0.9933993399339934, 0.9966996699669967, 0.9933993399339934, 0.9983498349834984, 0.9950576606260296, 0.9950576606260296, 0.9917627677100495, 0.9934102141680395, 0.9967051070840197, 0.9917627677100...</td>\n",
       "      <td>99.456044</td>\n",
       "      <td>[0.9933993399339934, 0.9966996699669967, 1.0, 1.0, 1.0, 1.0, 0.9901315789473685, 0.9966887417218543, 1.0, 0.9966777408637874]</td>\n",
       "      <td>0.99736</td>\n",
       "      <td>[0.9933993399339934, 0.9966996699669967, 0.9867986798679867, 0.9966996699669967, 0.9900990099009901, 0.9900990099009901, 0.9933993399339934, 0.9901315789473685, 0.993421052631579, 0.9868421052631579]</td>\n",
       "      <td>0.991759</td>\n",
       "      <td>[0.9933993399339934, 0.9966996699669967, 0.9933554817275748, 0.9983471074380166, 0.9950248756218906, 0.9950248756218906, 0.9917627677100495, 0.9933993399339934, 0.9966996699669968, 0.9917355371900...</td>\n",
       "      <td>0.994545</td>\n",
       "      <td>[0.6323529411764706, 0.6911764705882353, 0.5588235294117647, 0.6470588235294118, 0.6268656716417911, 0.5074626865671642, 0.5223880597014925, 0.6119402985074627, 0.5970149253731343, 0.6716417910447...</td>\n",
       "      <td>60.667252</td>\n",
       "      <td>5.780764</td>\n",
       "      <td>[0.6363636363636364, 0.6666666666666666, 0.5526315789473685, 0.6785714285714286, 0.6363636363636364, 0.5142857142857142, 0.5238095238095238, 0.6129032258064516, 0.5882352941176471, 0.6666666666666...</td>\n",
       "      <td>0.60765</td>\n",
       "      <td>[0.6176470588235294, 0.7647058823529411, 0.6176470588235294, 0.5588235294117647, 0.6176470588235294, 0.5294117647058824, 0.6470588235294118, 0.5757575757575758, 0.6060606060606061, 0.6666666666666...</td>\n",
       "      <td>0.620143</td>\n",
       "      <td>[0.6268656716417911, 0.7123287671232877, 0.5833333333333334, 0.6129032258064516, 0.6268656716417911, 0.5217391304347826, 0.5789473684210527, 0.59375, 0.5970149253731343, 0.6666666666666666]</td>\n",
       "      <td>0.612041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0110165</th>\n",
       "      <td>[0.976, 0.98, 0.988, 0.984, 0.988, 0.984, 1.0, 0.988, 0.9840637450199203, 0.9880478087649402]</td>\n",
       "      <td>98.601116</td>\n",
       "      <td>[0.9541984732824428, 0.9615384615384616, 0.9919354838709677, 0.9763779527559056, 0.9765625, 0.9689922480620154, 1.0, 0.9765625, 0.9763779527559056, 0.984251968503937]</td>\n",
       "      <td>0.97668</td>\n",
       "      <td>[1.0, 1.0, 0.984, 0.992, 1.0, 1.0, 1.0, 1.0, 0.992, 0.9920634920634921]</td>\n",
       "      <td>0.996006</td>\n",
       "      <td>[0.9765625, 0.9803921568627451, 0.9879518072289156, 0.9841269841269842, 0.9881422924901185, 0.9842519685039369, 1.0, 0.9881422924901185, 0.9841269841269842, 0.9881422924901185]</td>\n",
       "      <td>0.986184</td>\n",
       "      <td>[0.6428571428571429, 0.42857142857142855, 0.5357142857142857, 0.5, 0.4642857142857143, 0.5, 0.5714285714285714, 0.4642857142857143, 0.37037037037037035, 0.5555555555555556]</td>\n",
       "      <td>50.330688</td>\n",
       "      <td>7.363402</td>\n",
       "      <td>[0.625, 0.4166666666666667, 0.5555555555555556, 0.5, 0.47058823529411764, 0.5, 0.5714285714285714, 0.4666666666666667, 0.4117647058823529, 0.5454545454545454]</td>\n",
       "      <td>0.506312</td>\n",
       "      <td>[0.7142857142857143, 0.35714285714285715, 0.35714285714285715, 0.35714285714285715, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5, 0.5, 0.46153846153846156]</td>\n",
       "      <td>0.496154</td>\n",
       "      <td>[0.6666666666666666, 0.3846153846153846, 0.43478260869565216, 0.41666666666666663, 0.5161290322580646, 0.5333333333333333, 0.5714285714285714, 0.4827586206896552, 0.45161290322580644, 0.4999999999...</td>\n",
       "      <td>0.495799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                           Training Accuracy scores   \n",
       "GO:0000166   [0.9509761388286334, 0.9596529284164859, 0.9648742411101474, 0.9570685169124025, 0.9570685169124025, 0.9549002601908065, 0.9522983521248916, 0.9588031222896791, 0.9549002601908065, 0.95836947094536]  \\\n",
       "GO:0000287                                                                                                                                        [1.0, 1.0, 0.9930555555555556, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000325  [0.9871175523349437, 0.9871175523349437, 0.9871175523349437, 0.9855072463768116, 0.9903381642512077, 0.9806763285024155, 0.9919484702093397, 0.9887278582930756, 0.9951690821256038, 0.9903381642512...   \n",
       "GO:0000398                                                                                           [0.9932885906040269, 0.9932885906040269, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 0.9933333333333333, 1.0, 1.0]   \n",
       "GO:0000976   [0.992526158445441, 0.9910313901345291, 0.9940209267563528, 0.9970104633781763, 0.9940298507462687, 0.9925373134328358, 0.9895522388059701, 0.991044776119403, 0.9925373134328358, 0.9940298507462687]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                         [1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                         [0.9855072463768116, 1.0, 1.0, 1.0, 0.9856115107913669, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                         [1.0, 1.0, 1.0, 0.9925925925925926, 0.9925925925925926, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.9933993399339934, 0.9966996699669967, 0.9933993399339934, 0.9983498349834984, 0.9950576606260296, 0.9950576606260296, 0.9917627677100495, 0.9934102141680395, 0.9967051070840197, 0.9917627677100...   \n",
       "GO:0110165                                                                                                            [0.976, 0.98, 0.988, 0.984, 0.988, 0.984, 1.0, 0.988, 0.9840637450199203, 0.9880478087649402]   \n",
       "\n",
       "           Mean Training Accuracy   \n",
       "GO:0000166              95.689118  \\\n",
       "GO:0000287              99.930556   \n",
       "GO:0000325               98.84058   \n",
       "GO:0000398              99.731991   \n",
       "GO:0000976              99.283203   \n",
       "...                           ...   \n",
       "GO:0090502              99.759036   \n",
       "GO:0098869              99.711188   \n",
       "GO:0099503              99.851852   \n",
       "GO:0106310              99.456044   \n",
       "GO:0110165              98.601116   \n",
       "\n",
       "                                                                                                                                                                                         Training Precision scores   \n",
       "GO:0000166  [0.9545056867891514, 0.9715302491103203, 0.972663139329806, 0.9647266313932981, 0.9639084507042254, 0.961301671064204, 0.9461077844311377, 0.9640350877192982, 0.9704035874439462, 0.9714540588760036]  \\\n",
       "GO:0000287                                                                                                                                                      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000325  [0.9934640522875817, 0.9902597402597403, 0.9934640522875817, 0.990228013029316, 0.9967320261437909, 0.9869706840390879, 0.9903846153846154, 0.9871794871794872, 0.9967741935483871, 0.987220447284345]   \n",
       "GO:0000398                                                                                                                                                      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000976                                 [0.9969788519637462, 0.9910179640718563, 0.9940298507462687, 1.0, 1.0, 0.9940119760479041, 0.996969696969697, 0.993993993993994, 0.9940119760479041, 0.996996996996997]   \n",
       "...                                                                                                                                                                                                            ...   \n",
       "GO:0090502                                                                                                                                       [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767441860465116, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                                       [0.9855072463768116, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                                                      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310                                                                           [0.9933993399339934, 0.9966996699669967, 1.0, 1.0, 1.0, 1.0, 0.9901315789473685, 0.9966887417218543, 1.0, 0.9966777408637874]   \n",
       "GO:0110165                                  [0.9541984732824428, 0.9615384615384616, 0.9919354838709677, 0.9763779527559056, 0.9765625, 0.9689922480620154, 1.0, 0.9765625, 0.9763779527559056, 0.984251968503937]   \n",
       "\n",
       "           Mean Training Precision   \n",
       "GO:0000166                0.964064  \\\n",
       "GO:0000287                     1.0   \n",
       "GO:0000325                0.991268   \n",
       "GO:0000398                     1.0   \n",
       "GO:0000976                0.995801   \n",
       "...                            ...   \n",
       "GO:0090502                0.997674   \n",
       "GO:0098869                0.998551   \n",
       "GO:0099503                     1.0   \n",
       "GO:0106310                 0.99736   \n",
       "GO:0110165                 0.97668   \n",
       "\n",
       "                                                                                                                                                                                             Training Recall scores   \n",
       "GO:0000166  [0.9470486111111112, 0.9470945359930616, 0.9566348655680833, 0.9488291413703382, 0.9496964440589766, 0.9479618386816999, 0.9592367736339983, 0.9531656548135299, 0.9384215091066782, 0.9444926279271...  \\\n",
       "GO:0000287                                                                                                                                        [1.0, 1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000325  [0.9806451612903225, 0.9838709677419355, 0.9806451612903225, 0.9806451612903225, 0.9838709677419355, 0.9742765273311897, 0.9935691318327974, 0.9903536977491961, 0.9935691318327974, 0.9935691318327...   \n",
       "GO:0000398                                                                                           [0.9864864864864865, 0.9864864864864865, 1.0, 1.0, 1.0, 0.9866666666666667, 1.0, 0.9866666666666667, 1.0, 1.0]   \n",
       "GO:0000976     [0.9880239520958084, 0.9910179640718563, 0.9940298507462687, 0.9940298507462687, 0.9880597014925373, 0.991044776119403, 0.982089552238806, 0.9880597014925373, 0.991044776119403, 0.991044776119403]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                         [1.0, 1.0, 0.975609756097561, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                         [0.9855072463768116, 1.0, 1.0, 1.0, 0.9710144927536232, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                         [1.0, 1.0, 1.0, 0.9850746268656716, 0.9850746268656716, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.9933993399339934, 0.9966996699669967, 0.9867986798679867, 0.9966996699669967, 0.9900990099009901, 0.9900990099009901, 0.9933993399339934, 0.9901315789473685, 0.993421052631579, 0.9868421052631579]   \n",
       "GO:0110165                                                                                                                                  [1.0, 1.0, 0.984, 0.992, 1.0, 1.0, 1.0, 1.0, 0.992, 0.9920634920634921]   \n",
       "\n",
       "           Mean Training Recall   \n",
       "GO:0000166             0.949258  \\\n",
       "GO:0000287             0.998611   \n",
       "GO:0000325             0.985502   \n",
       "GO:0000398             0.994631   \n",
       "GO:0000976             0.989844   \n",
       "...                         ...   \n",
       "GO:0090502             0.997561   \n",
       "GO:0098869             0.995652   \n",
       "GO:0099503             0.997015   \n",
       "GO:0106310             0.991759   \n",
       "GO:0110165             0.996006   \n",
       "\n",
       "                                                                                                                                                                                                 Training F1 scores   \n",
       "GO:0000166  [0.9507625272331154, 0.9591567852437418, 0.964582422387407, 0.9567118495846088, 0.9567496723460026, 0.9545851528384279, 0.9526270456503014, 0.9585695595290012, 0.9541446208112875, 0.9577836411609498]  \\\n",
       "GO:0000287                                                                                                                                         [1.0, 1.0, 0.993006993006993, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000325   [0.987012987012987, 0.9870550161812298, 0.987012987012987, 0.9854132901134521, 0.9902597402597403, 0.9805825242718447, 0.9919743178170144, 0.9887640449438203, 0.9951690821256038, 0.9903846153846154]   \n",
       "GO:0000398                                                                                           [0.9931972789115647, 0.9931972789115647, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 0.9932885906040269, 1.0, 1.0]   \n",
       "GO:0000976   [0.9924812030075189, 0.9910179640718563, 0.9940298507462687, 0.997005988023952, 0.993993993993994, 0.9925261584454409, 0.9894736842105263, 0.9910179640718564, 0.9925261584454409, 0.9940119760479041]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                          [1.0, 1.0, 0.9876543209876543, 1.0, 1.0, 1.0, 1.0, 0.988235294117647, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                         [0.9855072463768116, 1.0, 1.0, 1.0, 0.9852941176470589, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                         [1.0, 1.0, 1.0, 0.9924812030075187, 0.9924812030075187, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.9933993399339934, 0.9966996699669967, 0.9933554817275748, 0.9983471074380166, 0.9950248756218906, 0.9950248756218906, 0.9917627677100495, 0.9933993399339934, 0.9966996699669968, 0.9917355371900...   \n",
       "GO:0110165                         [0.9765625, 0.9803921568627451, 0.9879518072289156, 0.9841269841269842, 0.9881422924901185, 0.9842519685039369, 1.0, 0.9881422924901185, 0.9841269841269842, 0.9881422924901185]   \n",
       "\n",
       "           Mean Training F1 Score   \n",
       "GO:0000166               0.956567  \\\n",
       "GO:0000287               0.999301   \n",
       "GO:0000325               0.988363   \n",
       "GO:0000398               0.997297   \n",
       "GO:0000976               0.992808   \n",
       "...                           ...   \n",
       "GO:0090502               0.997589   \n",
       "GO:0098869                0.99708   \n",
       "GO:0099503               0.998496   \n",
       "GO:0106310               0.994545   \n",
       "GO:0110165               0.986184   \n",
       "\n",
       "                                                                                                                                                                                         Validation Accuracy scores   \n",
       "GO:0000166                                                                                  [0.556420233463035, 0.5603112840466926, 0.62109375, 0.5625, 0.625, 0.54296875, 0.5546875, 0.5625, 0.546875, 0.55859375]  \\\n",
       "GO:0000287                                                                                                                               [0.625, 0.4375, 0.6875, 0.625, 0.4375, 0.6875, 0.5, 0.5625, 0.625, 0.5625]   \n",
       "GO:0000325  [0.7391304347826086, 0.7101449275362319, 0.7246376811594203, 0.7101449275362319, 0.7971014492753623, 0.6811594202898551, 0.7681159420289855, 0.7246376811594203, 0.6956521739130435, 0.7391304347826...   \n",
       "GO:0000398                                                 [0.7647058823529411, 0.6470588235294118, 0.7647058823529411, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.6875, 0.6875, 0.5625, 0.8125]   \n",
       "GO:0000976                             [0.72, 0.68, 0.5866666666666667, 0.7333333333333333, 0.7162162162162162, 0.7027027027027027, 0.7432432432432432, 0.7297297297297297, 0.6621621621621622, 0.6756756756756757]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                               [0.8, 0.6, 0.8888888888888888, 0.7777777777777778, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.5555555555555556, 0.7777777777777778]   \n",
       "GO:0098869                                                       [0.5, 0.5, 0.5625, 0.5625, 0.5333333333333333, 0.5333333333333333, 0.4666666666666667, 0.5333333333333333, 0.7333333333333333, 0.7333333333333333]   \n",
       "GO:0099503                                                                            [0.5333333333333333, 0.6, 0.5333333333333333, 0.6, 0.4, 0.5333333333333333, 0.4666666666666667, 0.6, 0.6, 0.3333333333333333]   \n",
       "GO:0106310  [0.6323529411764706, 0.6911764705882353, 0.5588235294117647, 0.6470588235294118, 0.6268656716417911, 0.5074626865671642, 0.5223880597014925, 0.6119402985074627, 0.5970149253731343, 0.6716417910447...   \n",
       "GO:0110165                             [0.6428571428571429, 0.42857142857142855, 0.5357142857142857, 0.5, 0.4642857142857143, 0.5, 0.5714285714285714, 0.4642857142857143, 0.37037037037037035, 0.5555555555555556]   \n",
       "\n",
       "           Mean Validation Accuracy Std Validation Accuracy   \n",
       "GO:0000166                56.909503                2.765536  \\\n",
       "GO:0000287                     57.5                    8.75   \n",
       "GO:0000325                72.898551                3.243917   \n",
       "GO:0000398                70.441176                7.062652   \n",
       "GO:0000976                69.497297                4.421575   \n",
       "...                             ...                     ...   \n",
       "GO:0090502                65.111111               13.879552   \n",
       "GO:0098869                56.583333                8.820746   \n",
       "GO:0099503                     52.0                8.844333   \n",
       "GO:0106310                60.667252                5.780764   \n",
       "GO:0110165                50.330688                7.363402   \n",
       "\n",
       "                                                                                                                                                                                        Validation Precision scores   \n",
       "GO:0000166    [0.556390977443609, 0.556390977443609, 0.6165413533834586, 0.5563380281690141, 0.6194029850746269, 0.5426356589147286, 0.547945205479452, 0.5526315789473685, 0.5441176470588235, 0.5555555555555556]  \\\n",
       "GO:0000287                                                        [0.625, 0.4, 0.6666666666666666, 0.625, 0.42857142857142855, 0.7142857142857143, 0.5, 0.5555555555555556, 0.6666666666666666, 0.5714285714285714]   \n",
       "GO:0000325                                         [0.7741935483870968, 0.7142857142857143, 0.75, 0.7027027027027027, 0.7837837837837838, 0.6666666666666666, 0.78125, 0.7419354838709677, 0.7096774193548387, 0.7]   \n",
       "GO:0000398                                            [0.7777777777777778, 0.6666666666666666, 1.0, 0.625, 0.8333333333333334, 0.6153846153846154, 0.6363636363636364, 0.6666666666666666, 0.5714285714285714, 1.0]   \n",
       "GO:0000976                             [0.6976744186046512, 0.6666666666666666, 0.5652173913043478, 0.7428571428571429, 0.6904761904761905, 0.7027027027027027, 0.8, 0.717948717948718, 0.6875, 0.6666666666666666]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                          [0.8, 0.6, 1.0, 0.7142857142857143, 0.6666666666666666, 0.6, 0.5, 0.4, 0.5, 0.6666666666666666]   \n",
       "GO:0098869                                                                            [0.5, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6, 0.5714285714285714, 0.5, 0.5, 0.7142857142857143, 0.6666666666666666]   \n",
       "GO:0099503                                                          [0.5555555555555556, 0.6666666666666666, 0.6666666666666666, 0.625, 0.42857142857142855, 0.5, 0.4, 0.5555555555555556, 0.6, 0.3333333333333333]   \n",
       "GO:0106310  [0.6363636363636364, 0.6666666666666666, 0.5526315789473685, 0.6785714285714286, 0.6363636363636364, 0.5142857142857142, 0.5238095238095238, 0.6129032258064516, 0.5882352941176471, 0.6666666666666...   \n",
       "GO:0110165                                           [0.625, 0.4166666666666667, 0.5555555555555556, 0.5, 0.47058823529411764, 0.5, 0.5714285714285714, 0.4666666666666667, 0.4117647058823529, 0.5454545454545454]   \n",
       "\n",
       "           Mean Validation Precision   \n",
       "GO:0000166                  0.564795  \\\n",
       "GO:0000287                  0.575317   \n",
       "GO:0000325                   0.73245   \n",
       "GO:0000398                  0.739262   \n",
       "GO:0000976                  0.693771   \n",
       "...                              ...   \n",
       "GO:0090502                  0.644762   \n",
       "GO:0098869                  0.588571   \n",
       "GO:0099503                  0.533135   \n",
       "GO:0106310                   0.60765   \n",
       "GO:0110165                  0.506312   \n",
       "\n",
       "                                                                                                                                                                                           Validation Recall scores   \n",
       "GO:0000166                                                                                            [0.5736434108527132, 0.578125, 0.640625, 0.6171875, 0.6484375, 0.546875, 0.625, 0.65625, 0.578125, 0.5859375]  \\\n",
       "GO:0000287                                                                                                                                           [0.625, 0.25, 0.75, 0.625, 0.375, 0.625, 0.5, 0.625, 0.5, 0.5]   \n",
       "GO:0000325  [0.6857142857142857, 0.7142857142857143, 0.6857142857142857, 0.7428571428571429, 0.8285714285714286, 0.7058823529411765, 0.7352941176470589, 0.6764705882352942, 0.6470588235294118, 0.8235294117647...   \n",
       "GO:0000398                                                                                                 [0.7777777777777778, 0.6666666666666666, 0.5555555555555556, 0.625, 0.625, 1.0, 0.875, 0.75, 0.5, 0.625]   \n",
       "GO:0000976  [0.7894736842105263, 0.7368421052631579, 0.7027027027027027, 0.7027027027027027, 0.7837837837837838, 0.7027027027027027, 0.6486486486486487, 0.7567567567567568, 0.5945945945945946, 0.7027027027027...   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                                      [0.8, 0.6, 0.8, 1.0, 0.4, 0.6, 0.5, 0.5, 0.25, 1.0]   \n",
       "GO:0098869                                                                                                   [0.625, 0.5, 0.25, 0.25, 0.375, 0.5, 0.5, 0.42857142857142855, 0.7142857142857143, 0.8571428571428571]   \n",
       "GO:0099503                                                                  [0.625, 0.5, 0.25, 0.625, 0.375, 0.42857142857142855, 0.2857142857142857, 0.7142857142857143, 0.42857142857142855, 0.42857142857142855]   \n",
       "GO:0106310  [0.6176470588235294, 0.7647058823529411, 0.6176470588235294, 0.5588235294117647, 0.6176470588235294, 0.5294117647058824, 0.6470588235294118, 0.5757575757575758, 0.6060606060606061, 0.6666666666666...   \n",
       "GO:0110165                           [0.7142857142857143, 0.35714285714285715, 0.35714285714285715, 0.35714285714285715, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5, 0.5, 0.46153846153846156]   \n",
       "\n",
       "           Mean Validation Recall   \n",
       "GO:0000166               0.605021  \\\n",
       "GO:0000287                 0.5375   \n",
       "GO:0000325               0.724538   \n",
       "GO:0000398                    0.7   \n",
       "GO:0000976               0.712091   \n",
       "...                           ...   \n",
       "GO:0090502                  0.645   \n",
       "GO:0098869                    0.5   \n",
       "GO:0099503               0.466071   \n",
       "GO:0106310               0.620143   \n",
       "GO:0110165               0.496154   \n",
       "\n",
       "                                                                                                                                                                                               Validation F1 scores   \n",
       "GO:0000166                    [0.564885496183206, 0.5670498084291187, 0.6283524904214559, 0.5851851851851853, 0.633587786259542, 0.5447470817120622, 0.583941605839416, 0.6, 0.5606060606060606, 0.570342205323194]  \\\n",
       "GO:0000287                                         [0.625, 0.3076923076923077, 0.7058823529411765, 0.625, 0.39999999999999997, 0.6666666666666666, 0.5, 0.5882352941176471, 0.5714285714285715, 0.5333333333333333]   \n",
       "GO:0000325  [0.7272727272727272, 0.7142857142857143, 0.7164179104477612, 0.7222222222222223, 0.8055555555555555, 0.6857142857142857, 0.7575757575757576, 0.7076923076923077, 0.6769230769230768, 0.7567567567567...   \n",
       "GO:0000398               [0.7777777777777778, 0.6666666666666666, 0.7142857142857143, 0.625, 0.7142857142857143, 0.761904761904762, 0.7368421052631579, 0.7058823529411765, 0.5333333333333333, 0.7692307692307693]   \n",
       "GO:0000976                 [0.7407407407407408, 0.7, 0.6265060240963856, 0.7222222222222223, 0.7341772151898734, 0.7027027027027027, 0.7164179104477612, 0.736842105263158, 0.6376811594202898, 0.6842105263157895]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                             [0.8000000000000002, 0.6, 0.888888888888889, 0.8333333333333333, 0.5, 0.6, 0.5, 0.4444444444444445, 0.3333333333333333, 0.8]   \n",
       "GO:0098869                                           [0.5555555555555556, 0.5, 0.36363636363636365, 0.36363636363636365, 0.4615384615384615, 0.5333333333333333, 0.5, 0.4615384615384615, 0.7142857142857143, 0.75]   \n",
       "GO:0099503                                        [0.5882352941176471, 0.5714285714285715, 0.36363636363636365, 0.625, 0.39999999999999997, 0.4615384615384615, 0.3333333333333333, 0.6250000000000001, 0.5, 0.375]   \n",
       "GO:0106310            [0.6268656716417911, 0.7123287671232877, 0.5833333333333334, 0.6129032258064516, 0.6268656716417911, 0.5217391304347826, 0.5789473684210527, 0.59375, 0.5970149253731343, 0.6666666666666666]   \n",
       "GO:0110165  [0.6666666666666666, 0.3846153846153846, 0.43478260869565216, 0.41666666666666663, 0.5161290322580646, 0.5333333333333333, 0.5714285714285714, 0.4827586206896552, 0.45161290322580644, 0.4999999999...   \n",
       "\n",
       "           Mean Validation F1 Score  \n",
       "GO:0000166                  0.58387  \n",
       "GO:0000287                 0.552324  \n",
       "GO:0000325                 0.727042  \n",
       "GO:0000398                 0.700521  \n",
       "GO:0000976                  0.70015  \n",
       "...                             ...  \n",
       "GO:0090502                     0.63  \n",
       "GO:0098869                 0.520352  \n",
       "GO:0099503                 0.484317  \n",
       "GO:0106310                 0.612041  \n",
       "GO:0110165                 0.495799  \n",
       "\n",
       "[227 rows x 17 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(CB_Root33_results_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0517fb5b-ee1b-4114-9ab6-a47e9ccd0b80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6033587573270027,\n",
       " 0.4785613354653603,\n",
       " 0.7066824950627767,\n",
       " 0.6652882205513786,\n",
       " 0.6925828200897343,\n",
       " 0.6253301046779308,\n",
       " 0.6145665726355382,\n",
       " 0.6065418590171087,\n",
       " 0.604373225775433,\n",
       " 0.5732612672382306,\n",
       " 0.5945088245088244,\n",
       " 0.5541375291375291,\n",
       " 0.6496609567078943,\n",
       " 0.6758428579852842,\n",
       " 0.6836907587079653,\n",
       " 0.8377775168216344,\n",
       " 0.4953535353535353,\n",
       " 0.5483516483516484,\n",
       " 0.5567354594384417,\n",
       " 0.5274276600592389,\n",
       " 0.5008458208458209,\n",
       " 0.643545155993432,\n",
       " 0.669902362744468,\n",
       " 0.6213095238095238,\n",
       " 0.6152564102564102,\n",
       " 0.5451666524934892,\n",
       " 0.48177489177489174,\n",
       " 0.5999188801040665,\n",
       " 0.6277196333110325,\n",
       " 0.6289910062966504,\n",
       " 0.5708214729886556,\n",
       " 0.597069597069597,\n",
       " 0.49942807544698525,\n",
       " 0.6275357975357976,\n",
       " 0.597507604566428,\n",
       " 0.7033787342688271,\n",
       " 0.5931110043662637,\n",
       " 0.601323657395479,\n",
       " 0.5442555994729907,\n",
       " 0.6157777389331576,\n",
       " 0.5849325108960082,\n",
       " 0.6171811338993876,\n",
       " 0.6290212166687754,\n",
       " 0.5237579242223205,\n",
       " 0.6228325825445152,\n",
       " 0.6369762641898865,\n",
       " 0.6786946386946386,\n",
       " 0.640631625856084,\n",
       " 0.7155804849572078,\n",
       " 0.6526102401157707,\n",
       " 0.6640471949182536,\n",
       " 0.6487596665460443,\n",
       " 0.6090931290931291,\n",
       " 0.6753161784611563,\n",
       " 0.5984076157892182,\n",
       " 0.5682401190141128,\n",
       " 0.6005445171384449,\n",
       " 0.6203981078236875,\n",
       " 0.6293026080738032,\n",
       " 0.7223356455661506,\n",
       " 0.7566375748731143,\n",
       " 0.6740042222605435,\n",
       " 0.8365041743347262,\n",
       " 0.5582288320125651,\n",
       " 0.5194310754604872,\n",
       " 0.5927610546672556,\n",
       " 0.5414640914640914,\n",
       " 0.5667458429293568,\n",
       " 0.7208408258408258,\n",
       " 0.678290586086688,\n",
       " 0.6653288912819132,\n",
       " 0.5104107004107004,\n",
       " 0.5952933808976042,\n",
       " 0.5587736276600604,\n",
       " 0.7802745929680914,\n",
       " 0.5354256854256854,\n",
       " 0.7319397422736917,\n",
       " 0.7405595453339042,\n",
       " 0.5577999777999777,\n",
       " 0.7048813209494325,\n",
       " 0.6083245064383763,\n",
       " 0.6762959262959263,\n",
       " 0.47055951600265544,\n",
       " 0.5954244709147258,\n",
       " 0.5866864565107897,\n",
       " 0.5245665445665445,\n",
       " 0.5477688977688978,\n",
       " 0.5935958485958486,\n",
       " 0.49475912975912967,\n",
       " 0.650769886430406,\n",
       " 0.5803183335829605,\n",
       " 0.7337245829505892,\n",
       " 0.6787339309011136,\n",
       " 0.6199703652447279,\n",
       " 0.5997211040389451,\n",
       " 0.5103760599164664,\n",
       " 0.5383150183150184,\n",
       " 0.5630860805860806,\n",
       " 0.706432005935995,\n",
       " 0.520635266109125,\n",
       " 0.5647043067309434,\n",
       " 0.5802131202131201,\n",
       " 0.5032010953719104,\n",
       " 0.39642857142857146,\n",
       " 0.5491484985602633,\n",
       " 0.5895921126340291,\n",
       " 0.5156561085972851,\n",
       " 0.6192468917391517,\n",
       " 0.600997335997336,\n",
       " 0.6860477173325472,\n",
       " 0.4646192696192696,\n",
       " 0.6140361675386579,\n",
       " 0.6258538484853835,\n",
       " 0.5906386200104365,\n",
       " 0.5429156223893066,\n",
       " 0.6997546897546896,\n",
       " 0.6233152850456639,\n",
       " 0.6879102742070294,\n",
       " 0.6857126995261492,\n",
       " 0.6598124098124097,\n",
       " 0.6731701905503897,\n",
       " 0.64594129447788,\n",
       " 0.6989135238464013,\n",
       " 0.4344429339331888,\n",
       " 0.7194733310507959,\n",
       " 0.6735885952294621,\n",
       " 0.65708180782888,\n",
       " 0.5599456099456098,\n",
       " 0.6166015926734797,\n",
       " 0.5011899847658362,\n",
       " 0.48994338994338993,\n",
       " 0.5531461585796006,\n",
       " 0.5084057467647931,\n",
       " 0.6094205861246635,\n",
       " 0.6110225885225884,\n",
       " 0.6206060606060605,\n",
       " 0.6336996336996338,\n",
       " 0.5865739512510435,\n",
       " 0.43261849261849267,\n",
       " 0.6491663991024605,\n",
       " 0.608001199580147,\n",
       " 0.65989780174655,\n",
       " 0.6068881118881118,\n",
       " 0.5344677544677545,\n",
       " 0.6732659170894465,\n",
       " 0.4794288417121234,\n",
       " 0.5621250971250971,\n",
       " 0.6968994812063609,\n",
       " 0.5887426900584796,\n",
       " 0.6077000777000776,\n",
       " 0.5881865726556126,\n",
       " 0.573323771563106,\n",
       " 0.6979590017825312,\n",
       " 0.7013247863247863,\n",
       " 0.5890723269720182,\n",
       " 0.5876737470737484,\n",
       " 0.5812305733301637,\n",
       " 0.5187503674936671,\n",
       " 0.5126495726495728,\n",
       " 0.6044209468692227,\n",
       " 0.5965781784706652,\n",
       " 0.5506197775238255,\n",
       " 0.6195373229586305,\n",
       " 0.5781895881895882,\n",
       " 0.5376788223823032,\n",
       " 0.6132640888523241,\n",
       " 0.5316832290516501,\n",
       " 0.5034257638004835,\n",
       " 0.6461896790444885,\n",
       " 0.5984728987487608,\n",
       " 0.651978165183676,\n",
       " 0.5704895104895105,\n",
       " 0.593939393939394,\n",
       " 0.6125023722546942,\n",
       " 0.8583375937787702,\n",
       " 0.8300846814004709,\n",
       " 0.6297514780796514,\n",
       " 0.6146692936692937,\n",
       " 0.5914389861346383,\n",
       " 0.5654894323290126,\n",
       " 0.5784383753501401,\n",
       " 0.8142433862433863,\n",
       " 0.6820855400189766,\n",
       " 0.6136154642036994,\n",
       " 0.5954505135387488,\n",
       " 0.6301282051282051,\n",
       " 0.658420797468785,\n",
       " 0.7506410256410255,\n",
       " 0.5840270676560998,\n",
       " 0.6289207031956393,\n",
       " 0.5509106738280386,\n",
       " 0.5538167061696473,\n",
       " 0.5626573442362915,\n",
       " 0.7879944960440317,\n",
       " 0.5771049238762325,\n",
       " 0.5474636474636474,\n",
       " 0.6574891774891775,\n",
       " 0.6016495338167165,\n",
       " 0.5209802295678105,\n",
       " 0.4865920457251726,\n",
       " 0.5536904761904762,\n",
       " 0.657878162429246,\n",
       " 0.5786641016201428,\n",
       " 0.5409029859029859,\n",
       " 0.5791162501337206,\n",
       " 0.5615405434912215,\n",
       " 0.5614527472871125,\n",
       " 0.5250108225108227,\n",
       " 0.6228680959563312,\n",
       " 0.5982832080200501,\n",
       " 0.6161654153143585,\n",
       " 0.5521645021645021,\n",
       " 0.5721833721833721,\n",
       " 0.5045805501687854,\n",
       " 0.6287564287564288,\n",
       " 0.5969334097275274,\n",
       " 0.535923843066466,\n",
       " 0.5704894245539408,\n",
       " 0.6934801041037921,\n",
       " 0.5652732888027007,\n",
       " 0.43055000555000544,\n",
       " 0.530424166476798,\n",
       " 0.5884415584415583,\n",
       " 0.5594722666394493,\n",
       " 0.5004029304029304,\n",
       " 0.6063220128220729,\n",
       " 0.5234089623430438]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "761d4fc7-f9af-4f75-8350-fe45130604fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.02777777777777778,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0008849557522123894,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.002352941176470588,\n",
       " 0.0036363636363636364,\n",
       " 0.13668989501064974,\n",
       " 0.5484609448522493,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.011300977392667508,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.010448872357008062,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.006475050475545485,\n",
       " 0.01689895470383275,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08732698567062279,\n",
       " 0.0,\n",
       " 0.06591328534596116,\n",
       " 0.5117988466084418,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.00842170931322864,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.39476502836952354,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06534962310202466,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.011764705882352941,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.008695652173913045,\n",
       " 0.0018518518518518517,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.009523809523809523,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08581077428802862,\n",
       " 0.022151476267666205,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.001176470588235294,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.24262626262626266,\n",
       " 0.43433150183150177,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Root33_results_df[\"Mean Validation F1 Score\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df181fdf-ef5e-44bd-9e5b-bca7b77bb31a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 0.7 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "201ecbad-8502-40b1-a6c9-ad246b38b3f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this GO Term, Reduced Majority Size:  1281 Minority Size:  1281\n",
      "For this GO Term, Reduced Majority Size:  80 Minority Size:  80\n",
      "For this GO Term, Reduced Majority Size:  345 Minority Size:  345\n",
      "For this GO Term, Reduced Majority Size:  83 Minority Size:  83\n",
      "For this GO Term, Reduced Majority Size:  372 Minority Size:  372\n",
      "For this GO Term, Reduced Majority Size:  122 Minority Size:  122\n",
      "For this GO Term, Reduced Majority Size:  140 Minority Size:  140\n",
      "For this GO Term, Reduced Majority Size:  2244 Minority Size:  2244\n",
      "For this GO Term, Reduced Majority Size:  344 Minority Size:  344\n",
      "For this GO Term, Reduced Majority Size:  1066 Minority Size:  1066\n",
      "For this GO Term, Reduced Majority Size:  63 Minority Size:  63\n",
      "For this GO Term, Reduced Majority Size:  55 Minority Size:  55\n",
      "For this GO Term, Reduced Majority Size:  822 Minority Size:  822\n",
      "For this GO Term, Reduced Majority Size:  539 Minority Size:  539\n",
      "For this GO Term, Reduced Majority Size:  478 Minority Size:  478\n",
      "For this GO Term, Reduced Majority Size:  182 Minority Size:  182\n",
      "For this GO Term, Reduced Majority Size:  54 Minority Size:  54\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  305 Minority Size:  305\n",
      "For this GO Term, Reduced Majority Size:  107 Minority Size:  107\n",
      "For this GO Term, Reduced Majority Size:  64 Minority Size:  64\n",
      "For this GO Term, Reduced Majority Size:  117 Minority Size:  117\n",
      "For this GO Term, Reduced Majority Size:  141 Minority Size:  141\n",
      "For this GO Term, Reduced Majority Size:  67 Minority Size:  67\n",
      "For this GO Term, Reduced Majority Size:  56 Minority Size:  56\n",
      "For this GO Term, Reduced Majority Size:  129 Minority Size:  129\n",
      "For this GO Term, Reduced Majority Size:  63 Minority Size:  63\n",
      "For this GO Term, Reduced Majority Size:  542 Minority Size:  542\n",
      "For this GO Term, Reduced Majority Size:  442 Minority Size:  442\n",
      "For this GO Term, Reduced Majority Size:  356 Minority Size:  356\n",
      "For this GO Term, Reduced Majority Size:  76 Minority Size:  76\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  140 Minority Size:  140\n",
      "For this GO Term, Reduced Majority Size:  68 Minority Size:  68\n",
      "For this GO Term, Reduced Majority Size:  176 Minority Size:  176\n",
      "For this GO Term, Reduced Majority Size:  105 Minority Size:  105\n",
      "For this GO Term, Reduced Majority Size:  136 Minority Size:  136\n",
      "For this GO Term, Reduced Majority Size:  3302 Minority Size:  3302\n",
      "For this GO Term, Reduced Majority Size:  114 Minority Size:  114\n",
      "For this GO Term, Reduced Majority Size:  1222 Minority Size:  1222\n",
      "For this GO Term, Reduced Majority Size:  148 Minority Size:  148\n",
      "For this GO Term, Reduced Majority Size:  933 Minority Size:  933\n",
      "For this GO Term, Reduced Majority Size:  787 Minority Size:  787\n",
      "For this GO Term, Reduced Majority Size:  83 Minority Size:  83\n",
      "For this GO Term, Reduced Majority Size:  2828 Minority Size:  2828\n",
      "For this GO Term, Reduced Majority Size:  84 Minority Size:  84\n",
      "For this GO Term, Reduced Majority Size:  62 Minority Size:  62\n",
      "For this GO Term, Reduced Majority Size:  78 Minority Size:  78\n",
      "For this GO Term, Reduced Majority Size:  279 Minority Size:  279\n",
      "For this GO Term, Reduced Majority Size:  1830 Minority Size:  1830\n",
      "For this GO Term, Reduced Majority Size:  811 Minority Size:  811\n",
      "For this GO Term, Reduced Majority Size:  105 Minority Size:  105\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  231 Minority Size:  231\n",
      "For this GO Term, Reduced Majority Size:  245 Minority Size:  245\n",
      "For this GO Term, Reduced Majority Size:  106 Minority Size:  106\n",
      "For this GO Term, Reduced Majority Size:  175 Minority Size:  175\n",
      "For this GO Term, Reduced Majority Size:  590 Minority Size:  590\n",
      "For this GO Term, Reduced Majority Size:  277 Minority Size:  277\n",
      "For this GO Term, Reduced Majority Size:  579 Minority Size:  579\n",
      "For this GO Term, Reduced Majority Size:  141 Minority Size:  141\n",
      "For this GO Term, Reduced Majority Size:  1300 Minority Size:  1300\n",
      "For this GO Term, Reduced Majority Size:  194 Minority Size:  194\n",
      "For this GO Term, Reduced Majority Size:  138 Minority Size:  138\n",
      "For this GO Term, Reduced Majority Size:  94 Minority Size:  94\n",
      "For this GO Term, Reduced Majority Size:  1402 Minority Size:  1402\n",
      "For this GO Term, Reduced Majority Size:  58 Minority Size:  58\n",
      "For this GO Term, Reduced Majority Size:  286 Minority Size:  286\n",
      "For this GO Term, Reduced Majority Size:  59 Minority Size:  59\n",
      "For this GO Term, Reduced Majority Size:  131 Minority Size:  131\n",
      "For this GO Term, Reduced Majority Size:  101 Minority Size:  101\n",
      "For this GO Term, Reduced Majority Size:  55 Minority Size:  55\n",
      "For this GO Term, Reduced Majority Size:  950 Minority Size:  950\n",
      "For this GO Term, Reduced Majority Size:  199 Minority Size:  199\n",
      "For this GO Term, Reduced Majority Size:  86 Minority Size:  86\n",
      "For this GO Term, Reduced Majority Size:  47 Minority Size:  47\n",
      "For this GO Term, Reduced Majority Size:  152 Minority Size:  152\n",
      "For this GO Term, Reduced Majority Size:  235 Minority Size:  235\n",
      "For this GO Term, Reduced Majority Size:  59 Minority Size:  59\n",
      "For this GO Term, Reduced Majority Size:  97 Minority Size:  97\n",
      "For this GO Term, Reduced Majority Size:  530 Minority Size:  530\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  250 Minority Size:  250\n",
      "For this GO Term, Reduced Majority Size:  127 Minority Size:  127\n",
      "For this GO Term, Reduced Majority Size:  268 Minority Size:  268\n",
      "For this GO Term, Reduced Majority Size:  80 Minority Size:  80\n",
      "For this GO Term, Reduced Majority Size:  67 Minority Size:  67\n",
      "For this GO Term, Reduced Majority Size:  197 Minority Size:  197\n",
      "For this GO Term, Reduced Majority Size:  56 Minority Size:  56\n",
      "For this GO Term, Reduced Majority Size:  111 Minority Size:  111\n",
      "For this GO Term, Reduced Majority Size:  590 Minority Size:  590\n",
      "For this GO Term, Reduced Majority Size:  71 Minority Size:  71\n",
      "For this GO Term, Reduced Majority Size:  108 Minority Size:  108\n",
      "For this GO Term, Reduced Majority Size:  134 Minority Size:  134\n",
      "For this GO Term, Reduced Majority Size:  130 Minority Size:  130\n",
      "For this GO Term, Reduced Majority Size:  198 Minority Size:  198\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  79 Minority Size:  79\n",
      "For this GO Term, Reduced Majority Size:  2242 Minority Size:  2242\n",
      "For this GO Term, Reduced Majority Size:  248 Minority Size:  248\n",
      "For this GO Term, Reduced Majority Size:  186 Minority Size:  186\n",
      "For this GO Term, Reduced Majority Size:  65 Minority Size:  65\n",
      "For this GO Term, Reduced Majority Size:  242 Minority Size:  242\n",
      "For this GO Term, Reduced Majority Size:  52 Minority Size:  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this GO Term, Reduced Majority Size:  59 Minority Size:  59\n",
      "For this GO Term, Reduced Majority Size:  309 Minority Size:  309\n",
      "For this GO Term, Reduced Majority Size:  85 Minority Size:  85\n",
      "For this GO Term, Reduced Majority Size:  94 Minority Size:  94\n",
      "For this GO Term, Reduced Majority Size:  67 Minority Size:  67\n",
      "For this GO Term, Reduced Majority Size:  91 Minority Size:  91\n",
      "For this GO Term, Reduced Majority Size:  59 Minority Size:  59\n",
      "For this GO Term, Reduced Majority Size:  90 Minority Size:  90\n",
      "For this GO Term, Reduced Majority Size:  154 Minority Size:  154\n",
      "For this GO Term, Reduced Majority Size:  184 Minority Size:  184\n",
      "For this GO Term, Reduced Majority Size:  89 Minority Size:  89\n",
      "For this GO Term, Reduced Majority Size:  53 Minority Size:  53\n",
      "For this GO Term, Reduced Majority Size:  185 Minority Size:  185\n",
      "For this GO Term, Reduced Majority Size:  434 Minority Size:  434\n",
      "For this GO Term, Reduced Majority Size:  1065 Minority Size:  1065\n",
      "For this GO Term, Reduced Majority Size:  43 Minority Size:  43\n",
      "For this GO Term, Reduced Majority Size:  101 Minority Size:  101\n",
      "For this GO Term, Reduced Majority Size:  178 Minority Size:  178\n",
      "For this GO Term, Reduced Majority Size:  1011 Minority Size:  1011\n",
      "For this GO Term, Reduced Majority Size:  127 Minority Size:  127\n",
      "For this GO Term, Reduced Majority Size:  337 Minority Size:  337\n",
      "For this GO Term, Reduced Majority Size:  173 Minority Size:  173\n",
      "For this GO Term, Reduced Majority Size:  101 Minority Size:  101\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  188 Minority Size:  188\n",
      "For this GO Term, Reduced Majority Size:  94 Minority Size:  94\n",
      "For this GO Term, Reduced Majority Size:  63 Minority Size:  63\n",
      "For this GO Term, Reduced Majority Size:  151 Minority Size:  151\n",
      "For this GO Term, Reduced Majority Size:  131 Minority Size:  131\n",
      "For this GO Term, Reduced Majority Size:  200 Minority Size:  200\n",
      "For this GO Term, Reduced Majority Size:  111 Minority Size:  111\n",
      "For this GO Term, Reduced Majority Size:  50 Minority Size:  50\n",
      "For this GO Term, Reduced Majority Size:  69 Minority Size:  69\n",
      "For this GO Term, Reduced Majority Size:  161 Minority Size:  161\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  107 Minority Size:  107\n",
      "For this GO Term, Reduced Majority Size:  118 Minority Size:  118\n",
      "For this GO Term, Reduced Majority Size:  247 Minority Size:  247\n",
      "For this GO Term, Reduced Majority Size:  60 Minority Size:  60\n",
      "For this GO Term, Reduced Majority Size:  56 Minority Size:  56\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  90 Minority Size:  90\n",
      "For this GO Term, Reduced Majority Size:  66 Minority Size:  66\n",
      "For this GO Term, Reduced Majority Size:  313 Minority Size:  313\n",
      "For this GO Term, Reduced Majority Size:  88 Minority Size:  88\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  3472 Minority Size:  3472\n",
      "For this GO Term, Reduced Majority Size:  2917 Minority Size:  2917\n",
      "For this GO Term, Reduced Majority Size:  72 Minority Size:  72\n",
      "For this GO Term, Reduced Majority Size:  134 Minority Size:  134\n",
      "For this GO Term, Reduced Majority Size:  695 Minority Size:  695\n",
      "For this GO Term, Reduced Majority Size:  694 Minority Size:  694\n",
      "For this GO Term, Reduced Majority Size:  632 Minority Size:  632\n",
      "For this GO Term, Reduced Majority Size:  385 Minority Size:  385\n",
      "For this GO Term, Reduced Majority Size:  53 Minority Size:  53\n",
      "For this GO Term, Reduced Majority Size:  133 Minority Size:  133\n",
      "For this GO Term, Reduced Majority Size:  1691 Minority Size:  1691\n",
      "For this GO Term, Reduced Majority Size:  132 Minority Size:  132\n",
      "For this GO Term, Reduced Majority Size:  269 Minority Size:  269\n",
      "For this GO Term, Reduced Majority Size:  56 Minority Size:  56\n",
      "For this GO Term, Reduced Majority Size:  1237 Minority Size:  1237\n",
      "For this GO Term, Reduced Majority Size:  71 Minority Size:  71\n",
      "For this GO Term, Reduced Majority Size:  192 Minority Size:  192\n",
      "For this GO Term, Reduced Majority Size:  184 Minority Size:  184\n",
      "For this GO Term, Reduced Majority Size:  119 Minority Size:  119\n",
      "For this GO Term, Reduced Majority Size:  123 Minority Size:  123\n",
      "For this GO Term, Reduced Majority Size:  197 Minority Size:  197\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  49 Minority Size:  49\n",
      "For this GO Term, Reduced Majority Size:  182 Minority Size:  182\n",
      "For this GO Term, Reduced Majority Size:  73 Minority Size:  73\n",
      "For this GO Term, Reduced Majority Size:  102 Minority Size:  102\n",
      "For this GO Term, Reduced Majority Size:  176 Minority Size:  176\n",
      "For this GO Term, Reduced Majority Size:  116 Minority Size:  116\n",
      "For this GO Term, Reduced Majority Size:  112 Minority Size:  112\n",
      "For this GO Term, Reduced Majority Size:  137 Minority Size:  137\n",
      "For this GO Term, Reduced Majority Size:  73 Minority Size:  73\n",
      "For this GO Term, Reduced Majority Size:  141 Minority Size:  141\n",
      "For this GO Term, Reduced Majority Size:  83 Minority Size:  83\n",
      "For this GO Term, Reduced Majority Size:  185 Minority Size:  185\n",
      "For this GO Term, Reduced Majority Size:  72 Minority Size:  72\n",
      "For this GO Term, Reduced Majority Size:  78 Minority Size:  78\n",
      "For this GO Term, Reduced Majority Size:  74 Minority Size:  74\n",
      "For this GO Term, Reduced Majority Size:  52 Minority Size:  52\n",
      "For this GO Term, Reduced Majority Size:  162 Minority Size:  162\n",
      "For this GO Term, Reduced Majority Size:  106 Minority Size:  106\n",
      "For this GO Term, Reduced Majority Size:  112 Minority Size:  112\n",
      "For this GO Term, Reduced Majority Size:  85 Minority Size:  85\n",
      "For this GO Term, Reduced Majority Size:  187 Minority Size:  187\n",
      "For this GO Term, Reduced Majority Size:  87 Minority Size:  87\n",
      "For this GO Term, Reduced Majority Size:  242 Minority Size:  242\n",
      "For this GO Term, Reduced Majority Size:  57 Minority Size:  57\n",
      "For this GO Term, Reduced Majority Size:  60 Minority Size:  60\n",
      "For this GO Term, Reduced Majority Size:  96 Minority Size:  96\n",
      "For this GO Term, Reduced Majority Size:  169 Minority Size:  169\n",
      "For this GO Term, Reduced Majority Size:  91 Minority Size:  91\n",
      "For this GO Term, Reduced Majority Size:  79 Minority Size:  79\n",
      "For this GO Term, Reduced Majority Size:  97 Minority Size:  97\n",
      "For this GO Term, Reduced Majority Size:  1632 Minority Size:  1632\n",
      "For this GO Term, Reduced Majority Size:  64 Minority Size:  64\n",
      "For this GO Term, Reduced Majority Size:  162 Minority Size:  162\n",
      "For this GO Term, Reduced Majority Size:  220 Minority Size:  220\n",
      "For this GO Term, Reduced Majority Size:  97 Minority Size:  97\n",
      "For this GO Term, Reduced Majority Size:  61 Minority Size:  61\n",
      "For this GO Term, Reduced Majority Size:  66 Minority Size:  66\n",
      "For this GO Term, Reduced Majority Size:  93 Minority Size:  93\n",
      "For this GO Term, Reduced Majority Size:  258 Minority Size:  258\n",
      "For this GO Term, Reduced Majority Size:  62 Minority Size:  62\n",
      "For this GO Term, Reduced Majority Size:  56 Minority Size:  56\n",
      "For this GO Term, Reduced Majority Size:  76 Minority Size:  76\n",
      "For this GO Term, Reduced Majority Size:  132 Minority Size:  132\n",
      "For this GO Term, Reduced Majority Size:  72 Minority Size:  72\n",
      "For this GO Term, Reduced Majority Size:  400 Minority Size:  400\n",
      "For this GO Term, Reduced Majority Size:  132 Minority Size:  132\n",
      "For this GO Term, Reduced Majority Size:  121 Minority Size:  121\n",
      "For this GO Term, Reduced Majority Size:  195 Minority Size:  195\n",
      "For this GO Term, Reduced Majority Size:  49 Minority Size:  49\n",
      "For this GO Term, Reduced Majority Size:  105 Minority Size:  105\n",
      "For this GO Term, Reduced Majority Size:  46 Minority Size:  46\n",
      "For this GO Term, Reduced Majority Size:  77 Minority Size:  77\n",
      "For this GO Term, Reduced Majority Size:  75 Minority Size:  75\n",
      "For this GO Term, Reduced Majority Size:  337 Minority Size:  337\n",
      "For this GO Term, Reduced Majority Size:  139 Minority Size:  139\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# change the threshold in cross_validation function\n",
    "\n",
    "CB_Root33_Go_Mean_Accuracy = []\n",
    "CB_Root33_Go_Std_Accuracy = []\n",
    "CB_Root33_Go_10cross_Accuracies = []\n",
    "CB_Root33_Go_10cross_Mean_F1=[]\n",
    "CB_Root33_Go_10cross_Mean_Precision=[]\n",
    "CB_Root33_Go_10cross_Mean_Recall=[]\n",
    "CB_Root33_results_dict = {}\n",
    "column_headers = list(Root33_data_setA.columns)\n",
    "\n",
    "\n",
    "for i in range(len(Feature_columns), Root33_data_setA.shape[1]):\n",
    "    X = Root33_data_setA.iloc[:, :len(Feature_columns) ]\n",
    "    y = Root33_data_setA.iloc[:,i]\n",
    "\n",
    "    # Determin the minority class\n",
    "    minority_class = 1 if sum(y == 1) < sum(y == 0) else 0\n",
    "    minority_indices = np.where(y == minority_class)[0]\n",
    "    majority_indices = np.where(y != minority_class)[0]\n",
    "    majority_indices_downsampled = resample(majority_indices, replace=False, n_samples=len(minority_indices), random_state=0)\n",
    "\n",
    "    print(\"For this GO Term, Reduced Majority Size: \", len(majority_indices_downsampled), \"Minority Size: \", len(minority_indices))\n",
    "    \n",
    "    # Combine minority and downsampled majority indices\n",
    "    indices_combined = np.concatenate([minority_indices, majority_indices_downsampled])\n",
    "\n",
    "    # Subset the data based on the selected indices\n",
    "    X_balanced = X.iloc[indices_combined, :]\n",
    "    y_balanced = y.iloc[indices_combined]\n",
    "\n",
    "    # Train the random forest model\n",
    "    goterm = column_headers[i]\n",
    "    CB_Root33_results_dict[goterm] = cross_validation(pipe, X_balanced, y_balanced, 10)\n",
    "\n",
    "    # CB_Root33_Go_Mean_Accuracy.append(Random_forest_result[\"Mean Validation Accuracy\"])\n",
    "    # CB_Root33_Go_Std_Accuracy.append(Random_forest_result[\"Std Validation Accuracy\"])\n",
    "    # CB_Root33_Go_10cross_Accuracies.append(Random_forest_result[\"Validation Accuracy scores\"])\n",
    "    # CB_Root33_Go_10cross_Mean_F1.append(Random_forest_result[\"Mean Validation F1 Score\"])\n",
    "    # CB_Root33_Go_10cross_Mean_Precision.append(Random_forest_result[\"Mean Validation Precision\"])\n",
    "    # CB_Root33_Go_10cross_Mean_Recall.append(Random_forest_result[\"Mean Validation Recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d79090f2-e473-41a2-8797-73b0a2bd90d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy scores</th>\n",
       "      <th>Mean Training Accuracy</th>\n",
       "      <th>Training Precision scores</th>\n",
       "      <th>Mean Training Precision</th>\n",
       "      <th>Training Recall scores</th>\n",
       "      <th>Mean Training Recall</th>\n",
       "      <th>Training F1 scores</th>\n",
       "      <th>Mean Training F1 Score</th>\n",
       "      <th>Validation Accuracy scores</th>\n",
       "      <th>Mean Validation Accuracy</th>\n",
       "      <th>Std Validation Accuracy</th>\n",
       "      <th>Validation Precision scores</th>\n",
       "      <th>Mean Validation Precision</th>\n",
       "      <th>Validation Recall scores</th>\n",
       "      <th>Mean Validation Recall</th>\n",
       "      <th>Validation F1 scores</th>\n",
       "      <th>Mean Validation F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0000166</th>\n",
       "      <td>[0.9544468546637744, 0.9592190889370933, 0.9605377276669558, 0.9575021682567216, 0.9631396357328708, 0.9640069384215091, 0.9562012142237641, 0.9449262792714658, 0.9531656548135299, 0.9549002601908...</td>\n",
       "      <td>95.680458</td>\n",
       "      <td>[0.9596136962247586, 0.9600347523892268, 0.9633507853403142, 0.9799818016378526, 0.9635416666666666, 0.9709507042253521, 0.9503424657534246, 0.9422413793103448, 0.9539530842745438, 0.9645704162976...</td>\n",
       "      <td>0.960858</td>\n",
       "      <td>[0.9487847222222222, 0.95836947094536, 0.9575021682567216, 0.9340849956634866, 0.9627059843885516, 0.9566348655680833, 0.9627059843885516, 0.9479618386816999, 0.9522983521248916, 0.9444926279271466]</td>\n",
       "      <td>0.952554</td>\n",
       "      <td>[0.9541684853775645, 0.959201388888889, 0.9604175728577643, 0.9564831261101243, 0.9631236442516269, 0.9637396242900831, 0.9564842740198191, 0.945092952875054, 0.9531249999999999, 0.9544259421560036]</td>\n",
       "      <td>0.956626</td>\n",
       "      <td>[0.5719844357976653, 0.5875486381322957, 0.58203125, 0.53125, 0.59765625, 0.56640625, 0.5546875, 0.5859375, 0.58203125, 0.578125]</td>\n",
       "      <td>57.376581</td>\n",
       "      <td>1.814778</td>\n",
       "      <td>[0.5736434108527132, 0.5901639344262295, 0.5704697986577181, 0.5298507462686567, 0.5912408759124088, 0.5555555555555556, 0.5486111111111112, 0.5753424657534246, 0.5724137931034483, 0.5714285714285...</td>\n",
       "      <td>0.567872</td>\n",
       "      <td>[0.5736434108527132, 0.5625, 0.6640625, 0.5546875, 0.6328125, 0.6640625, 0.6171875, 0.65625, 0.6484375, 0.625]</td>\n",
       "      <td>0.619864</td>\n",
       "      <td>[0.5736434108527132, 0.576, 0.6137184115523465, 0.5419847328244274, 0.6113207547169812, 0.6049822064056939, 0.5808823529411764, 0.6131386861313868, 0.6080586080586081, 0.5970149253731343]</td>\n",
       "      <td>0.592074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000287</th>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9930555555555556, 1.0, 1.0, 1.0, 0.9930555555555556, 0.9930555555555556, 1.0]</td>\n",
       "      <td>99.791667</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 0.9861111111111112, 0.9861111111111112, 1.0]</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.993006993006993, 1.0, 1.0, 1.0, 0.993006993006993, 0.993006993006993, 1.0]</td>\n",
       "      <td>0.997902</td>\n",
       "      <td>[0.625, 0.625, 0.5625, 0.625, 0.4375, 0.625, 0.5, 0.5, 0.6875, 0.5625]</td>\n",
       "      <td>57.5</td>\n",
       "      <td>7.28869</td>\n",
       "      <td>[0.625, 0.6666666666666666, 0.5555555555555556, 0.625, 0.4444444444444444, 0.6666666666666666, 0.5, 0.5, 0.7142857142857143, 0.5555555555555556]</td>\n",
       "      <td>0.585317</td>\n",
       "      <td>[0.625, 0.5, 0.625, 0.625, 0.5, 0.5, 0.5, 0.5, 0.625, 0.625]</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>[0.625, 0.5714285714285715, 0.5882352941176471, 0.625, 0.47058823529411764, 0.5714285714285715, 0.5, 0.5, 0.6666666666666666, 0.5882352941176471]</td>\n",
       "      <td>0.570658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000325</th>\n",
       "      <td>[0.9758454106280193, 0.9855072463768116, 0.9838969404186796, 0.9903381642512077, 0.9919484702093397, 0.9790660225442834, 0.9806763285024155, 0.9903381642512077, 0.9838969404186796, 0.9935587761674...</td>\n",
       "      <td>98.550725</td>\n",
       "      <td>[0.9773462783171522, 0.9839228295819936, 0.987012987012987, 0.9935064935064936, 0.9935275080906149, 0.9869281045751634, 0.9776357827476039, 0.987220447284345, 0.990228013029316, 0.9935691318327974]</td>\n",
       "      <td>0.98709</td>\n",
       "      <td>[0.9741935483870968, 0.9870967741935484, 0.9806451612903225, 0.9870967741935484, 0.9903225806451613, 0.9710610932475884, 0.9839228295819936, 0.9935691318327974, 0.977491961414791, 0.9935691318327974]</td>\n",
       "      <td>0.983897</td>\n",
       "      <td>[0.9757673667205171, 0.9855072463768115, 0.9838187702265372, 0.9902912621359222, 0.9919224555735057, 0.978930307941653, 0.9807692307692308, 0.9903846153846154, 0.9838187702265372, 0.9935691318327974]</td>\n",
       "      <td>0.985478</td>\n",
       "      <td>[0.7391304347826086, 0.6521739130434783, 0.6666666666666666, 0.6666666666666666, 0.7391304347826086, 0.7101449275362319, 0.6666666666666666, 0.7246376811594203, 0.7536231884057971, 0.7681159420289...</td>\n",
       "      <td>70.869565</td>\n",
       "      <td>4.018964</td>\n",
       "      <td>[0.7575757575757576, 0.6571428571428571, 0.6666666666666666, 0.6666666666666666, 0.7297297297297297, 0.6944444444444444, 0.6571428571428571, 0.7586206896551724, 0.7741935483870968, 0.7647058823529...</td>\n",
       "      <td>0.712689</td>\n",
       "      <td>[0.7142857142857143, 0.6571428571428571, 0.6857142857142857, 0.6857142857142857, 0.7714285714285715, 0.7352941176470589, 0.6764705882352942, 0.6470588235294118, 0.7058823529411765, 0.7647058823529...</td>\n",
       "      <td>0.70437</td>\n",
       "      <td>[0.7352941176470589, 0.6571428571428571, 0.676056338028169, 0.676056338028169, 0.75, 0.7142857142857144, 0.6666666666666666, 0.6984126984126984, 0.7384615384615385, 0.7647058823529412]</td>\n",
       "      <td>0.707708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000398</th>\n",
       "      <td>[0.9932885906040269, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 0.9933333333333333, 1.0, 0.9933333333333333]</td>\n",
       "      <td>99.732438</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.9864864864864865, 1.0, 1.0, 1.0, 0.9866666666666667, 1.0, 1.0, 0.9866666666666667, 1.0, 0.9866666666666667]</td>\n",
       "      <td>0.994649</td>\n",
       "      <td>[0.9931972789115647, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 0.9932885906040269, 1.0, 0.9932885906040269]</td>\n",
       "      <td>0.997306</td>\n",
       "      <td>[0.7647058823529411, 0.5882352941176471, 0.7647058823529411, 0.5882352941176471, 0.7058823529411765, 0.7058823529411765, 0.5625, 0.75, 0.625, 0.6875]</td>\n",
       "      <td>67.426471</td>\n",
       "      <td>7.347792</td>\n",
       "      <td>[0.7777777777777778, 0.6, 0.8571428571428571, 0.5555555555555556, 0.8, 0.6153846153846154, 0.5454545454545454, 0.75, 0.625, 0.8]</td>\n",
       "      <td>0.692632</td>\n",
       "      <td>[0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.625, 0.5, 1.0, 0.75, 0.75, 0.625, 0.5]</td>\n",
       "      <td>0.686111</td>\n",
       "      <td>[0.7777777777777778, 0.631578947368421, 0.75, 0.5882352941176471, 0.6153846153846154, 0.761904761904762, 0.631578947368421, 0.75, 0.625, 0.6153846153846154]</td>\n",
       "      <td>0.674684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000976</th>\n",
       "      <td>[0.9910313901345291, 0.9970104633781763, 0.9940209267563528, 0.9895366218236173, 0.9880597014925373, 0.9940298507462687, 0.9940298507462687, 0.9865671641791045, 0.991044776119403, 0.9955223880597015]</td>\n",
       "      <td>99.208531</td>\n",
       "      <td>[0.996969696969697, 1.0, 1.0, 0.9910179640718563, 0.9969604863221885, 1.0, 0.996996996996997, 1.0, 0.993993993993994, 0.9940476190476191]</td>\n",
       "      <td>0.996999</td>\n",
       "      <td>[0.9850299401197605, 0.9940119760479041, 0.9880597014925373, 0.9880597014925373, 0.9791044776119403, 0.9880597014925373, 0.991044776119403, 0.9731343283582089, 0.9880597014925373, 0.9970149253731343]</td>\n",
       "      <td>0.987158</td>\n",
       "      <td>[0.9909638554216867, 0.996996996996997, 0.993993993993994, 0.9895366218236173, 0.9879518072289157, 0.993993993993994, 0.9940119760479041, 0.9863842662632374, 0.9910179640718564, 0.9955290611028316]</td>\n",
       "      <td>0.992038</td>\n",
       "      <td>[0.6933333333333334, 0.64, 0.5866666666666667, 0.76, 0.7162162162162162, 0.6351351351351351, 0.6756756756756757, 0.6756756756756757, 0.6621621621621622, 0.6621621621621622]</td>\n",
       "      <td>67.07027</td>\n",
       "      <td>4.474677</td>\n",
       "      <td>[0.6744186046511628, 0.6571428571428571, 0.5681818181818182, 0.8064516129032258, 0.7222222222222222, 0.631578947368421, 0.7241379310344828, 0.6756756756756757, 0.6666666666666666, 0.6578947368421053]</td>\n",
       "      <td>0.678437</td>\n",
       "      <td>[0.7631578947368421, 0.6052631578947368, 0.6756756756756757, 0.6756756756756757, 0.7027027027027027, 0.6486486486486487, 0.5675675675675675, 0.6756756756756757, 0.6486486486486487, 0.6756756756756...</td>\n",
       "      <td>0.663869</td>\n",
       "      <td>[0.7160493827160495, 0.6301369863013698, 0.617283950617284, 0.7352941176470588, 0.7123287671232876, 0.64, 0.6363636363636365, 0.6756756756756757, 0.6575342465753425, 0.6666666666666667]</td>\n",
       "      <td>0.668733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0090502</th>\n",
       "      <td>[1.0, 0.9878048780487805, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 1.0, 0.9879518072289156, 0.9879518072289156]</td>\n",
       "      <td>99.516603</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.9761904761904762, 1.0, 1.0, 1.0, 0.9767441860465116, 1.0]</td>\n",
       "      <td>0.995293</td>\n",
       "      <td>[1.0, 0.975609756097561, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9761904761904762]</td>\n",
       "      <td>0.99518</td>\n",
       "      <td>[1.0, 0.9876543209876543, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 1.0, 0.988235294117647, 0.9879518072289156]</td>\n",
       "      <td>0.995179</td>\n",
       "      <td>[0.7, 0.8, 0.5555555555555556, 0.6666666666666666, 0.4444444444444444, 0.4444444444444444, 0.5555555555555556, 0.3333333333333333, 0.5555555555555556, 0.8888888888888888]</td>\n",
       "      <td>59.444444</td>\n",
       "      <td>16.218036</td>\n",
       "      <td>[1.0, 0.8, 0.6, 0.6666666666666666, 0.5, 0.5, 0.5, 0.25, 0.5, 0.8]</td>\n",
       "      <td>0.611667</td>\n",
       "      <td>[0.4, 0.8, 0.6, 0.8, 0.4, 0.2, 0.5, 0.25, 0.5, 1.0]</td>\n",
       "      <td>0.545</td>\n",
       "      <td>[0.5714285714285715, 0.8000000000000002, 0.6, 0.7272727272727272, 0.4444444444444445, 0.28571428571428575, 0.5, 0.25, 0.5, 0.888888888888889]</td>\n",
       "      <td>0.556775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0098869</th>\n",
       "      <td>[0.9927536231884058, 0.9927536231884058, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.855072</td>\n",
       "      <td>[0.9857142857142858, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998571</td>\n",
       "      <td>[1.0, 0.9855072463768116, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>[0.9928057553956835, 0.9927007299270074, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>[0.5, 0.5, 0.75, 0.6875, 0.3333333333333333, 0.4666666666666667, 0.6666666666666666, 0.5333333333333333, 0.7333333333333333, 0.8]</td>\n",
       "      <td>59.708333</td>\n",
       "      <td>14.339691</td>\n",
       "      <td>[0.5, 0.5, 1.0, 0.8, 0.4, 0.5, 0.6666666666666666, 0.5, 0.7142857142857143, 0.8333333333333334]</td>\n",
       "      <td>0.641429</td>\n",
       "      <td>[0.625, 0.5, 0.5, 0.5, 0.5, 0.625, 0.75, 0.2857142857142857, 0.7142857142857143, 0.7142857142857143]</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[0.5555555555555556, 0.5, 0.6666666666666666, 0.6153846153846154, 0.4444444444444445, 0.5555555555555556, 0.7058823529411765, 0.36363636363636365, 0.7142857142857143, 0.7692307692307692]</td>\n",
       "      <td>0.589064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0099503</th>\n",
       "      <td>[0.9703703703703703, 1.0, 1.0, 1.0, 0.9925925925925926, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.62963</td>\n",
       "      <td>[0.9846153846153847, 1.0, 1.0, 1.0, 0.9852941176470589, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>[0.9552238805970149, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.995522</td>\n",
       "      <td>[0.9696969696969696, 1.0, 1.0, 1.0, 0.9925925925925926, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.996229</td>\n",
       "      <td>[0.6666666666666666, 0.5333333333333333, 0.6666666666666666, 0.6666666666666666, 0.5333333333333333, 0.6666666666666666, 0.5333333333333333, 0.4, 0.5333333333333333, 0.26666666666666666]</td>\n",
       "      <td>54.666667</td>\n",
       "      <td>12.578642</td>\n",
       "      <td>[0.6666666666666666, 0.5555555555555556, 0.8, 0.6666666666666666, 0.5555555555555556, 0.625, 0.5, 0.3333333333333333, 0.5, 0.3]</td>\n",
       "      <td>0.550278</td>\n",
       "      <td>[0.75, 0.625, 0.5, 0.75, 0.625, 0.7142857142857143, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855, 0.42857142857142855]</td>\n",
       "      <td>0.539286</td>\n",
       "      <td>[0.7058823529411765, 0.5882352941176471, 0.6153846153846154, 0.7058823529411765, 0.5882352941176471, 0.6666666666666666, 0.36363636363636365, 0.30769230769230765, 0.4615384615384615, 0.35294117647...</td>\n",
       "      <td>0.535609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0106310</th>\n",
       "      <td>[0.995049504950495, 0.9933993399339934, 0.9900990099009901, 0.9933993399339934, 0.9917627677100495, 0.9934102141680395, 0.9868204283360791, 0.9967051070840197, 0.9917627677100495, 0.9917627677100495]</td>\n",
       "      <td>99.241712</td>\n",
       "      <td>[1.0, 0.9901639344262295, 0.9966555183946488, 0.9966777408637874, 0.9901315789473685, 0.9966777408637874, 0.9966329966329966, 1.0, 0.9933993399339934, 0.9966777408637874]</td>\n",
       "      <td>0.995702</td>\n",
       "      <td>[0.9900990099009901, 0.9966996699669967, 0.9834983498349835, 0.9900990099009901, 0.9933993399339934, 0.9900990099009901, 0.976897689768977, 0.993421052631579, 0.9901315789473685, 0.9868421052631579]</td>\n",
       "      <td>0.989119</td>\n",
       "      <td>[0.9950248756218906, 0.9934210526315789, 0.9900332225913622, 0.9933774834437086, 0.9917627677100495, 0.9933774834437086, 0.9866666666666667, 0.9966996699669968, 0.9917627677100495, 0.9917355371900...</td>\n",
       "      <td>0.992386</td>\n",
       "      <td>[0.7058823529411765, 0.75, 0.47058823529411764, 0.6470588235294118, 0.5522388059701493, 0.6417910447761194, 0.6567164179104478, 0.6567164179104478, 0.5671641791044776, 0.6119402985074627]</td>\n",
       "      <td>62.600966</td>\n",
       "      <td>7.588799</td>\n",
       "      <td>[0.6944444444444444, 0.7575757575757576, 0.475, 0.6470588235294118, 0.5666666666666667, 0.65625, 0.627906976744186, 0.65625, 0.55, 0.6060606060606061]</td>\n",
       "      <td>0.623721</td>\n",
       "      <td>[0.7352941176470589, 0.7352941176470589, 0.5588235294117647, 0.6470588235294118, 0.5, 0.6176470588235294, 0.7941176470588235, 0.6363636363636364, 0.6666666666666666, 0.6060606060606061]</td>\n",
       "      <td>0.649733</td>\n",
       "      <td>[0.7142857142857144, 0.746268656716418, 0.5135135135135136, 0.6470588235294118, 0.53125, 0.6363636363636364, 0.7012987012987012, 0.6461538461538462, 0.6027397260273972, 0.6060606060606061]</td>\n",
       "      <td>0.634499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0110165</th>\n",
       "      <td>[0.996, 0.992, 0.992, 0.972, 0.992, 0.984, 0.996, 0.988, 0.9880478087649402, 0.9880478087649402]</td>\n",
       "      <td>98.880956</td>\n",
       "      <td>[0.9920634920634921, 0.992, 0.984251968503937, 0.9538461538461539, 0.984251968503937, 0.9763779527559056, 0.9920634920634921, 0.9765625, 0.9765625, 0.9767441860465116]</td>\n",
       "      <td>0.980472</td>\n",
       "      <td>[1.0, 0.992, 1.0, 0.992, 1.0, 0.992, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>[0.9960159362549801, 0.992, 0.9920634920634921, 0.9725490196078431, 0.9920634920634921, 0.9841269841269842, 0.9960159362549801, 0.9881422924901185, 0.9881422924901185, 0.988235294117647]</td>\n",
       "      <td>0.988935</td>\n",
       "      <td>[0.39285714285714285, 0.4642857142857143, 0.5, 0.5, 0.5714285714285714, 0.6071428571428571, 0.5714285714285714, 0.39285714285714285, 0.6666666666666666, 0.48148148148148145]</td>\n",
       "      <td>51.481481</td>\n",
       "      <td>8.476101</td>\n",
       "      <td>[0.2857142857142857, 0.46153846153846156, 0.5, 0.5, 0.5833333333333334, 0.6153846153846154, 0.5625, 0.4, 0.6666666666666666, 0.45454545454545453]</td>\n",
       "      <td>0.502968</td>\n",
       "      <td>[0.14285714285714285, 0.42857142857142855, 0.5, 0.5714285714285714, 0.5, 0.5714285714285714, 0.6428571428571429, 0.42857142857142855, 0.7142857142857143, 0.38461538461538464]</td>\n",
       "      <td>0.488462</td>\n",
       "      <td>[0.19047619047619047, 0.4444444444444445, 0.5, 0.5333333333333333, 0.5384615384615384, 0.5925925925925927, 0.6000000000000001, 0.4137931034482759, 0.689655172413793, 0.41666666666666663]</td>\n",
       "      <td>0.491942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                           Training Accuracy scores   \n",
       "GO:0000166  [0.9544468546637744, 0.9592190889370933, 0.9605377276669558, 0.9575021682567216, 0.9631396357328708, 0.9640069384215091, 0.9562012142237641, 0.9449262792714658, 0.9531656548135299, 0.9549002601908...  \\\n",
       "GO:0000287                                                                                                          [1.0, 1.0, 1.0, 0.9930555555555556, 1.0, 1.0, 1.0, 0.9930555555555556, 0.9930555555555556, 1.0]   \n",
       "GO:0000325  [0.9758454106280193, 0.9855072463768116, 0.9838969404186796, 0.9903381642512077, 0.9919484702093397, 0.9790660225442834, 0.9806763285024155, 0.9903381642512077, 0.9838969404186796, 0.9935587761674...   \n",
       "GO:0000398                                                                                           [0.9932885906040269, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 0.9933333333333333, 1.0, 0.9933333333333333]   \n",
       "GO:0000976  [0.9910313901345291, 0.9970104633781763, 0.9940209267563528, 0.9895366218236173, 0.9880597014925373, 0.9940298507462687, 0.9940298507462687, 0.9865671641791045, 0.991044776119403, 0.9955223880597015]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                           [1.0, 0.9878048780487805, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 1.0, 0.9879518072289156, 0.9879518072289156]   \n",
       "GO:0098869                                                                                                                         [0.9927536231884058, 0.9927536231884058, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                         [0.9703703703703703, 1.0, 1.0, 1.0, 0.9925925925925926, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.995049504950495, 0.9933993399339934, 0.9900990099009901, 0.9933993399339934, 0.9917627677100495, 0.9934102141680395, 0.9868204283360791, 0.9967051070840197, 0.9917627677100495, 0.9917627677100495]   \n",
       "GO:0110165                                                                                                         [0.996, 0.992, 0.992, 0.972, 0.992, 0.984, 0.996, 0.988, 0.9880478087649402, 0.9880478087649402]   \n",
       "\n",
       "           Mean Training Accuracy   \n",
       "GO:0000166              95.680458  \\\n",
       "GO:0000287              99.791667   \n",
       "GO:0000325              98.550725   \n",
       "GO:0000398              99.732438   \n",
       "GO:0000976              99.208531   \n",
       "...                           ...   \n",
       "GO:0090502              99.516603   \n",
       "GO:0098869              99.855072   \n",
       "GO:0099503               99.62963   \n",
       "GO:0106310              99.241712   \n",
       "GO:0110165              98.880956   \n",
       "\n",
       "                                                                                                                                                                                          Training Precision scores   \n",
       "GO:0000166  [0.9596136962247586, 0.9600347523892268, 0.9633507853403142, 0.9799818016378526, 0.9635416666666666, 0.9709507042253521, 0.9503424657534246, 0.9422413793103448, 0.9539530842745438, 0.9645704162976...  \\\n",
       "GO:0000287                                                                                                                                                       [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000325    [0.9773462783171522, 0.9839228295819936, 0.987012987012987, 0.9935064935064936, 0.9935275080906149, 0.9869281045751634, 0.9776357827476039, 0.987220447284345, 0.990228013029316, 0.9935691318327974]   \n",
       "GO:0000398                                                                                                                                                       [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000976                                                                [0.996969696969697, 1.0, 1.0, 0.9910179640718563, 0.9969604863221885, 1.0, 0.996996996996997, 1.0, 0.993993993993994, 0.9940476190476191]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                         [1.0, 1.0, 1.0, 1.0, 0.9761904761904762, 1.0, 1.0, 1.0, 0.9767441860465116, 1.0]   \n",
       "GO:0098869                                                                                                                                        [0.9857142857142858, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                         [0.9846153846153847, 1.0, 1.0, 1.0, 0.9852941176470589, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310                               [1.0, 0.9901639344262295, 0.9966555183946488, 0.9966777408637874, 0.9901315789473685, 0.9966777408637874, 0.9966329966329966, 1.0, 0.9933993399339934, 0.9966777408637874]   \n",
       "GO:0110165                                  [0.9920634920634921, 0.992, 0.984251968503937, 0.9538461538461539, 0.984251968503937, 0.9763779527559056, 0.9920634920634921, 0.9765625, 0.9765625, 0.9767441860465116]   \n",
       "\n",
       "           Mean Training Precision   \n",
       "GO:0000166                0.960858  \\\n",
       "GO:0000287                     1.0   \n",
       "GO:0000325                 0.98709   \n",
       "GO:0000398                     1.0   \n",
       "GO:0000976                0.996999   \n",
       "...                            ...   \n",
       "GO:0090502                0.995293   \n",
       "GO:0098869                0.998571   \n",
       "GO:0099503                0.996991   \n",
       "GO:0106310                0.995702   \n",
       "GO:0110165                0.980472   \n",
       "\n",
       "                                                                                                                                                                                             Training Recall scores   \n",
       "GO:0000166   [0.9487847222222222, 0.95836947094536, 0.9575021682567216, 0.9340849956634866, 0.9627059843885516, 0.9566348655680833, 0.9627059843885516, 0.9479618386816999, 0.9522983521248916, 0.9444926279271466]  \\\n",
       "GO:0000287                                                                                                          [1.0, 1.0, 1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 0.9861111111111112, 0.9861111111111112, 1.0]   \n",
       "GO:0000325  [0.9741935483870968, 0.9870967741935484, 0.9806451612903225, 0.9870967741935484, 0.9903225806451613, 0.9710610932475884, 0.9839228295819936, 0.9935691318327974, 0.977491961414791, 0.9935691318327974]   \n",
       "GO:0000398                                                                                           [0.9864864864864865, 1.0, 1.0, 1.0, 0.9866666666666667, 1.0, 1.0, 0.9866666666666667, 1.0, 0.9866666666666667]   \n",
       "GO:0000976  [0.9850299401197605, 0.9940119760479041, 0.9880597014925373, 0.9880597014925373, 0.9791044776119403, 0.9880597014925373, 0.991044776119403, 0.9731343283582089, 0.9880597014925373, 0.9970149253731343]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                          [1.0, 0.975609756097561, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9761904761904762]   \n",
       "GO:0098869                                                                                                                                        [1.0, 0.9855072463768116, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                                        [0.9552238805970149, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310   [0.9900990099009901, 0.9966996699669967, 0.9834983498349835, 0.9900990099009901, 0.9933993399339934, 0.9900990099009901, 0.976897689768977, 0.993421052631579, 0.9901315789473685, 0.9868421052631579]   \n",
       "GO:0110165                                                                                                                                                 [1.0, 0.992, 1.0, 0.992, 1.0, 0.992, 1.0, 1.0, 1.0, 1.0]   \n",
       "\n",
       "           Mean Training Recall   \n",
       "GO:0000166             0.952554  \\\n",
       "GO:0000287             0.995833   \n",
       "GO:0000325             0.983897   \n",
       "GO:0000398             0.994649   \n",
       "GO:0000976             0.987158   \n",
       "...                         ...   \n",
       "GO:0090502              0.99518   \n",
       "GO:0098869             0.998551   \n",
       "GO:0099503             0.995522   \n",
       "GO:0106310             0.989119   \n",
       "GO:0110165               0.9976   \n",
       "\n",
       "                                                                                                                                                                                                 Training F1 scores   \n",
       "GO:0000166   [0.9541684853775645, 0.959201388888889, 0.9604175728577643, 0.9564831261101243, 0.9631236442516269, 0.9637396242900831, 0.9564842740198191, 0.945092952875054, 0.9531249999999999, 0.9544259421560036]  \\\n",
       "GO:0000287                                                                                                             [1.0, 1.0, 1.0, 0.993006993006993, 1.0, 1.0, 1.0, 0.993006993006993, 0.993006993006993, 1.0]   \n",
       "GO:0000325  [0.9757673667205171, 0.9855072463768115, 0.9838187702265372, 0.9902912621359222, 0.9919224555735057, 0.978930307941653, 0.9807692307692308, 0.9903846153846154, 0.9838187702265372, 0.9935691318327974]   \n",
       "GO:0000398                                                                                           [0.9931972789115647, 1.0, 1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 0.9932885906040269, 1.0, 0.9932885906040269]   \n",
       "GO:0000976    [0.9909638554216867, 0.996996996996997, 0.993993993993994, 0.9895366218236173, 0.9879518072289157, 0.993993993993994, 0.9940119760479041, 0.9863842662632374, 0.9910179640718564, 0.9955290611028316]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                            [1.0, 0.9876543209876543, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 1.0, 0.988235294117647, 0.9879518072289156]   \n",
       "GO:0098869                                                                                                                         [0.9928057553956835, 0.9927007299270074, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                         [0.9696969696969696, 1.0, 1.0, 1.0, 0.9925925925925926, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.9950248756218906, 0.9934210526315789, 0.9900332225913622, 0.9933774834437086, 0.9917627677100495, 0.9933774834437086, 0.9866666666666667, 0.9966996699669968, 0.9917627677100495, 0.9917355371900...   \n",
       "GO:0110165               [0.9960159362549801, 0.992, 0.9920634920634921, 0.9725490196078431, 0.9920634920634921, 0.9841269841269842, 0.9960159362549801, 0.9881422924901185, 0.9881422924901185, 0.988235294117647]   \n",
       "\n",
       "           Mean Training F1 Score   \n",
       "GO:0000166               0.956626  \\\n",
       "GO:0000287               0.997902   \n",
       "GO:0000325               0.985478   \n",
       "GO:0000398               0.997306   \n",
       "GO:0000976               0.992038   \n",
       "...                           ...   \n",
       "GO:0090502               0.995179   \n",
       "GO:0098869               0.998551   \n",
       "GO:0099503               0.996229   \n",
       "GO:0106310               0.992386   \n",
       "GO:0110165               0.988935   \n",
       "\n",
       "                                                                                                                                                                                         Validation Accuracy scores   \n",
       "GO:0000166                                                                        [0.5719844357976653, 0.5875486381322957, 0.58203125, 0.53125, 0.59765625, 0.56640625, 0.5546875, 0.5859375, 0.58203125, 0.578125]  \\\n",
       "GO:0000287                                                                                                                                   [0.625, 0.625, 0.5625, 0.625, 0.4375, 0.625, 0.5, 0.5, 0.6875, 0.5625]   \n",
       "GO:0000325  [0.7391304347826086, 0.6521739130434783, 0.6666666666666666, 0.6666666666666666, 0.7391304347826086, 0.7101449275362319, 0.6666666666666666, 0.7246376811594203, 0.7536231884057971, 0.7681159420289...   \n",
       "GO:0000398                                                    [0.7647058823529411, 0.5882352941176471, 0.7647058823529411, 0.5882352941176471, 0.7058823529411765, 0.7058823529411765, 0.5625, 0.75, 0.625, 0.6875]   \n",
       "GO:0000976                             [0.6933333333333334, 0.64, 0.5866666666666667, 0.76, 0.7162162162162162, 0.6351351351351351, 0.6756756756756757, 0.6756756756756757, 0.6621621621621622, 0.6621621621621622]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                               [0.7, 0.8, 0.5555555555555556, 0.6666666666666666, 0.4444444444444444, 0.4444444444444444, 0.5555555555555556, 0.3333333333333333, 0.5555555555555556, 0.8888888888888888]   \n",
       "GO:0098869                                                                        [0.5, 0.5, 0.75, 0.6875, 0.3333333333333333, 0.4666666666666667, 0.6666666666666666, 0.5333333333333333, 0.7333333333333333, 0.8]   \n",
       "GO:0099503               [0.6666666666666666, 0.5333333333333333, 0.6666666666666666, 0.6666666666666666, 0.5333333333333333, 0.6666666666666666, 0.5333333333333333, 0.4, 0.5333333333333333, 0.26666666666666666]   \n",
       "GO:0106310              [0.7058823529411765, 0.75, 0.47058823529411764, 0.6470588235294118, 0.5522388059701493, 0.6417910447761194, 0.6567164179104478, 0.6567164179104478, 0.5671641791044776, 0.6119402985074627]   \n",
       "GO:0110165                            [0.39285714285714285, 0.4642857142857143, 0.5, 0.5, 0.5714285714285714, 0.6071428571428571, 0.5714285714285714, 0.39285714285714285, 0.6666666666666666, 0.48148148148148145]   \n",
       "\n",
       "           Mean Validation Accuracy Std Validation Accuracy   \n",
       "GO:0000166                57.376581                1.814778  \\\n",
       "GO:0000287                     57.5                 7.28869   \n",
       "GO:0000325                70.869565                4.018964   \n",
       "GO:0000398                67.426471                7.347792   \n",
       "GO:0000976                 67.07027                4.474677   \n",
       "...                             ...                     ...   \n",
       "GO:0090502                59.444444               16.218036   \n",
       "GO:0098869                59.708333               14.339691   \n",
       "GO:0099503                54.666667               12.578642   \n",
       "GO:0106310                62.600966                7.588799   \n",
       "GO:0110165                51.481481                8.476101   \n",
       "\n",
       "                                                                                                                                                                                        Validation Precision scores   \n",
       "GO:0000166  [0.5736434108527132, 0.5901639344262295, 0.5704697986577181, 0.5298507462686567, 0.5912408759124088, 0.5555555555555556, 0.5486111111111112, 0.5753424657534246, 0.5724137931034483, 0.5714285714285...  \\\n",
       "GO:0000287                                                         [0.625, 0.6666666666666666, 0.5555555555555556, 0.625, 0.4444444444444444, 0.6666666666666666, 0.5, 0.5, 0.7142857142857143, 0.5555555555555556]   \n",
       "GO:0000325  [0.7575757575757576, 0.6571428571428571, 0.6666666666666666, 0.6666666666666666, 0.7297297297297297, 0.6944444444444444, 0.6571428571428571, 0.7586206896551724, 0.7741935483870968, 0.7647058823529...   \n",
       "GO:0000398                                                                         [0.7777777777777778, 0.6, 0.8571428571428571, 0.5555555555555556, 0.8, 0.6153846153846154, 0.5454545454545454, 0.75, 0.625, 0.8]   \n",
       "GO:0000976  [0.6744186046511628, 0.6571428571428571, 0.5681818181818182, 0.8064516129032258, 0.7222222222222222, 0.631578947368421, 0.7241379310344828, 0.6756756756756757, 0.6666666666666666, 0.6578947368421053]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                       [1.0, 0.8, 0.6, 0.6666666666666666, 0.5, 0.5, 0.5, 0.25, 0.5, 0.8]   \n",
       "GO:0098869                                                                                                          [0.5, 0.5, 1.0, 0.8, 0.4, 0.5, 0.6666666666666666, 0.5, 0.7142857142857143, 0.8333333333333334]   \n",
       "GO:0099503                                                                          [0.6666666666666666, 0.5555555555555556, 0.8, 0.6666666666666666, 0.5555555555555556, 0.625, 0.5, 0.3333333333333333, 0.5, 0.3]   \n",
       "GO:0106310                                                   [0.6944444444444444, 0.7575757575757576, 0.475, 0.6470588235294118, 0.5666666666666667, 0.65625, 0.627906976744186, 0.65625, 0.55, 0.6060606060606061]   \n",
       "GO:0110165                                                        [0.2857142857142857, 0.46153846153846156, 0.5, 0.5, 0.5833333333333334, 0.6153846153846154, 0.5625, 0.4, 0.6666666666666666, 0.45454545454545453]   \n",
       "\n",
       "           Mean Validation Precision   \n",
       "GO:0000166                  0.567872  \\\n",
       "GO:0000287                  0.585317   \n",
       "GO:0000325                  0.712689   \n",
       "GO:0000398                  0.692632   \n",
       "GO:0000976                  0.678437   \n",
       "...                              ...   \n",
       "GO:0090502                  0.611667   \n",
       "GO:0098869                  0.641429   \n",
       "GO:0099503                  0.550278   \n",
       "GO:0106310                  0.623721   \n",
       "GO:0110165                  0.502968   \n",
       "\n",
       "                                                                                                                                                                                           Validation Recall scores   \n",
       "GO:0000166                                                                                           [0.5736434108527132, 0.5625, 0.6640625, 0.5546875, 0.6328125, 0.6640625, 0.6171875, 0.65625, 0.6484375, 0.625]  \\\n",
       "GO:0000287                                                                                                                                             [0.625, 0.5, 0.625, 0.625, 0.5, 0.5, 0.5, 0.5, 0.625, 0.625]   \n",
       "GO:0000325  [0.7142857142857143, 0.6571428571428571, 0.6857142857142857, 0.6857142857142857, 0.7714285714285715, 0.7352941176470589, 0.6764705882352942, 0.6470588235294118, 0.7058823529411765, 0.7647058823529...   \n",
       "GO:0000398                                                                                                    [0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.625, 0.5, 1.0, 0.75, 0.75, 0.625, 0.5]   \n",
       "GO:0000976  [0.7631578947368421, 0.6052631578947368, 0.6756756756756757, 0.6756756756756757, 0.7027027027027027, 0.6486486486486487, 0.5675675675675675, 0.6756756756756757, 0.6486486486486487, 0.6756756756756...   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                                      [0.4, 0.8, 0.6, 0.8, 0.4, 0.2, 0.5, 0.25, 0.5, 1.0]   \n",
       "GO:0098869                                                                                                     [0.625, 0.5, 0.5, 0.5, 0.5, 0.625, 0.75, 0.2857142857142857, 0.7142857142857143, 0.7142857142857143]   \n",
       "GO:0099503                                                                    [0.75, 0.625, 0.5, 0.75, 0.625, 0.7142857142857143, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855, 0.42857142857142855]   \n",
       "GO:0106310                [0.7352941176470589, 0.7352941176470589, 0.5588235294117647, 0.6470588235294118, 0.5, 0.6176470588235294, 0.7941176470588235, 0.6363636363636364, 0.6666666666666666, 0.6060606060606061]   \n",
       "GO:0110165                           [0.14285714285714285, 0.42857142857142855, 0.5, 0.5714285714285714, 0.5, 0.5714285714285714, 0.6428571428571429, 0.42857142857142855, 0.7142857142857143, 0.38461538461538464]   \n",
       "\n",
       "           Mean Validation Recall   \n",
       "GO:0000166               0.619864  \\\n",
       "GO:0000287                 0.5625   \n",
       "GO:0000325                0.70437   \n",
       "GO:0000398               0.686111   \n",
       "GO:0000976               0.663869   \n",
       "...                           ...   \n",
       "GO:0090502                  0.545   \n",
       "GO:0098869               0.571429   \n",
       "GO:0099503               0.539286   \n",
       "GO:0106310               0.649733   \n",
       "GO:0110165               0.488462   \n",
       "\n",
       "                                                                                                                                                                                               Validation F1 scores   \n",
       "GO:0000166              [0.5736434108527132, 0.576, 0.6137184115523465, 0.5419847328244274, 0.6113207547169812, 0.6049822064056939, 0.5808823529411764, 0.6131386861313868, 0.6080586080586081, 0.5970149253731343]  \\\n",
       "GO:0000287                                                        [0.625, 0.5714285714285715, 0.5882352941176471, 0.625, 0.47058823529411764, 0.5714285714285715, 0.5, 0.5, 0.6666666666666666, 0.5882352941176471]   \n",
       "GO:0000325                 [0.7352941176470589, 0.6571428571428571, 0.676056338028169, 0.676056338028169, 0.75, 0.7142857142857144, 0.6666666666666666, 0.6984126984126984, 0.7384615384615385, 0.7647058823529412]   \n",
       "GO:0000398                                             [0.7777777777777778, 0.631578947368421, 0.75, 0.5882352941176471, 0.6153846153846154, 0.761904761904762, 0.631578947368421, 0.75, 0.625, 0.6153846153846154]   \n",
       "GO:0000976                [0.7160493827160495, 0.6301369863013698, 0.617283950617284, 0.7352941176470588, 0.7123287671232876, 0.64, 0.6363636363636365, 0.6756756756756757, 0.6575342465753425, 0.6666666666666667]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                            [0.5714285714285715, 0.8000000000000002, 0.6, 0.7272727272727272, 0.4444444444444445, 0.28571428571428575, 0.5, 0.25, 0.5, 0.888888888888889]   \n",
       "GO:0098869               [0.5555555555555556, 0.5, 0.6666666666666666, 0.6153846153846154, 0.4444444444444445, 0.5555555555555556, 0.7058823529411765, 0.36363636363636365, 0.7142857142857143, 0.7692307692307692]   \n",
       "GO:0099503  [0.7058823529411765, 0.5882352941176471, 0.6153846153846154, 0.7058823529411765, 0.5882352941176471, 0.6666666666666666, 0.36363636363636365, 0.30769230769230765, 0.4615384615384615, 0.35294117647...   \n",
       "GO:0106310             [0.7142857142857144, 0.746268656716418, 0.5135135135135136, 0.6470588235294118, 0.53125, 0.6363636363636364, 0.7012987012987012, 0.6461538461538462, 0.6027397260273972, 0.6060606060606061]   \n",
       "GO:0110165               [0.19047619047619047, 0.4444444444444445, 0.5, 0.5333333333333333, 0.5384615384615384, 0.5925925925925927, 0.6000000000000001, 0.4137931034482759, 0.689655172413793, 0.41666666666666663]   \n",
       "\n",
       "           Mean Validation F1 Score  \n",
       "GO:0000166                 0.592074  \n",
       "GO:0000287                 0.570658  \n",
       "GO:0000325                 0.707708  \n",
       "GO:0000398                 0.674684  \n",
       "GO:0000976                 0.668733  \n",
       "...                             ...  \n",
       "GO:0090502                 0.556775  \n",
       "GO:0098869                 0.589064  \n",
       "GO:0099503                 0.535609  \n",
       "GO:0106310                 0.634499  \n",
       "GO:0110165                 0.491942  \n",
       "\n",
       "[227 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CB_Root33_results_df_t7 = pd.DataFrame(CB_Root33_results_dict).T\n",
    "csv_file_path = '28Jan2024_DAPSeq0.7_Root33_results_df.csv'\n",
    "CB_Root33_results_df_t7.to_csv(csv_file_path, index_label='GO')\n",
    "CB_Root33_results_df_t7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68b99e0e-9a92-46df-beec-d4250d8fa5ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy scores</th>\n",
       "      <th>Mean Training Accuracy</th>\n",
       "      <th>Training Precision scores</th>\n",
       "      <th>Mean Training Precision</th>\n",
       "      <th>Training Recall scores</th>\n",
       "      <th>Mean Training Recall</th>\n",
       "      <th>Training F1 scores</th>\n",
       "      <th>Mean Training F1 Score</th>\n",
       "      <th>Validation Accuracy scores</th>\n",
       "      <th>Mean Validation Accuracy</th>\n",
       "      <th>Std Validation Accuracy</th>\n",
       "      <th>Validation Precision scores</th>\n",
       "      <th>Mean Validation Precision</th>\n",
       "      <th>Validation Recall scores</th>\n",
       "      <th>Mean Validation Recall</th>\n",
       "      <th>Validation F1 scores</th>\n",
       "      <th>Mean Validation F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0000166</th>\n",
       "      <td>[0.9527114967462039, 0.9683297180043384, 0.9531656548135299, 0.9648742411101474, 0.9627059843885516, 0.9540329575021682, 0.9527320034692107, 0.9570685169124025, 0.9605377276669558, 0.9657415437987...</td>\n",
       "      <td>95.918998</td>\n",
       "      <td>[0.9554585152838428, 0.9753521126760564, 0.9611650485436893, 0.981149012567325, 0.9717064544650752, 0.9540329575021682, 0.9547038327526133, 0.9598603839441536, 0.9682539682539683, 0.9718804920913884]</td>\n",
       "      <td>0.965356</td>\n",
       "      <td>[0.9496527777777778, 0.9609713790112749, 0.9444926279271466, 0.9479618386816999, 0.9531656548135299, 0.9540329575021682, 0.9505637467476149, 0.9540329575021682, 0.9522983521248916, 0.9592367736339...</td>\n",
       "      <td>0.952641</td>\n",
       "      <td>[0.9525468001741403, 0.9681083442551333, 0.952755905511811, 0.9642699602999559, 0.9623467600700526, 0.9540329575021682, 0.9526292916123424, 0.9569377990430623, 0.960209881941408, 0.9655172413793104]</td>\n",
       "      <td>0.958935</td>\n",
       "      <td>[0.5486381322957199, 0.5680933852140078, 0.6015625, 0.59375, 0.59375, 0.609375, 0.53515625, 0.58203125, 0.546875, 0.59375]</td>\n",
       "      <td>57.729815</td>\n",
       "      <td>2.461647</td>\n",
       "      <td>[0.5503875968992248, 0.568, 0.5902777777777778, 0.5857142857142857, 0.5895522388059702, 0.6029411764705882, 0.5319148936170213, 0.5695364238410596, 0.5454545454545454, 0.5821917808219178]</td>\n",
       "      <td>0.571597</td>\n",
       "      <td>[0.5503875968992248, 0.5546875, 0.6640625, 0.640625, 0.6171875, 0.640625, 0.5859375, 0.671875, 0.5625, 0.6640625]</td>\n",
       "      <td>0.615195</td>\n",
       "      <td>[0.5503875968992248, 0.5612648221343872, 0.6250000000000001, 0.6119402985074627, 0.6030534351145038, 0.6212121212121211, 0.5576208178438662, 0.6164874551971327, 0.5538461538461538, 0.6204379562043...</td>\n",
       "      <td>0.592125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000287</th>\n",
       "      <td>[1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 0.9930555555555556, 1.0, 0.9930555555555556, 0.9930555555555556, 1.0]</td>\n",
       "      <td>99.652778</td>\n",
       "      <td>[1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998611</td>\n",
       "      <td>[1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 0.9861111111111112, 1.0, 0.9861111111111112, 0.9861111111111112, 1.0]</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>[1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 0.993006993006993, 1.0, 0.993006993006993, 0.993006993006993, 1.0]</td>\n",
       "      <td>0.996513</td>\n",
       "      <td>[0.4375, 0.5625, 0.375, 0.625, 0.4375, 0.625, 0.6875, 0.4375, 0.8125, 0.5625]</td>\n",
       "      <td>55.625</td>\n",
       "      <td>12.945197</td>\n",
       "      <td>[0.4444444444444444, 0.5714285714285714, 0.25, 0.625, 0.4444444444444444, 0.625, 0.6666666666666666, 0.42857142857142855, 1.0, 0.5714285714285714]</td>\n",
       "      <td>0.562698</td>\n",
       "      <td>[0.5, 0.5, 0.125, 0.625, 0.5, 0.625, 0.75, 0.375, 0.625, 0.5]</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>[0.47058823529411764, 0.5333333333333333, 0.16666666666666666, 0.625, 0.47058823529411764, 0.625, 0.7058823529411765, 0.39999999999999997, 0.7692307692307693, 0.5333333333333333]</td>\n",
       "      <td>0.529962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000325</th>\n",
       "      <td>[0.9790660225442834, 0.9855072463768116, 0.9774557165861514, 0.9855072463768116, 0.9935587761674718, 0.9855072463768116, 0.9919484702093397, 0.9967793880837359, 0.9871175523349437, 0.9806763285024...</td>\n",
       "      <td>98.63124</td>\n",
       "      <td>[0.9805825242718447, 0.9870550161812298, 0.9868421052631579, 0.9934426229508196, 0.9872611464968153, 0.9839743589743589, 0.9935483870967742, 0.9967845659163987, 0.9902912621359223, 0.9776357827476...</td>\n",
       "      <td>0.987742</td>\n",
       "      <td>[0.9774193548387097, 0.9838709677419355, 0.967741935483871, 0.9774193548387097, 1.0, 0.9871382636655949, 0.9903536977491961, 0.9967845659163987, 0.9839228295819936, 0.9839228295819936]</td>\n",
       "      <td>0.984857</td>\n",
       "      <td>[0.9789983844911146, 0.9854604200323102, 0.9771986970684038, 0.9853658536585366, 0.9935897435897436, 0.985553772070626, 0.9919484702093397, 0.9967845659163987, 0.9870967741935485, 0.9807692307692308]</td>\n",
       "      <td>0.986277</td>\n",
       "      <td>[0.7246376811594203, 0.6956521739130435, 0.7101449275362319, 0.7101449275362319, 0.7681159420289855, 0.6956521739130435, 0.7101449275362319, 0.6956521739130435, 0.7536231884057971, 0.7536231884057...</td>\n",
       "      <td>72.173913</td>\n",
       "      <td>2.576288</td>\n",
       "      <td>[0.7352941176470589, 0.6944444444444444, 0.7027027027027027, 0.7027027027027027, 0.7567567567567568, 0.6666666666666666, 0.71875, 0.696969696969697, 0.7297297297297297, 0.7297297297297297]</td>\n",
       "      <td>0.713375</td>\n",
       "      <td>[0.7142857142857143, 0.7142857142857143, 0.7428571428571429, 0.7428571428571429, 0.8, 0.7647058823529411, 0.6764705882352942, 0.6764705882352942, 0.7941176470588235, 0.7941176470588235]</td>\n",
       "      <td>0.742017</td>\n",
       "      <td>[0.7246376811594202, 0.7042253521126761, 0.7222222222222223, 0.7222222222222223, 0.7777777777777778, 0.7123287671232877, 0.696969696969697, 0.6865671641791046, 0.7605633802816901, 0.7605633802816901]</td>\n",
       "      <td>0.726808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000398</th>\n",
       "      <td>[0.9932885906040269, 1.0, 0.9932885906040269, 0.9932885906040269, 0.9865771812080537, 1.0, 0.9933333333333333, 1.0, 1.0, 0.9933333333333333]</td>\n",
       "      <td>99.531096</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9868421052631579, 0.9866666666666667, 1.0, 0.9868421052631579, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.996035</td>\n",
       "      <td>[0.9864864864864865, 1.0, 0.9864864864864865, 1.0, 0.9866666666666667, 1.0, 1.0, 1.0, 1.0, 0.9866666666666667]</td>\n",
       "      <td>0.994631</td>\n",
       "      <td>[0.9931972789115647, 1.0, 0.9931972789115647, 0.9933774834437086, 0.9866666666666668, 1.0, 0.9933774834437086, 1.0, 1.0, 0.9932885906040269]</td>\n",
       "      <td>0.99531</td>\n",
       "      <td>[0.7058823529411765, 0.6470588235294118, 0.7058823529411765, 0.5294117647058824, 0.7058823529411765, 0.7647058823529411, 0.6875, 0.6875, 0.6875, 0.8125]</td>\n",
       "      <td>69.338235</td>\n",
       "      <td>6.991483</td>\n",
       "      <td>[0.75, 0.6363636363636364, 0.8333333333333334, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6363636363636364, 0.6666666666666666, 0.6666666666666666, 1.0]</td>\n",
       "      <td>0.702273</td>\n",
       "      <td>[0.6666666666666666, 0.7777777777777778, 0.5555555555555556, 0.5, 0.75, 1.0, 0.875, 0.75, 0.75, 0.625]</td>\n",
       "      <td>0.725</td>\n",
       "      <td>[0.7058823529411765, 0.7000000000000001, 0.6666666666666667, 0.5, 0.7058823529411765, 0.8, 0.7368421052631579, 0.7058823529411765, 0.7058823529411765, 0.7692307692307693]</td>\n",
       "      <td>0.699627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000976</th>\n",
       "      <td>[0.992526158445441, 0.9940209267563528, 0.9955156950672646, 0.9940209267563528, 0.9880597014925373, 0.9925373134328358, 0.9895522388059701, 0.991044776119403, 0.9925373134328358, 0.991044776119403]</td>\n",
       "      <td>99.208598</td>\n",
       "      <td>[0.9969788519637462, 0.9940119760479041, 0.9940476190476191, 0.996996996996997, 0.9969604863221885, 0.9969879518072289, 0.996969696969697, 0.991044776119403, 0.9969879518072289, 0.9969788519637462]</td>\n",
       "      <td>0.995797</td>\n",
       "      <td>[0.9880239520958084, 0.9940119760479041, 0.9970149253731343, 0.991044776119403, 0.9791044776119403, 0.9880597014925373, 0.982089552238806, 0.991044776119403, 0.9880597014925373, 0.9850746268656716]</td>\n",
       "      <td>0.988353</td>\n",
       "      <td>[0.9924812030075189, 0.9940119760479041, 0.9955290611028316, 0.9940119760479041, 0.9879518072289157, 0.9925037481259371, 0.9894736842105263, 0.991044776119403, 0.9925037481259371, 0.990990990990991]</td>\n",
       "      <td>0.99205</td>\n",
       "      <td>[0.68, 0.64, 0.5333333333333333, 0.76, 0.7027027027027027, 0.7837837837837838, 0.7162162162162162, 0.7972972972972973, 0.6351351351351351, 0.6621621621621622]</td>\n",
       "      <td>69.106306</td>\n",
       "      <td>7.548179</td>\n",
       "      <td>[0.6944444444444444, 0.6170212765957447, 0.5208333333333334, 0.7714285714285715, 0.6923076923076923, 0.8, 0.8333333333333334, 0.775, 0.631578947368421, 0.6666666666666666]</td>\n",
       "      <td>0.700261</td>\n",
       "      <td>[0.6578947368421053, 0.7631578947368421, 0.6756756756756757, 0.7297297297297297, 0.7297297297297297, 0.7567567567567568, 0.5405405405405406, 0.8378378378378378, 0.6486486486486487, 0.6486486486486...</td>\n",
       "      <td>0.698862</td>\n",
       "      <td>[0.6756756756756757, 0.6823529411764706, 0.5882352941176472, 0.75, 0.7105263157894737, 0.7777777777777778, 0.6557377049180328, 0.8051948051948051, 0.64, 0.6575342465753425]</td>\n",
       "      <td>0.694303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0090502</th>\n",
       "      <td>[0.9878048780487805, 1.0, 1.0, 1.0, 0.9759036144578314, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0]</td>\n",
       "      <td>99.516603</td>\n",
       "      <td>[0.9761904761904762, 1.0, 1.0, 1.0, 0.975609756097561, 1.0, 1.0, 0.9767441860465116, 1.0, 1.0]</td>\n",
       "      <td>0.992854</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.975609756097561, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.997561</td>\n",
       "      <td>[0.9879518072289156, 1.0, 1.0, 1.0, 0.975609756097561, 1.0, 1.0, 0.988235294117647, 1.0, 1.0]</td>\n",
       "      <td>0.99518</td>\n",
       "      <td>[0.7, 0.7, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.4444444444444444, 0.5555555555555556, 0.7777777777777778, 0.6666666666666666]</td>\n",
       "      <td>59.555556</td>\n",
       "      <td>10.590002</td>\n",
       "      <td>[0.6666666666666666, 0.625, 0.6, 0.5714285714285714, 0.6666666666666666, 0.5, 0.3333333333333333, 0.5, 0.75, 0.5714285714285714]</td>\n",
       "      <td>0.578452</td>\n",
       "      <td>[0.8, 1.0, 0.6, 0.8, 0.4, 0.2, 0.25, 0.5, 0.75, 1.0]</td>\n",
       "      <td>0.63</td>\n",
       "      <td>[0.7272727272727272, 0.7692307692307693, 0.6, 0.6666666666666666, 0.5, 0.28571428571428575, 0.28571428571428575, 0.5, 0.75, 0.7272727272727273]</td>\n",
       "      <td>0.581187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0098869</th>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9927536231884058, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.927536</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9857142857142858, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998571</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9928057553956835, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.999281</td>\n",
       "      <td>[0.5625, 0.75, 0.75, 0.5625, 0.6666666666666666, 0.4666666666666667, 0.4666666666666667, 0.6, 0.8, 0.6666666666666666]</td>\n",
       "      <td>62.916667</td>\n",
       "      <td>11.124298</td>\n",
       "      <td>[0.5555555555555556, 0.7, 1.0, 0.6, 0.7142857142857143, 0.5, 0.5, 0.6, 0.8333333333333334, 0.6666666666666666]</td>\n",
       "      <td>0.666984</td>\n",
       "      <td>[0.625, 0.875, 0.5, 0.375, 0.625, 0.625, 0.5, 0.42857142857142855, 0.7142857142857143, 0.5714285714285714]</td>\n",
       "      <td>0.583929</td>\n",
       "      <td>[0.5882352941176471, 0.7777777777777777, 0.6666666666666666, 0.4615384615384615, 0.6666666666666666, 0.5555555555555556, 0.5, 0.5, 0.7692307692307692, 0.6153846153846153]</td>\n",
       "      <td>0.610106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0099503</th>\n",
       "      <td>[1.0, 1.0, 0.9925925925925926, 1.0, 0.9925925925925926, 0.9925925925925926, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.777778</td>\n",
       "      <td>[1.0, 1.0, 0.9852941176470589, 1.0, 1.0, 0.9855072463768116, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.99708</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.9850746268656716, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998507</td>\n",
       "      <td>[1.0, 1.0, 0.9925925925925926, 1.0, 0.9924812030075187, 0.9927007299270074, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.997777</td>\n",
       "      <td>[0.5333333333333333, 0.4666666666666667, 0.8666666666666667, 0.6666666666666666, 0.5333333333333333, 0.6, 0.6, 0.5333333333333333, 0.5333333333333333, 0.4]</td>\n",
       "      <td>57.333333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>[0.5454545454545454, 0.5, 1.0, 0.6666666666666666, 0.5714285714285714, 0.5555555555555556, 0.5714285714285714, 0.5, 0.5, 0.3333333333333333]</td>\n",
       "      <td>0.574387</td>\n",
       "      <td>[0.75, 0.375, 0.75, 0.75, 0.5, 0.7142857142857143, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.2857142857142857]</td>\n",
       "      <td>0.569643</td>\n",
       "      <td>[0.631578947368421, 0.42857142857142855, 0.8571428571428571, 0.7058823529411765, 0.5333333333333333, 0.6250000000000001, 0.5714285714285714, 0.5333333333333333, 0.4615384615384615, 0.3076923076923...</td>\n",
       "      <td>0.56555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0106310</th>\n",
       "      <td>[0.995049504950495, 0.9917491749174917, 0.9884488448844885, 0.9966996699669967, 0.9901153212520593, 0.9917627677100495, 0.9901153212520593, 0.9950576606260296, 0.9934102141680395, 0.9917627677100495]</td>\n",
       "      <td>99.241712</td>\n",
       "      <td>[0.9901960784313726, 0.9901315789473685, 0.9836601307189542, 1.0, 0.9933554817275747, 0.9966666666666667, 0.9966555183946488, 0.9934426229508196, 0.9966887417218543, 0.9933993399339934]</td>\n",
       "      <td>0.99342</td>\n",
       "      <td>[1.0, 0.9933993399339934, 0.9933993399339934, 0.9933993399339934, 0.9867986798679867, 0.9867986798679867, 0.9834983498349835, 0.9967105263157895, 0.9901315789473685, 0.9901315789473685]</td>\n",
       "      <td>0.991427</td>\n",
       "      <td>[0.9950738916256158, 0.9917627677100495, 0.9885057471264367, 0.9966887417218543, 0.9900662251655629, 0.9917081260364843, 0.9900332225913622, 0.9950738916256158, 0.9933993399339934, 0.9917627677100...</td>\n",
       "      <td>0.992407</td>\n",
       "      <td>[0.7794117647058824, 0.7352941176470589, 0.5735294117647058, 0.5147058823529411, 0.5373134328358209, 0.5671641791044776, 0.5671641791044776, 0.6417910447761194, 0.6716417910447762, 0.5373134328358...</td>\n",
       "      <td>61.253292</td>\n",
       "      <td>8.590271</td>\n",
       "      <td>[0.7714285714285715, 0.7352941176470589, 0.5757575757575758, 0.5161290322580645, 0.5384615384615384, 0.5714285714285714, 0.5581395348837209, 0.6451612903225806, 0.6486486486486487, 0.53125]</td>\n",
       "      <td>0.60917</td>\n",
       "      <td>[0.7941176470588235, 0.7352941176470589, 0.5588235294117647, 0.47058823529411764, 0.6176470588235294, 0.5882352941176471, 0.7058823529411765, 0.6060606060606061, 0.7272727272727273, 0.515151515151...</td>\n",
       "      <td>0.631907</td>\n",
       "      <td>[0.782608695652174, 0.735294117647059, 0.5671641791044776, 0.4923076923076923, 0.5753424657534247, 0.5797101449275363, 0.6233766233766234, 0.625, 0.6857142857142857, 0.5230769230769231]</td>\n",
       "      <td>0.61896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0110165</th>\n",
       "      <td>[0.984, 0.992, 0.992, 0.988, 0.992, 0.98, 0.988, 0.988, 0.9880478087649402, 0.9920318725099602]</td>\n",
       "      <td>98.840797</td>\n",
       "      <td>[0.9689922480620154, 0.984251968503937, 0.984251968503937, 0.9765625, 0.984251968503937, 0.9615384615384616, 0.9765625, 0.9765625, 0.9765625, 0.984375]</td>\n",
       "      <td>0.977391</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.9842519685039369, 0.9920634920634921, 0.9920634920634921, 0.9881422924901185, 0.9920634920634921, 0.9803921568627451, 0.9881422924901185, 0.9881422924901185, 0.9881422924901185, 0.9921259842519...</td>\n",
       "      <td>0.988553</td>\n",
       "      <td>[0.6428571428571429, 0.42857142857142855, 0.5, 0.4642857142857143, 0.5357142857142857, 0.5, 0.5357142857142857, 0.5714285714285714, 0.5555555555555556, 0.5555555555555556]</td>\n",
       "      <td>52.896825</td>\n",
       "      <td>5.676268</td>\n",
       "      <td>[0.6666666666666666, 0.4375, 0.5, 0.47058823529411764, 0.5263157894736842, 0.5, 0.5263157894736842, 0.5833333333333334, 0.6, 0.5454545454545454]</td>\n",
       "      <td>0.535617</td>\n",
       "      <td>[0.5714285714285714, 0.5, 0.5, 0.5714285714285714, 0.7142857142857143, 0.5, 0.7142857142857143, 0.5, 0.42857142857142855, 0.46153846153846156]</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>[0.6153846153846153, 0.4666666666666667, 0.5, 0.5161290322580646, 0.6060606060606061, 0.5, 0.6060606060606061, 0.5384615384615384, 0.5, 0.4999999999999999]</td>\n",
       "      <td>0.534876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                           Training Accuracy scores   \n",
       "GO:0000166  [0.9527114967462039, 0.9683297180043384, 0.9531656548135299, 0.9648742411101474, 0.9627059843885516, 0.9540329575021682, 0.9527320034692107, 0.9570685169124025, 0.9605377276669558, 0.9657415437987...  \\\n",
       "GO:0000287                                                                                           [1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 0.9930555555555556, 1.0, 0.9930555555555556, 0.9930555555555556, 1.0]   \n",
       "GO:0000325  [0.9790660225442834, 0.9855072463768116, 0.9774557165861514, 0.9855072463768116, 0.9935587761674718, 0.9855072463768116, 0.9919484702093397, 0.9967793880837359, 0.9871175523349437, 0.9806763285024...   \n",
       "GO:0000398                                                             [0.9932885906040269, 1.0, 0.9932885906040269, 0.9932885906040269, 0.9865771812080537, 1.0, 0.9933333333333333, 1.0, 1.0, 0.9933333333333333]   \n",
       "GO:0000976    [0.992526158445441, 0.9940209267563528, 0.9955156950672646, 0.9940209267563528, 0.9880597014925373, 0.9925373134328358, 0.9895522388059701, 0.991044776119403, 0.9925373134328358, 0.991044776119403]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                          [0.9878048780487805, 1.0, 1.0, 1.0, 0.9759036144578314, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                                        [1.0, 1.0, 1.0, 0.9927536231884058, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                          [1.0, 1.0, 0.9925925925925926, 1.0, 0.9925925925925926, 0.9925925925925926, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.995049504950495, 0.9917491749174917, 0.9884488448844885, 0.9966996699669967, 0.9901153212520593, 0.9917627677100495, 0.9901153212520593, 0.9950576606260296, 0.9934102141680395, 0.9917627677100495]   \n",
       "GO:0110165                                                                                                          [0.984, 0.992, 0.992, 0.988, 0.992, 0.98, 0.988, 0.988, 0.9880478087649402, 0.9920318725099602]   \n",
       "\n",
       "           Mean Training Accuracy   \n",
       "GO:0000166              95.918998  \\\n",
       "GO:0000287              99.652778   \n",
       "GO:0000325               98.63124   \n",
       "GO:0000398              99.531096   \n",
       "GO:0000976              99.208598   \n",
       "...                           ...   \n",
       "GO:0090502              99.516603   \n",
       "GO:0098869              99.927536   \n",
       "GO:0099503              99.777778   \n",
       "GO:0106310              99.241712   \n",
       "GO:0110165              98.840797   \n",
       "\n",
       "                                                                                                                                                                                          Training Precision scores   \n",
       "GO:0000166  [0.9554585152838428, 0.9753521126760564, 0.9611650485436893, 0.981149012567325, 0.9717064544650752, 0.9540329575021682, 0.9547038327526133, 0.9598603839441536, 0.9682539682539683, 0.9718804920913884]  \\\n",
       "GO:0000287                                                                                                                                        [1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000325  [0.9805825242718447, 0.9870550161812298, 0.9868421052631579, 0.9934426229508196, 0.9872611464968153, 0.9839743589743589, 0.9935483870967742, 0.9967845659163987, 0.9902912621359223, 0.9776357827476...   \n",
       "GO:0000398                                                                                                          [1.0, 1.0, 1.0, 0.9868421052631579, 0.9866666666666667, 1.0, 0.9868421052631579, 1.0, 1.0, 1.0]   \n",
       "GO:0000976    [0.9969788519637462, 0.9940119760479041, 0.9940476190476191, 0.996996996996997, 0.9969604863221885, 0.9969879518072289, 0.996969696969697, 0.991044776119403, 0.9969879518072289, 0.9969788519637462]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                           [0.9761904761904762, 1.0, 1.0, 1.0, 0.975609756097561, 1.0, 1.0, 0.9767441860465116, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                                        [1.0, 1.0, 1.0, 0.9857142857142858, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                         [1.0, 1.0, 0.9852941176470589, 1.0, 1.0, 0.9855072463768116, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310                [0.9901960784313726, 0.9901315789473685, 0.9836601307189542, 1.0, 0.9933554817275747, 0.9966666666666667, 0.9966555183946488, 0.9934426229508196, 0.9966887417218543, 0.9933993399339934]   \n",
       "GO:0110165                                                  [0.9689922480620154, 0.984251968503937, 0.984251968503937, 0.9765625, 0.984251968503937, 0.9615384615384616, 0.9765625, 0.9765625, 0.9765625, 0.984375]   \n",
       "\n",
       "           Mean Training Precision   \n",
       "GO:0000166                0.965356  \\\n",
       "GO:0000287                0.998611   \n",
       "GO:0000325                0.987742   \n",
       "GO:0000398                0.996035   \n",
       "GO:0000976                0.995797   \n",
       "...                            ...   \n",
       "GO:0090502                0.992854   \n",
       "GO:0098869                0.998571   \n",
       "GO:0099503                 0.99708   \n",
       "GO:0106310                 0.99342   \n",
       "GO:0110165                0.977391   \n",
       "\n",
       "                                                                                                                                                                                             Training Recall scores   \n",
       "GO:0000166  [0.9496527777777778, 0.9609713790112749, 0.9444926279271466, 0.9479618386816999, 0.9531656548135299, 0.9540329575021682, 0.9505637467476149, 0.9540329575021682, 0.9522983521248916, 0.9592367736339...  \\\n",
       "GO:0000287                                                                                           [1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 0.9861111111111112, 1.0, 0.9861111111111112, 0.9861111111111112, 1.0]   \n",
       "GO:0000325                 [0.9774193548387097, 0.9838709677419355, 0.967741935483871, 0.9774193548387097, 1.0, 0.9871382636655949, 0.9903536977491961, 0.9967845659163987, 0.9839228295819936, 0.9839228295819936]   \n",
       "GO:0000398                                                                                           [0.9864864864864865, 1.0, 0.9864864864864865, 1.0, 0.9866666666666667, 1.0, 1.0, 1.0, 1.0, 0.9866666666666667]   \n",
       "GO:0000976    [0.9880239520958084, 0.9940119760479041, 0.9970149253731343, 0.991044776119403, 0.9791044776119403, 0.9880597014925373, 0.982089552238806, 0.991044776119403, 0.9880597014925373, 0.9850746268656716]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                         [1.0, 1.0, 1.0, 1.0, 0.975609756097561, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                                                       [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                                        [1.0, 1.0, 1.0, 1.0, 0.9850746268656716, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310                [1.0, 0.9933993399339934, 0.9933993399339934, 0.9933993399339934, 0.9867986798679867, 0.9867986798679867, 0.9834983498349835, 0.9967105263157895, 0.9901315789473685, 0.9901315789473685]   \n",
       "GO:0110165                                                                                                                                                       [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "\n",
       "           Mean Training Recall   \n",
       "GO:0000166             0.952641  \\\n",
       "GO:0000287             0.994444   \n",
       "GO:0000325             0.984857   \n",
       "GO:0000398             0.994631   \n",
       "GO:0000976             0.988353   \n",
       "...                         ...   \n",
       "GO:0090502             0.997561   \n",
       "GO:0098869                  1.0   \n",
       "GO:0099503             0.998507   \n",
       "GO:0106310             0.991427   \n",
       "GO:0110165                  1.0   \n",
       "\n",
       "                                                                                                                                                                                                 Training F1 scores   \n",
       "GO:0000166   [0.9525468001741403, 0.9681083442551333, 0.952755905511811, 0.9642699602999559, 0.9623467600700526, 0.9540329575021682, 0.9526292916123424, 0.9569377990430623, 0.960209881941408, 0.9655172413793104]  \\\n",
       "GO:0000287                                                                                              [1.0, 0.9861111111111112, 1.0, 1.0, 1.0, 0.993006993006993, 1.0, 0.993006993006993, 0.993006993006993, 1.0]   \n",
       "GO:0000325  [0.9789983844911146, 0.9854604200323102, 0.9771986970684038, 0.9853658536585366, 0.9935897435897436, 0.985553772070626, 0.9919484702093397, 0.9967845659163987, 0.9870967741935485, 0.9807692307692308]   \n",
       "GO:0000398                                                             [0.9931972789115647, 1.0, 0.9931972789115647, 0.9933774834437086, 0.9866666666666668, 1.0, 0.9933774834437086, 1.0, 1.0, 0.9932885906040269]   \n",
       "GO:0000976   [0.9924812030075189, 0.9940119760479041, 0.9955290611028316, 0.9940119760479041, 0.9879518072289157, 0.9925037481259371, 0.9894736842105263, 0.991044776119403, 0.9925037481259371, 0.990990990990991]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                            [0.9879518072289156, 1.0, 1.0, 1.0, 0.975609756097561, 1.0, 1.0, 0.988235294117647, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                                        [1.0, 1.0, 1.0, 0.9928057553956835, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                          [1.0, 1.0, 0.9925925925925926, 1.0, 0.9924812030075187, 0.9927007299270074, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.9950738916256158, 0.9917627677100495, 0.9885057471264367, 0.9966887417218543, 0.9900662251655629, 0.9917081260364843, 0.9900332225913622, 0.9950738916256158, 0.9933993399339934, 0.9917627677100...   \n",
       "GO:0110165  [0.9842519685039369, 0.9920634920634921, 0.9920634920634921, 0.9881422924901185, 0.9920634920634921, 0.9803921568627451, 0.9881422924901185, 0.9881422924901185, 0.9881422924901185, 0.9921259842519...   \n",
       "\n",
       "           Mean Training F1 Score   \n",
       "GO:0000166               0.958935  \\\n",
       "GO:0000287               0.996513   \n",
       "GO:0000325               0.986277   \n",
       "GO:0000398                0.99531   \n",
       "GO:0000976                0.99205   \n",
       "...                           ...   \n",
       "GO:0090502                0.99518   \n",
       "GO:0098869               0.999281   \n",
       "GO:0099503               0.997777   \n",
       "GO:0106310               0.992407   \n",
       "GO:0110165               0.988553   \n",
       "\n",
       "                                                                                                                                                                                         Validation Accuracy scores   \n",
       "GO:0000166                                                                               [0.5486381322957199, 0.5680933852140078, 0.6015625, 0.59375, 0.59375, 0.609375, 0.53515625, 0.58203125, 0.546875, 0.59375]  \\\n",
       "GO:0000287                                                                                                                            [0.4375, 0.5625, 0.375, 0.625, 0.4375, 0.625, 0.6875, 0.4375, 0.8125, 0.5625]   \n",
       "GO:0000325  [0.7246376811594203, 0.6956521739130435, 0.7101449275362319, 0.7101449275362319, 0.7681159420289855, 0.6956521739130435, 0.7101449275362319, 0.6956521739130435, 0.7536231884057971, 0.7536231884057...   \n",
       "GO:0000398                                                 [0.7058823529411765, 0.6470588235294118, 0.7058823529411765, 0.5294117647058824, 0.7058823529411765, 0.7647058823529411, 0.6875, 0.6875, 0.6875, 0.8125]   \n",
       "GO:0000976                                           [0.68, 0.64, 0.5333333333333333, 0.76, 0.7027027027027027, 0.7837837837837838, 0.7162162162162162, 0.7972972972972973, 0.6351351351351351, 0.6621621621621622]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                               [0.7, 0.7, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.4444444444444444, 0.5555555555555556, 0.7777777777777778, 0.6666666666666666]   \n",
       "GO:0098869                                                                                   [0.5625, 0.75, 0.75, 0.5625, 0.6666666666666666, 0.4666666666666667, 0.4666666666666667, 0.6, 0.8, 0.6666666666666666]   \n",
       "GO:0099503                                              [0.5333333333333333, 0.4666666666666667, 0.8666666666666667, 0.6666666666666666, 0.5333333333333333, 0.6, 0.6, 0.5333333333333333, 0.5333333333333333, 0.4]   \n",
       "GO:0106310  [0.7794117647058824, 0.7352941176470589, 0.5735294117647058, 0.5147058823529411, 0.5373134328358209, 0.5671641791044776, 0.5671641791044776, 0.6417910447761194, 0.6716417910447762, 0.5373134328358...   \n",
       "GO:0110165                              [0.6428571428571429, 0.42857142857142855, 0.5, 0.4642857142857143, 0.5357142857142857, 0.5, 0.5357142857142857, 0.5714285714285714, 0.5555555555555556, 0.5555555555555556]   \n",
       "\n",
       "           Mean Validation Accuracy Std Validation Accuracy   \n",
       "GO:0000166                57.729815                2.461647  \\\n",
       "GO:0000287                   55.625               12.945197   \n",
       "GO:0000325                72.173913                2.576288   \n",
       "GO:0000398                69.338235                6.991483   \n",
       "GO:0000976                69.106306                7.548179   \n",
       "...                             ...                     ...   \n",
       "GO:0090502                59.555556               10.590002   \n",
       "GO:0098869                62.916667               11.124298   \n",
       "GO:0099503                57.333333                    12.0   \n",
       "GO:0106310                61.253292                8.590271   \n",
       "GO:0110165                52.896825                5.676268   \n",
       "\n",
       "                                                                                                                                                                              Validation Precision scores   \n",
       "GO:0000166    [0.5503875968992248, 0.568, 0.5902777777777778, 0.5857142857142857, 0.5895522388059702, 0.6029411764705882, 0.5319148936170213, 0.5695364238410596, 0.5454545454545454, 0.5821917808219178]  \\\n",
       "GO:0000287                                             [0.4444444444444444, 0.5714285714285714, 0.25, 0.625, 0.4444444444444444, 0.625, 0.6666666666666666, 0.42857142857142855, 1.0, 0.5714285714285714]   \n",
       "GO:0000325   [0.7352941176470589, 0.6944444444444444, 0.7027027027027027, 0.7027027027027027, 0.7567567567567568, 0.6666666666666666, 0.71875, 0.696969696969697, 0.7297297297297297, 0.7297297297297297]   \n",
       "GO:0000398                                   [0.75, 0.6363636363636364, 0.8333333333333334, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6363636363636364, 0.6666666666666666, 0.6666666666666666, 1.0]   \n",
       "GO:0000976                    [0.6944444444444444, 0.6170212765957447, 0.5208333333333334, 0.7714285714285715, 0.6923076923076923, 0.8, 0.8333333333333334, 0.775, 0.631578947368421, 0.6666666666666666]   \n",
       "...                                                                                                                                                                                                   ...   \n",
       "GO:0090502                                                               [0.6666666666666666, 0.625, 0.6, 0.5714285714285714, 0.6666666666666666, 0.5, 0.3333333333333333, 0.5, 0.75, 0.5714285714285714]   \n",
       "GO:0098869                                                                                 [0.5555555555555556, 0.7, 1.0, 0.6, 0.7142857142857143, 0.5, 0.5, 0.6, 0.8333333333333334, 0.6666666666666666]   \n",
       "GO:0099503                                                   [0.5454545454545454, 0.5, 1.0, 0.6666666666666666, 0.5714285714285714, 0.5555555555555556, 0.5714285714285714, 0.5, 0.5, 0.3333333333333333]   \n",
       "GO:0106310  [0.7714285714285715, 0.7352941176470589, 0.5757575757575758, 0.5161290322580645, 0.5384615384615384, 0.5714285714285714, 0.5581395348837209, 0.6451612903225806, 0.6486486486486487, 0.53125]   \n",
       "GO:0110165                                               [0.6666666666666666, 0.4375, 0.5, 0.47058823529411764, 0.5263157894736842, 0.5, 0.5263157894736842, 0.5833333333333334, 0.6, 0.5454545454545454]   \n",
       "\n",
       "           Mean Validation Precision   \n",
       "GO:0000166                  0.571597  \\\n",
       "GO:0000287                  0.562698   \n",
       "GO:0000325                  0.713375   \n",
       "GO:0000398                  0.702273   \n",
       "GO:0000976                  0.700261   \n",
       "...                              ...   \n",
       "GO:0090502                  0.578452   \n",
       "GO:0098869                  0.666984   \n",
       "GO:0099503                  0.574387   \n",
       "GO:0106310                   0.60917   \n",
       "GO:0110165                  0.535617   \n",
       "\n",
       "                                                                                                                                                                                           Validation Recall scores   \n",
       "GO:0000166                                                                                        [0.5503875968992248, 0.5546875, 0.6640625, 0.640625, 0.6171875, 0.640625, 0.5859375, 0.671875, 0.5625, 0.6640625]  \\\n",
       "GO:0000287                                                                                                                                            [0.5, 0.5, 0.125, 0.625, 0.5, 0.625, 0.75, 0.375, 0.625, 0.5]   \n",
       "GO:0000325                [0.7142857142857143, 0.7142857142857143, 0.7428571428571429, 0.7428571428571429, 0.8, 0.7647058823529411, 0.6764705882352942, 0.6764705882352942, 0.7941176470588235, 0.7941176470588235]   \n",
       "GO:0000398                                                                                                   [0.6666666666666666, 0.7777777777777778, 0.5555555555555556, 0.5, 0.75, 1.0, 0.875, 0.75, 0.75, 0.625]   \n",
       "GO:0000976  [0.6578947368421053, 0.7631578947368421, 0.6756756756756757, 0.7297297297297297, 0.7297297297297297, 0.7567567567567568, 0.5405405405405406, 0.8378378378378378, 0.6486486486486487, 0.6486486486486...   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                                     [0.8, 1.0, 0.6, 0.8, 0.4, 0.2, 0.25, 0.5, 0.75, 1.0]   \n",
       "GO:0098869                                                                                               [0.625, 0.875, 0.5, 0.375, 0.625, 0.625, 0.5, 0.42857142857142855, 0.7142857142857143, 0.5714285714285714]   \n",
       "GO:0099503                                                                      [0.75, 0.375, 0.75, 0.75, 0.5, 0.7142857142857143, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.2857142857142857]   \n",
       "GO:0106310  [0.7941176470588235, 0.7352941176470589, 0.5588235294117647, 0.47058823529411764, 0.6176470588235294, 0.5882352941176471, 0.7058823529411765, 0.6060606060606061, 0.7272727272727273, 0.515151515151...   \n",
       "GO:0110165                                                           [0.5714285714285714, 0.5, 0.5, 0.5714285714285714, 0.7142857142857143, 0.5, 0.7142857142857143, 0.5, 0.42857142857142855, 0.46153846153846156]   \n",
       "\n",
       "           Mean Validation Recall   \n",
       "GO:0000166               0.615195  \\\n",
       "GO:0000287                 0.5125   \n",
       "GO:0000325               0.742017   \n",
       "GO:0000398                  0.725   \n",
       "GO:0000976               0.698862   \n",
       "...                           ...   \n",
       "GO:0090502                   0.63   \n",
       "GO:0098869               0.583929   \n",
       "GO:0099503               0.569643   \n",
       "GO:0106310               0.631907   \n",
       "GO:0110165               0.546154   \n",
       "\n",
       "                                                                                                                                                                                               Validation F1 scores   \n",
       "GO:0000166  [0.5503875968992248, 0.5612648221343872, 0.6250000000000001, 0.6119402985074627, 0.6030534351145038, 0.6212121212121211, 0.5576208178438662, 0.6164874551971327, 0.5538461538461538, 0.6204379562043...  \\\n",
       "GO:0000287                       [0.47058823529411764, 0.5333333333333333, 0.16666666666666666, 0.625, 0.47058823529411764, 0.625, 0.7058823529411765, 0.39999999999999997, 0.7692307692307693, 0.5333333333333333]   \n",
       "GO:0000325  [0.7246376811594202, 0.7042253521126761, 0.7222222222222223, 0.7222222222222223, 0.7777777777777778, 0.7123287671232877, 0.696969696969697, 0.6865671641791046, 0.7605633802816901, 0.7605633802816901]   \n",
       "GO:0000398                               [0.7058823529411765, 0.7000000000000001, 0.6666666666666667, 0.5, 0.7058823529411765, 0.8, 0.7368421052631579, 0.7058823529411765, 0.7058823529411765, 0.7692307692307693]   \n",
       "GO:0000976                             [0.6756756756756757, 0.6823529411764706, 0.5882352941176472, 0.75, 0.7105263157894737, 0.7777777777777778, 0.6557377049180328, 0.8051948051948051, 0.64, 0.6575342465753425]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                          [0.7272727272727272, 0.7692307692307693, 0.6, 0.6666666666666666, 0.5, 0.28571428571428575, 0.28571428571428575, 0.5, 0.75, 0.7272727272727273]   \n",
       "GO:0098869                               [0.5882352941176471, 0.7777777777777777, 0.6666666666666666, 0.4615384615384615, 0.6666666666666666, 0.5555555555555556, 0.5, 0.5, 0.7692307692307692, 0.6153846153846153]   \n",
       "GO:0099503  [0.631578947368421, 0.42857142857142855, 0.8571428571428571, 0.7058823529411765, 0.5333333333333333, 0.6250000000000001, 0.5714285714285714, 0.5333333333333333, 0.4615384615384615, 0.3076923076923...   \n",
       "GO:0106310                [0.782608695652174, 0.735294117647059, 0.5671641791044776, 0.4923076923076923, 0.5753424657534247, 0.5797101449275363, 0.6233766233766234, 0.625, 0.6857142857142857, 0.5230769230769231]   \n",
       "GO:0110165                                              [0.6153846153846153, 0.4666666666666667, 0.5, 0.5161290322580646, 0.6060606060606061, 0.5, 0.6060606060606061, 0.5384615384615384, 0.5, 0.4999999999999999]   \n",
       "\n",
       "           Mean Validation F1 Score  \n",
       "GO:0000166                 0.592125  \n",
       "GO:0000287                 0.529962  \n",
       "GO:0000325                 0.726808  \n",
       "GO:0000398                 0.699627  \n",
       "GO:0000976                 0.694303  \n",
       "...                             ...  \n",
       "GO:0090502                 0.581187  \n",
       "GO:0098869                 0.610106  \n",
       "GO:0099503                  0.56555  \n",
       "GO:0106310                  0.61896  \n",
       "GO:0110165                 0.534876  \n",
       "\n",
       "[227 rows x 17 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CB_Root33_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f4544-823e-4780-8299-b93f4163eb1d",
   "metadata": {},
   "source": [
    "### Comparison between 0.5 and 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db8dbd83-5a3d-4a66-bcc7-02e7d27811fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming that the \"Mean Validation Accuracy\" columns exist in both dataframes\n",
    "mean_val_acc_df1 = CB_Root33_results_df[\"Mean Validation Accuracy\"]\n",
    "mean_val_acc_df2 = CB_Root33_results_df_t7[\"Mean Validation Accuracy\"]\n",
    "\n",
    "# Create a list of data to plot\n",
    "data_to_plot = [mean_val_acc_df1, mean_val_acc_df2]\n",
    "\n",
    "# Create a figure instance\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Create a boxplot\n",
    "ax.boxplot(data_to_plot, labels=[\"CB_Root33_results_df\", \"CB_Root33_results_df_t7\"])\n",
    "ax.set_title(\"Comparison of Mean Validation Accuracy\")\n",
    "ax.set_ylabel(\"Mean Validation Accuracy\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d1b66-f7a0-4cf1-882f-a2b391968405",
   "metadata": {},
   "source": [
    "# Novelty predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad03ee3c-f22a-4e62-ad73-d68a7c266ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this GO Term, Reduced Majority Size: 1281 Minority Size: 1281\n",
      "For this GO Term, Reduced Majority Size: 80 Minority Size: 80\n",
      "For this GO Term, Reduced Majority Size: 345 Minority Size: 345\n",
      "For this GO Term, Reduced Majority Size: 83 Minority Size: 83\n",
      "For this GO Term, Reduced Majority Size: 372 Minority Size: 372\n",
      "For this GO Term, Reduced Majority Size: 122 Minority Size: 122\n",
      "For this GO Term, Reduced Majority Size: 140 Minority Size: 140\n",
      "For this GO Term, Reduced Majority Size: 2244 Minority Size: 2244\n",
      "For this GO Term, Reduced Majority Size: 344 Minority Size: 344\n",
      "For this GO Term, Reduced Majority Size: 1066 Minority Size: 1066\n",
      "For this GO Term, Reduced Majority Size: 63 Minority Size: 63\n",
      "For this GO Term, Reduced Majority Size: 55 Minority Size: 55\n",
      "For this GO Term, Reduced Majority Size: 822 Minority Size: 822\n",
      "For this GO Term, Reduced Majority Size: 539 Minority Size: 539\n",
      "For this GO Term, Reduced Majority Size: 478 Minority Size: 478\n",
      "For this GO Term, Reduced Majority Size: 182 Minority Size: 182\n",
      "For this GO Term, Reduced Majority Size: 54 Minority Size: 54\n",
      "For this GO Term, Reduced Majority Size: 57 Minority Size: 57\n",
      "For this GO Term, Reduced Majority Size: 305 Minority Size: 305\n",
      "For this GO Term, Reduced Majority Size: 107 Minority Size: 107\n",
      "For this GO Term, Reduced Majority Size: 64 Minority Size: 64\n",
      "For this GO Term, Reduced Majority Size: 117 Minority Size: 117\n",
      "For this GO Term, Reduced Majority Size: 141 Minority Size: 141\n",
      "For this GO Term, Reduced Majority Size: 67 Minority Size: 67\n",
      "For this GO Term, Reduced Majority Size: 56 Minority Size: 56\n",
      "For this GO Term, Reduced Majority Size: 129 Minority Size: 129\n",
      "For this GO Term, Reduced Majority Size: 63 Minority Size: 63\n",
      "For this GO Term, Reduced Majority Size: 542 Minority Size: 542\n",
      "For this GO Term, Reduced Majority Size: 442 Minority Size: 442\n",
      "For this GO Term, Reduced Majority Size: 356 Minority Size: 356\n",
      "For this GO Term, Reduced Majority Size: 76 Minority Size: 76\n",
      "For this GO Term, Reduced Majority Size: 61 Minority Size: 61\n",
      "For this GO Term, Reduced Majority Size: 140 Minority Size: 140\n",
      "For this GO Term, Reduced Majority Size: 68 Minority Size: 68\n",
      "For this GO Term, Reduced Majority Size: 176 Minority Size: 176\n",
      "For this GO Term, Reduced Majority Size: 105 Minority Size: 105\n",
      "For this GO Term, Reduced Majority Size: 136 Minority Size: 136\n",
      "For this GO Term, Reduced Majority Size: 3302 Minority Size: 3302\n",
      "For this GO Term, Reduced Majority Size: 114 Minority Size: 114\n",
      "For this GO Term, Reduced Majority Size: 1222 Minority Size: 1222\n",
      "For this GO Term, Reduced Majority Size: 148 Minority Size: 148\n",
      "For this GO Term, Reduced Majority Size: 933 Minority Size: 933\n",
      "For this GO Term, Reduced Majority Size: 787 Minority Size: 787\n",
      "For this GO Term, Reduced Majority Size: 83 Minority Size: 83\n",
      "For this GO Term, Reduced Majority Size: 2828 Minority Size: 2828\n",
      "For this GO Term, Reduced Majority Size: 84 Minority Size: 84\n",
      "For this GO Term, Reduced Majority Size: 62 Minority Size: 62\n",
      "For this GO Term, Reduced Majority Size: 78 Minority Size: 78\n",
      "For this GO Term, Reduced Majority Size: 279 Minority Size: 279\n",
      "For this GO Term, Reduced Majority Size: 1830 Minority Size: 1830\n",
      "For this GO Term, Reduced Majority Size: 811 Minority Size: 811\n",
      "For this GO Term, Reduced Majority Size: 105 Minority Size: 105\n",
      "For this GO Term, Reduced Majority Size: 61 Minority Size: 61\n",
      "For this GO Term, Reduced Majority Size: 231 Minority Size: 231\n",
      "For this GO Term, Reduced Majority Size: 245 Minority Size: 245\n",
      "For this GO Term, Reduced Majority Size: 106 Minority Size: 106\n",
      "For this GO Term, Reduced Majority Size: 175 Minority Size: 175\n",
      "For this GO Term, Reduced Majority Size: 590 Minority Size: 590\n",
      "For this GO Term, Reduced Majority Size: 277 Minority Size: 277\n",
      "For this GO Term, Reduced Majority Size: 579 Minority Size: 579\n",
      "For this GO Term, Reduced Majority Size: 141 Minority Size: 141\n",
      "For this GO Term, Reduced Majority Size: 1300 Minority Size: 1300\n",
      "For this GO Term, Reduced Majority Size: 194 Minority Size: 194\n",
      "For this GO Term, Reduced Majority Size: 138 Minority Size: 138\n",
      "For this GO Term, Reduced Majority Size: 94 Minority Size: 94\n",
      "For this GO Term, Reduced Majority Size: 1402 Minority Size: 1402\n",
      "For this GO Term, Reduced Majority Size: 58 Minority Size: 58\n",
      "For this GO Term, Reduced Majority Size: 286 Minority Size: 286\n",
      "For this GO Term, Reduced Majority Size: 59 Minority Size: 59\n",
      "For this GO Term, Reduced Majority Size: 131 Minority Size: 131\n",
      "For this GO Term, Reduced Majority Size: 101 Minority Size: 101\n",
      "For this GO Term, Reduced Majority Size: 55 Minority Size: 55\n",
      "For this GO Term, Reduced Majority Size: 950 Minority Size: 950\n",
      "For this GO Term, Reduced Majority Size: 199 Minority Size: 199\n",
      "For this GO Term, Reduced Majority Size: 86 Minority Size: 86\n",
      "For this GO Term, Reduced Majority Size: 47 Minority Size: 47\n",
      "For this GO Term, Reduced Majority Size: 152 Minority Size: 152\n",
      "For this GO Term, Reduced Majority Size: 235 Minority Size: 235\n",
      "For this GO Term, Reduced Majority Size: 59 Minority Size: 59\n",
      "For this GO Term, Reduced Majority Size: 97 Minority Size: 97\n",
      "For this GO Term, Reduced Majority Size: 530 Minority Size: 530\n",
      "For this GO Term, Reduced Majority Size: 57 Minority Size: 57\n",
      "For this GO Term, Reduced Majority Size: 250 Minority Size: 250\n",
      "For this GO Term, Reduced Majority Size: 127 Minority Size: 127\n",
      "For this GO Term, Reduced Majority Size: 268 Minority Size: 268\n",
      "For this GO Term, Reduced Majority Size: 80 Minority Size: 80\n",
      "For this GO Term, Reduced Majority Size: 67 Minority Size: 67\n",
      "For this GO Term, Reduced Majority Size: 197 Minority Size: 197\n",
      "For this GO Term, Reduced Majority Size: 56 Minority Size: 56\n",
      "For this GO Term, Reduced Majority Size: 111 Minority Size: 111\n",
      "For this GO Term, Reduced Majority Size: 590 Minority Size: 590\n",
      "For this GO Term, Reduced Majority Size: 71 Minority Size: 71\n",
      "For this GO Term, Reduced Majority Size: 108 Minority Size: 108\n",
      "For this GO Term, Reduced Majority Size: 134 Minority Size: 134\n",
      "For this GO Term, Reduced Majority Size: 130 Minority Size: 130\n",
      "For this GO Term, Reduced Majority Size: 198 Minority Size: 198\n",
      "For this GO Term, Reduced Majority Size: 57 Minority Size: 57\n",
      "For this GO Term, Reduced Majority Size: 79 Minority Size: 79\n",
      "For this GO Term, Reduced Majority Size: 2242 Minority Size: 2242\n",
      "For this GO Term, Reduced Majority Size: 248 Minority Size: 248\n",
      "For this GO Term, Reduced Majority Size: 186 Minority Size: 186\n",
      "For this GO Term, Reduced Majority Size: 65 Minority Size: 65\n",
      "For this GO Term, Reduced Majority Size: 242 Minority Size: 242\n",
      "For this GO Term, Reduced Majority Size: 52 Minority Size: 52\n",
      "For this GO Term, Reduced Majority Size: 59 Minority Size: 59\n",
      "For this GO Term, Reduced Majority Size: 309 Minority Size: 309\n",
      "For this GO Term, Reduced Majority Size: 85 Minority Size: 85\n",
      "For this GO Term, Reduced Majority Size: 94 Minority Size: 94\n",
      "For this GO Term, Reduced Majority Size: 67 Minority Size: 67\n",
      "For this GO Term, Reduced Majority Size: 91 Minority Size: 91\n",
      "For this GO Term, Reduced Majority Size: 59 Minority Size: 59\n",
      "For this GO Term, Reduced Majority Size: 90 Minority Size: 90\n",
      "For this GO Term, Reduced Majority Size: 154 Minority Size: 154\n",
      "For this GO Term, Reduced Majority Size: 184 Minority Size: 184\n",
      "For this GO Term, Reduced Majority Size: 89 Minority Size: 89\n",
      "For this GO Term, Reduced Majority Size: 53 Minority Size: 53\n",
      "For this GO Term, Reduced Majority Size: 185 Minority Size: 185\n",
      "For this GO Term, Reduced Majority Size: 434 Minority Size: 434\n",
      "For this GO Term, Reduced Majority Size: 1065 Minority Size: 1065\n",
      "For this GO Term, Reduced Majority Size: 43 Minority Size: 43\n",
      "For this GO Term, Reduced Majority Size: 101 Minority Size: 101\n",
      "For this GO Term, Reduced Majority Size: 178 Minority Size: 178\n",
      "For this GO Term, Reduced Majority Size: 1011 Minority Size: 1011\n",
      "For this GO Term, Reduced Majority Size: 127 Minority Size: 127\n",
      "For this GO Term, Reduced Majority Size: 337 Minority Size: 337\n",
      "For this GO Term, Reduced Majority Size: 173 Minority Size: 173\n",
      "For this GO Term, Reduced Majority Size: 101 Minority Size: 101\n",
      "For this GO Term, Reduced Majority Size: 61 Minority Size: 61\n",
      "For this GO Term, Reduced Majority Size: 188 Minority Size: 188\n",
      "For this GO Term, Reduced Majority Size: 94 Minority Size: 94\n",
      "For this GO Term, Reduced Majority Size: 63 Minority Size: 63\n",
      "For this GO Term, Reduced Majority Size: 151 Minority Size: 151\n",
      "For this GO Term, Reduced Majority Size: 131 Minority Size: 131\n",
      "For this GO Term, Reduced Majority Size: 200 Minority Size: 200\n",
      "For this GO Term, Reduced Majority Size: 111 Minority Size: 111\n",
      "For this GO Term, Reduced Majority Size: 50 Minority Size: 50\n",
      "For this GO Term, Reduced Majority Size: 69 Minority Size: 69\n",
      "For this GO Term, Reduced Majority Size: 161 Minority Size: 161\n",
      "For this GO Term, Reduced Majority Size: 57 Minority Size: 57\n",
      "For this GO Term, Reduced Majority Size: 107 Minority Size: 107\n",
      "For this GO Term, Reduced Majority Size: 118 Minority Size: 118\n",
      "For this GO Term, Reduced Majority Size: 247 Minority Size: 247\n",
      "For this GO Term, Reduced Majority Size: 60 Minority Size: 60\n",
      "For this GO Term, Reduced Majority Size: 56 Minority Size: 56\n",
      "For this GO Term, Reduced Majority Size: 61 Minority Size: 61\n",
      "For this GO Term, Reduced Majority Size: 90 Minority Size: 90\n",
      "For this GO Term, Reduced Majority Size: 66 Minority Size: 66\n",
      "For this GO Term, Reduced Majority Size: 313 Minority Size: 313\n",
      "For this GO Term, Reduced Majority Size: 88 Minority Size: 88\n",
      "For this GO Term, Reduced Majority Size: 61 Minority Size: 61\n",
      "For this GO Term, Reduced Majority Size: 3472 Minority Size: 3472\n",
      "For this GO Term, Reduced Majority Size: 2917 Minority Size: 2917\n",
      "For this GO Term, Reduced Majority Size: 72 Minority Size: 72\n",
      "For this GO Term, Reduced Majority Size: 134 Minority Size: 134\n",
      "For this GO Term, Reduced Majority Size: 695 Minority Size: 695\n",
      "For this GO Term, Reduced Majority Size: 694 Minority Size: 694\n",
      "For this GO Term, Reduced Majority Size: 632 Minority Size: 632\n",
      "For this GO Term, Reduced Majority Size: 385 Minority Size: 385\n",
      "For this GO Term, Reduced Majority Size: 53 Minority Size: 53\n",
      "For this GO Term, Reduced Majority Size: 133 Minority Size: 133\n",
      "For this GO Term, Reduced Majority Size: 1691 Minority Size: 1691\n",
      "For this GO Term, Reduced Majority Size: 132 Minority Size: 132\n",
      "For this GO Term, Reduced Majority Size: 269 Minority Size: 269\n",
      "For this GO Term, Reduced Majority Size: 56 Minority Size: 56\n",
      "For this GO Term, Reduced Majority Size: 1237 Minority Size: 1237\n",
      "For this GO Term, Reduced Majority Size: 71 Minority Size: 71\n",
      "For this GO Term, Reduced Majority Size: 192 Minority Size: 192\n",
      "For this GO Term, Reduced Majority Size: 184 Minority Size: 184\n",
      "For this GO Term, Reduced Majority Size: 119 Minority Size: 119\n",
      "For this GO Term, Reduced Majority Size: 123 Minority Size: 123\n",
      "For this GO Term, Reduced Majority Size: 197 Minority Size: 197\n",
      "For this GO Term, Reduced Majority Size: 57 Minority Size: 57\n",
      "For this GO Term, Reduced Majority Size: 49 Minority Size: 49\n",
      "For this GO Term, Reduced Majority Size: 182 Minority Size: 182\n",
      "For this GO Term, Reduced Majority Size: 73 Minority Size: 73\n",
      "For this GO Term, Reduced Majority Size: 102 Minority Size: 102\n",
      "For this GO Term, Reduced Majority Size: 176 Minority Size: 176\n",
      "For this GO Term, Reduced Majority Size: 116 Minority Size: 116\n",
      "For this GO Term, Reduced Majority Size: 112 Minority Size: 112\n",
      "For this GO Term, Reduced Majority Size: 137 Minority Size: 137\n",
      "For this GO Term, Reduced Majority Size: 73 Minority Size: 73\n",
      "For this GO Term, Reduced Majority Size: 141 Minority Size: 141\n",
      "For this GO Term, Reduced Majority Size: 83 Minority Size: 83\n",
      "For this GO Term, Reduced Majority Size: 185 Minority Size: 185\n",
      "For this GO Term, Reduced Majority Size: 72 Minority Size: 72\n",
      "For this GO Term, Reduced Majority Size: 78 Minority Size: 78\n",
      "For this GO Term, Reduced Majority Size: 74 Minority Size: 74\n",
      "For this GO Term, Reduced Majority Size: 52 Minority Size: 52\n",
      "For this GO Term, Reduced Majority Size: 162 Minority Size: 162\n",
      "For this GO Term, Reduced Majority Size: 106 Minority Size: 106\n",
      "For this GO Term, Reduced Majority Size: 112 Minority Size: 112\n",
      "For this GO Term, Reduced Majority Size: 85 Minority Size: 85\n",
      "For this GO Term, Reduced Majority Size: 187 Minority Size: 187\n",
      "For this GO Term, Reduced Majority Size: 87 Minority Size: 87\n",
      "For this GO Term, Reduced Majority Size: 242 Minority Size: 242\n",
      "For this GO Term, Reduced Majority Size: 57 Minority Size: 57\n",
      "For this GO Term, Reduced Majority Size: 60 Minority Size: 60\n",
      "For this GO Term, Reduced Majority Size: 96 Minority Size: 96\n",
      "For this GO Term, Reduced Majority Size: 169 Minority Size: 169\n",
      "For this GO Term, Reduced Majority Size: 91 Minority Size: 91\n",
      "For this GO Term, Reduced Majority Size: 79 Minority Size: 79\n",
      "For this GO Term, Reduced Majority Size: 97 Minority Size: 97\n",
      "For this GO Term, Reduced Majority Size: 1632 Minority Size: 1632\n",
      "For this GO Term, Reduced Majority Size: 64 Minority Size: 64\n",
      "For this GO Term, Reduced Majority Size: 162 Minority Size: 162\n",
      "For this GO Term, Reduced Majority Size: 220 Minority Size: 220\n",
      "For this GO Term, Reduced Majority Size: 97 Minority Size: 97\n",
      "For this GO Term, Reduced Majority Size: 61 Minority Size: 61\n",
      "For this GO Term, Reduced Majority Size: 66 Minority Size: 66\n",
      "For this GO Term, Reduced Majority Size: 93 Minority Size: 93\n",
      "For this GO Term, Reduced Majority Size: 258 Minority Size: 258\n",
      "For this GO Term, Reduced Majority Size: 62 Minority Size: 62\n",
      "For this GO Term, Reduced Majority Size: 56 Minority Size: 56\n",
      "For this GO Term, Reduced Majority Size: 76 Minority Size: 76\n",
      "For this GO Term, Reduced Majority Size: 132 Minority Size: 132\n",
      "For this GO Term, Reduced Majority Size: 72 Minority Size: 72\n",
      "For this GO Term, Reduced Majority Size: 400 Minority Size: 400\n",
      "For this GO Term, Reduced Majority Size: 132 Minority Size: 132\n",
      "For this GO Term, Reduced Majority Size: 121 Minority Size: 121\n",
      "For this GO Term, Reduced Majority Size: 195 Minority Size: 195\n",
      "For this GO Term, Reduced Majority Size: 49 Minority Size: 49\n",
      "For this GO Term, Reduced Majority Size: 105 Minority Size: 105\n",
      "For this GO Term, Reduced Majority Size: 46 Minority Size: 46\n",
      "For this GO Term, Reduced Majority Size: 77 Minority Size: 77\n",
      "For this GO Term, Reduced Majority Size: 75 Minority Size: 75\n",
      "For this GO Term, Reduced Majority Size: 337 Minority Size: 337\n",
      "For this GO Term, Reduced Majority Size: 139 Minority Size: 139\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "def cross_validation(model, _X, _y, _cv=5, x_unseen=None, y_unseen=None):\n",
    "    '''\n",
    "    Function to perform 5 Folds Cross-Validation and evaluate accuracy on unseen data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Python Class, default=None\n",
    "            This is the machine learning algorithm to be used for training.\n",
    "    _X: array\n",
    "        This is the matrix of features.\n",
    "    _y: array\n",
    "        This is the target variable.\n",
    "    _cv: int, default=5\n",
    "        Determines the number of folds for cross-validation.\n",
    "    x_unseen: array, default=None\n",
    "        Unseen data features for evaluation.\n",
    "    y_unseen: array, default=None\n",
    "        Unseen data labels for evaluation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The function returns a dictionary containing the following metrics for both training set, validation set,\n",
    "    and the unseen data:\n",
    "    - 'Training Accuracy scores'\n",
    "    - 'Mean Training Accuracy'\n",
    "    - 'Training Precision scores'\n",
    "    - 'Mean Training Precision'\n",
    "    - 'Training Recall scores'\n",
    "    - 'Mean Training Recall'\n",
    "    - 'Training F1 scores'\n",
    "    - 'Mean Training F1 Score'\n",
    "    - 'Validation Accuracy scores'\n",
    "    - 'Mean Validation Accuracy'\n",
    "    - 'Std Validation Accuracy'\n",
    "    - 'Validation Precision scores'\n",
    "    - 'Mean Validation Precision'\n",
    "    - 'Validation Recall scores'\n",
    "    - 'Mean Validation Recall'\n",
    "    - 'Validation F1 scores'\n",
    "    - 'Mean Validation F1 Score'\n",
    "    - 'Mean Unseen Data Accuracy (Threshold 0.5)'\n",
    "    - 'Mean Unseen Data Accuracy (Threshold 0.7)'\n",
    "    '''\n",
    "    _scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    results = cross_validate(estimator=model,\n",
    "                            X=_X,\n",
    "                            y=_y,\n",
    "                            cv=_cv,\n",
    "                            scoring=_scoring,\n",
    "                            return_train_score=True)\n",
    "    \n",
    "    train_accuracy = results['train_accuracy'].mean() * 100\n",
    "    val_accuracy = results['test_accuracy'].mean() * 100\n",
    "\n",
    "    model.fit(_X,_y)\n",
    "    \n",
    "    if x_unseen is not None and y_unseen is not None:\n",
    "        x_unseen_transformed = preprocessor.transform(x_unseen)\n",
    "        y_pred_unseen_05 = (model.predict_proba(x_unseen_transformed)[:, 1] > 0.5).astype(int)\n",
    "        unseen_accuracy_05 = accuracy_score(y_unseen, y_pred_unseen_05) * 100\n",
    "    else:\n",
    "        unseen_accuracy_05 = None\n",
    "\n",
    "    if x_unseen is not None and y_unseen is not None:\n",
    "        x_unseen_transformed = preprocessor.transform(x_unseen)\n",
    "        y_pred_unseen_07 = (model.predict_proba(x_unseen_transformed)[:, 1] > 0.7).astype(int)\n",
    "        unseen_accuracy_07 = accuracy_score(y_unseen, y_pred_unseen_07) * 100\n",
    "    else:\n",
    "        unseen_accuracy_07 = None\n",
    "    \n",
    "    return {\"Training Accuracy scores\": results['train_accuracy'],\n",
    "            \"Mean Training Accuracy\": train_accuracy,\n",
    "            \"Training Precision scores\": results['train_precision'],\n",
    "            \"Mean Training Precision\": results['train_precision'].mean(),\n",
    "            \"Training Recall scores\": results['train_recall'],\n",
    "            \"Mean Training Recall\": results['train_recall'].mean(),\n",
    "            \"Training F1 scores\": results['train_f1'],\n",
    "            \"Mean Training F1 Score\": results['train_f1'].mean(),\n",
    "            \"Validation Accuracy scores\": results['test_accuracy'],\n",
    "            \"Mean Validation Accuracy\": val_accuracy,\n",
    "            \"Std Validation Accuracy\": results['test_accuracy'].std()*100,\n",
    "            \"Validation Precision scores\": results['test_precision'],\n",
    "            \"Mean Validation Precision\": results['test_precision'].mean(),\n",
    "            \"Validation Recall scores\": results['test_recall'],\n",
    "            \"Mean Validation Recall\": results['test_recall'].mean(),\n",
    "            \"Validation F1 scores\": results['test_f1'],\n",
    "            \"Mean Validation F1 Score\": results['test_f1'].mean(),\n",
    "            \"Mean Unseen Data Accuracy (Threshold 0.5)\": unseen_accuracy_05,\n",
    "            \"Mean Unseen Data Accuracy (Threshold 0.7)\": unseen_accuracy_07}\n",
    "\n",
    "CB_Root33_Go_Mean_Accuracy = []\n",
    "CB_Root33_Go_Std_Accuracy = []\n",
    "CB_Root33_Go_10cross_Accuracies = []\n",
    "CB_Root33_Go_10cross_Mean_F1 = []\n",
    "CB_Root33_Go_10cross_Mean_Precision = []\n",
    "CB_Root33_Go_10cross_Mean_Recall = []\n",
    "CB_Root33_results_dict = {}\n",
    "column_headers = list(Root33_data_setA.columns)\n",
    "\n",
    "# Define the new threshold (e.g., 0.7)\n",
    "new_threshold = 0.7\n",
    "\n",
    "for i in range(len(Feature_columns), Root33_data_setA.shape[1]):\n",
    "    X = Root33_data_setA.iloc[:, :len(Feature_columns)]\n",
    "    y = Root33_data_setA.iloc[:, i]\n",
    "\n",
    "    X_unseen = Root33_data_setA.iloc[:, :len(Feature_columns)]\n",
    "    y_unseen = Root33_data_setA.iloc[:, i]\n",
    "\n",
    "    # Determine the minority class\n",
    "    minority_class = 1 if sum(y == 1) < sum(y == 0) else 0\n",
    "    minority_indices = np.where(y == minority_class)[0]\n",
    "    majority_indices = np.where(y != minority_class)[0]\n",
    "    majority_indices_downsampled = resample(majority_indices, replace=False, n_samples=len(minority_indices), random_state=0)\n",
    "\n",
    "    print(\"For this GO Term, Reduced Majority Size:\", len(majority_indices_downsampled), \"Minority Size:\", len(minority_indices))\n",
    "\n",
    "    # Combine minority and downsampled majority indices\n",
    "    indices_combined = np.concatenate([minority_indices, majority_indices_downsampled])\n",
    "\n",
    "    # Subset the data based on the selected indices\n",
    "    X_balanced = X.iloc[indices_combined, :]\n",
    "    y_balanced = y.iloc[indices_combined]\n",
    "\n",
    "    X_unseen_balanced = X_unseen.iloc[indices_combined, :]\n",
    "    y_unseen_balanced = y_unseen.iloc[indices_combined]\n",
    "\n",
    "    # Train the random forest model\n",
    "    goterm = column_headers[i]\n",
    "    CB_Root33_results_dict[goterm] = cross_validation(pipe, X_balanced, y_balanced, 10, x_unseen=X_unseen_balanced, y_unseen=y_unseen_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb2233e-918b-49ff-bf30-2eec5bb63437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy scores</th>\n",
       "      <th>Mean Training Accuracy</th>\n",
       "      <th>Training Precision scores</th>\n",
       "      <th>Mean Training Precision</th>\n",
       "      <th>Training Recall scores</th>\n",
       "      <th>Mean Training Recall</th>\n",
       "      <th>Training F1 scores</th>\n",
       "      <th>Mean Training F1 Score</th>\n",
       "      <th>Validation Accuracy scores</th>\n",
       "      <th>Mean Validation Accuracy</th>\n",
       "      <th>Std Validation Accuracy</th>\n",
       "      <th>Validation Precision scores</th>\n",
       "      <th>Mean Validation Precision</th>\n",
       "      <th>Validation Recall scores</th>\n",
       "      <th>Mean Validation Recall</th>\n",
       "      <th>Validation F1 scores</th>\n",
       "      <th>Mean Validation F1 Score</th>\n",
       "      <th>Mean Unseen Data Accuracy (Threshold 0.5)</th>\n",
       "      <th>Mean Unseen Data Accuracy (Threshold 0.7)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0000166</th>\n",
       "      <td>[0.9622559652928416, 0.9557483731019523, 0.9601040763226366, 0.9631396357328708, 0.9601040763226366, 0.9622723330442324, 0.9575021682567216, 0.9566348655680833, 0.9505637467476149, 0.9592367736339...</td>\n",
       "      <td>95.87562</td>\n",
       "      <td>[0.9683377308707124, 0.9518486672398968, 0.9674008810572687, 0.978494623655914, 0.9657594381035997, 0.9741992882562278, 0.9655781112091791, 0.9606299212598425, 0.9537117903930131, 0.9600347523892268]</td>\n",
       "      <td>0.9646</td>\n",
       "      <td>[0.9557291666666666, 0.9601040763226366, 0.9522983521248916, 0.9470945359930616, 0.9540329575021682, 0.9496964440589766, 0.9488291413703382, 0.9522983521248916, 0.9470945359930616, 0.95836947094536]</td>\n",
       "      <td>0.952555</td>\n",
       "      <td>[0.961992136304063, 0.955958549222798, 0.9597902097902098, 0.9625385632437197, 0.9598603839441535, 0.9617918313570487, 0.9571303587051617, 0.9564459930313589, 0.9503916449086162, 0.959201388888889]</td>\n",
       "      <td>0.95851</td>\n",
       "      <td>[0.5447470817120622, 0.603112840466926, 0.6171875, 0.55078125, 0.625, 0.609375, 0.55078125, 0.5234375, 0.55859375, 0.62109375]</td>\n",
       "      <td>58.041099</td>\n",
       "      <td>3.619364</td>\n",
       "      <td>[0.5491803278688525, 0.5955882352941176, 0.5925925925925926, 0.5528455284552846, 0.6159420289855072, 0.6060606060606061, 0.5454545454545454, 0.5182926829268293, 0.5555555555555556, 0.6131386861313...</td>\n",
       "      <td>0.574465</td>\n",
       "      <td>[0.5193798449612403, 0.6328125, 0.75, 0.53125, 0.6640625, 0.625, 0.609375, 0.6640625, 0.5859375, 0.65625]</td>\n",
       "      <td>0.623813</td>\n",
       "      <td>[0.5338645418326694, 0.6136363636363635, 0.6620689655172414, 0.5418326693227092, 0.6390977443609022, 0.6153846153846154, 0.5756457564575646, 0.5821917808219178, 0.570342205323194, 0.6339622641509434]</td>\n",
       "      <td>0.596803</td>\n",
       "      <td>73.185012</td>\n",
       "      <td>56.167057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000287</th>\n",
       "      <td>[1.0, 1.0, 0.9930555555555556, 1.0, 0.9930555555555556, 1.0, 1.0, 0.9861111111111112, 1.0, 1.0]</td>\n",
       "      <td>99.722222</td>\n",
       "      <td>[1.0, 1.0, 0.9863013698630136, 1.0, 0.9863013698630136, 1.0, 1.0, 0.9861111111111112, 1.0, 1.0]</td>\n",
       "      <td>0.995871</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9861111111111112, 1.0, 1.0]</td>\n",
       "      <td>0.998611</td>\n",
       "      <td>[1.0, 1.0, 0.993103448275862, 1.0, 0.993103448275862, 1.0, 1.0, 0.9861111111111112, 1.0, 1.0]</td>\n",
       "      <td>0.997232</td>\n",
       "      <td>[0.625, 0.5625, 0.625, 0.625, 0.5, 0.75, 0.3125, 0.4375, 0.625, 0.5]</td>\n",
       "      <td>55.625</td>\n",
       "      <td>11.675964</td>\n",
       "      <td>[0.6, 0.5714285714285714, 0.6, 0.625, 0.5, 0.8333333333333334, 0.3333333333333333, 0.42857142857142855, 0.6666666666666666, 0.5]</td>\n",
       "      <td>0.565833</td>\n",
       "      <td>[0.75, 0.5, 0.75, 0.625, 0.375, 0.625, 0.375, 0.375, 0.5, 0.625]</td>\n",
       "      <td>0.55</td>\n",
       "      <td>[0.6666666666666665, 0.5333333333333333, 0.6666666666666665, 0.625, 0.42857142857142855, 0.7142857142857143, 0.35294117647058826, 0.39999999999999997, 0.5714285714285715, 0.5555555555555556]</td>\n",
       "      <td>0.551445</td>\n",
       "      <td>91.25</td>\n",
       "      <td>71.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000325</th>\n",
       "      <td>[0.9887278582930756, 0.9951690821256038, 0.9855072463768116, 0.9855072463768116, 0.9855072463768116, 0.9887278582930756, 0.9951690821256038, 0.9838969404186796, 0.9871175523349437, 0.9871175523349...</td>\n",
       "      <td>98.824477</td>\n",
       "      <td>[0.9840255591054313, 0.9967637540453075, 0.9870550161812298, 0.9870550161812298, 0.9934426229508196, 0.9840764331210191, 0.9935897435897436, 0.9966996699669967, 0.9967213114754099, 0.9840255591054...</td>\n",
       "      <td>0.990345</td>\n",
       "      <td>[0.9935483870967742, 0.9935483870967742, 0.9838709677419355, 0.9838709677419355, 0.9774193548387097, 0.9935691318327974, 0.9967845659163987, 0.9710610932475884, 0.977491961414791, 0.9903536977491961]</td>\n",
       "      <td>0.986152</td>\n",
       "      <td>[0.9887640449438203, 0.9951534733441034, 0.9854604200323102, 0.9854604200323102, 0.9853658536585366, 0.9887999999999999, 0.9951845906902087, 0.9837133550488598, 0.987012987012987, 0.9871794871794871]</td>\n",
       "      <td>0.988209</td>\n",
       "      <td>[0.7536231884057971, 0.7246376811594203, 0.6521739130434783, 0.6666666666666666, 0.7391304347826086, 0.7101449275362319, 0.6956521739130435, 0.7246376811594203, 0.7536231884057971, 0.782608695652174]</td>\n",
       "      <td>72.028986</td>\n",
       "      <td>3.83716</td>\n",
       "      <td>[0.75, 0.7352941176470589, 0.6666666666666666, 0.6666666666666666, 0.7741935483870968, 0.7058823529411765, 0.696969696969697, 0.7272727272727273, 0.7428571428571429, 0.7435897435897436]</td>\n",
       "      <td>0.720939</td>\n",
       "      <td>[0.7714285714285715, 0.7142857142857143, 0.6285714285714286, 0.6857142857142857, 0.6857142857142857, 0.7058823529411765, 0.6764705882352942, 0.7058823529411765, 0.7647058823529411, 0.8529411764705...</td>\n",
       "      <td>0.71916</td>\n",
       "      <td>[0.7605633802816902, 0.7246376811594202, 0.6470588235294118, 0.676056338028169, 0.7272727272727272, 0.7058823529411765, 0.6865671641791046, 0.7164179104477613, 0.7536231884057971, 0.7945205479452054]</td>\n",
       "      <td>0.71926</td>\n",
       "      <td>61.449275</td>\n",
       "      <td>53.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000398</th>\n",
       "      <td>[1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 1.0, 0.9933333333333333]</td>\n",
       "      <td>99.799105</td>\n",
       "      <td>[1.0, 1.0, 0.9866666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.9866666666666667, 1.0, 1.0, 1.0, 0.9866666666666667]</td>\n",
       "      <td>0.997333</td>\n",
       "      <td>[1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 1.0, 0.9932885906040269]</td>\n",
       "      <td>0.997987</td>\n",
       "      <td>[0.7058823529411765, 0.6470588235294118, 0.7647058823529411, 0.6470588235294118, 0.7647058823529411, 0.7647058823529411, 0.625, 0.8125, 0.6875, 0.75]</td>\n",
       "      <td>71.691176</td>\n",
       "      <td>6.023132</td>\n",
       "      <td>[0.7, 0.6363636363636364, 0.8571428571428571, 0.6666666666666666, 0.75, 0.6666666666666666, 0.5833333333333334, 0.7777777777777778, 0.8, 0.75]</td>\n",
       "      <td>0.718795</td>\n",
       "      <td>[0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.5, 0.75, 1.0, 0.875, 0.875, 0.5, 0.75]</td>\n",
       "      <td>0.747222</td>\n",
       "      <td>[0.7368421052631577, 0.7000000000000001, 0.75, 0.5714285714285715, 0.75, 0.8, 0.7000000000000001, 0.823529411764706, 0.6153846153846154, 0.75]</td>\n",
       "      <td>0.719718</td>\n",
       "      <td>73.493976</td>\n",
       "      <td>60.240964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000976</th>\n",
       "      <td>[0.9910313901345291, 0.9895366218236173, 0.9865470852017937, 0.9820627802690582, 0.9895522388059701, 0.9895522388059701, 0.9940298507462687, 0.9940298507462687, 0.9925373134328358, 0.9940298507462...</td>\n",
       "      <td>99.029092</td>\n",
       "      <td>[0.9939759036144579, 0.9939577039274925, 0.9939393939393939, 0.9938837920489296, 0.9910179640718563, 0.9939759036144579, 1.0, 1.0, 0.9910714285714286, 0.996996996996997]</td>\n",
       "      <td>0.994882</td>\n",
       "      <td>[0.9880239520958084, 0.9850299401197605, 0.9791044776119403, 0.9701492537313433, 0.9880597014925373, 0.9850746268656716, 0.9880597014925373, 0.9880597014925373, 0.9940298507462687, 0.991044776119403]</td>\n",
       "      <td>0.985664</td>\n",
       "      <td>[0.990990990990991, 0.9894736842105264, 0.9864661654135337, 0.9818731117824773, 0.9895366218236173, 0.9895052473763117, 0.993993993993994, 0.993993993993994, 0.9925484351713861, 0.9940119760479041]</td>\n",
       "      <td>0.990239</td>\n",
       "      <td>[0.6933333333333334, 0.68, 0.64, 0.6666666666666666, 0.6351351351351351, 0.7027027027027027, 0.6891891891891891, 0.7297297297297297, 0.5945945945945946, 0.5675675675675675]</td>\n",
       "      <td>65.989189</td>\n",
       "      <td>4.78885</td>\n",
       "      <td>[0.7142857142857143, 0.6666666666666666, 0.6136363636363636, 0.6666666666666666, 0.625, 0.7027027027027027, 0.75, 0.7073170731707317, 0.6060606060606061, 0.5581395348837209]</td>\n",
       "      <td>0.661048</td>\n",
       "      <td>[0.6578947368421053, 0.7368421052631579, 0.7297297297297297, 0.6486486486486487, 0.6756756756756757, 0.7027027027027027, 0.5675675675675675, 0.7837837837837838, 0.5405405405405406, 0.6486486486486...</td>\n",
       "      <td>0.669203</td>\n",
       "      <td>[0.684931506849315, 0.7, 0.6666666666666666, 0.6575342465753425, 0.6493506493506493, 0.7027027027027027, 0.6461538461538462, 0.7435897435897435, 0.5714285714285714, 0.6]</td>\n",
       "      <td>0.662236</td>\n",
       "      <td>87.5</td>\n",
       "      <td>74.327957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0090502</th>\n",
       "      <td>[1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.759036</td>\n",
       "      <td>[1.0, 1.0, 0.9761904761904762, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.997619</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.975609756097561, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.997561</td>\n",
       "      <td>[1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 0.9876543209876543, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.997561</td>\n",
       "      <td>[0.7, 0.7, 0.7777777777777778, 0.6666666666666666, 0.4444444444444444, 0.4444444444444444, 0.6666666666666666, 0.6666666666666666, 0.8888888888888888, 0.7777777777777778]</td>\n",
       "      <td>67.333333</td>\n",
       "      <td>13.214283</td>\n",
       "      <td>[0.75, 0.75, 0.8, 0.625, 0.5, 0.5, 0.6, 0.6666666666666666, 1.0, 0.75]</td>\n",
       "      <td>0.694167</td>\n",
       "      <td>[0.6, 0.6, 0.8, 1.0, 0.4, 0.2, 0.75, 0.5, 0.75, 0.75]</td>\n",
       "      <td>0.635</td>\n",
       "      <td>[0.6666666666666665, 0.6666666666666665, 0.8000000000000002, 0.7692307692307693, 0.4444444444444445, 0.28571428571428575, 0.6666666666666665, 0.5714285714285715, 0.8571428571428571, 0.75]</td>\n",
       "      <td>0.647796</td>\n",
       "      <td>93.478261</td>\n",
       "      <td>90.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0098869</th>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9927536231884058, 1.0, 0.9928057553956835, 0.9928057553956835, 0.9928057553956835, 0.9928057553956835, 1.0]</td>\n",
       "      <td>99.639766</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9857142857142858, 1.0, 1.0, 0.9857142857142858, 0.9859154929577465, 0.9859154929577465, 1.0]</td>\n",
       "      <td>0.994326</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.9855072463768116, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9928057553956835, 1.0, 0.9927007299270074, 0.9928057553956835, 0.9929078014184397, 0.9929078014184397, 1.0]</td>\n",
       "      <td>0.996413</td>\n",
       "      <td>[0.5, 0.5625, 0.6875, 0.6875, 0.4, 0.4666666666666667, 0.6, 0.7333333333333333, 0.7333333333333333, 0.6666666666666666]</td>\n",
       "      <td>60.375</td>\n",
       "      <td>11.151498</td>\n",
       "      <td>[0.5, 0.5714285714285714, 1.0, 0.8, 0.4, 0.5, 0.6, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666]</td>\n",
       "      <td>0.637143</td>\n",
       "      <td>[0.625, 0.5, 0.375, 0.5, 0.25, 0.5, 0.75, 0.8571428571428571, 0.8571428571428571, 0.5714285714285714]</td>\n",
       "      <td>0.578571</td>\n",
       "      <td>[0.5555555555555556, 0.5333333333333333, 0.5454545454545454, 0.6153846153846154, 0.3076923076923077, 0.5, 0.6666666666666665, 0.75, 0.75, 0.6153846153846153]</td>\n",
       "      <td>0.583947</td>\n",
       "      <td>75.974026</td>\n",
       "      <td>61.038961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0099503</th>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.9851851851851852, 0.9925925925925926, 1.0, 1.0, 1.0]</td>\n",
       "      <td>99.777778</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.9852941176470589, 0.9855072463768116, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.99708</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.9852941176470589, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.998529</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.9852941176470589, 0.9927007299270074, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.997799</td>\n",
       "      <td>[0.5333333333333333, 0.5333333333333333, 0.9333333333333333, 0.8, 0.4666666666666667, 0.5333333333333333, 0.5333333333333333, 0.6, 0.5333333333333333, 0.4666666666666667]</td>\n",
       "      <td>59.333333</td>\n",
       "      <td>14.437605</td>\n",
       "      <td>[0.5555555555555556, 0.6, 1.0, 0.7777777777777778, 0.5, 0.5, 0.5, 0.5714285714285714, 0.5, 0.45454545454545453]</td>\n",
       "      <td>0.595931</td>\n",
       "      <td>[0.625, 0.375, 0.875, 0.875, 0.5, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.7142857142857143]</td>\n",
       "      <td>0.610714</td>\n",
       "      <td>[0.5882352941176471, 0.4615384615384615, 0.9333333333333333, 0.823529411764706, 0.5, 0.5333333333333333, 0.5333333333333333, 0.5714285714285714, 0.4615384615384615, 0.5555555555555556]</td>\n",
       "      <td>0.596183</td>\n",
       "      <td>88.666667</td>\n",
       "      <td>68.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0106310</th>\n",
       "      <td>[0.9884488448844885, 0.9917491749174917, 0.9867986798679867, 0.9884488448844885, 0.9917627677100495, 0.9983525535420099, 0.9901153212520593, 0.9934102141680395, 0.9917627677100495, 0.9934102141680...</td>\n",
       "      <td>99.142594</td>\n",
       "      <td>[1.0, 0.9966666666666667, 0.9966329966329966, 0.9868421052631579, 0.9933774834437086, 1.0, 0.9966555183946488, 1.0, 0.9966777408637874, 0.9966887417218543]</td>\n",
       "      <td>0.996354</td>\n",
       "      <td>[0.976897689768977, 0.9867986798679867, 0.976897689768977, 0.9900990099009901, 0.9900990099009901, 0.9966996699669967, 0.9834983498349835, 0.9868421052631579, 0.9868421052631579, 0.9901315789473685]</td>\n",
       "      <td>0.986481</td>\n",
       "      <td>[0.988313856427379, 0.9917081260364843, 0.9866666666666667, 0.9884678747940692, 0.9917355371900826, 0.9983471074380166, 0.9900332225913622, 0.9933774834437086, 0.9917355371900826, 0.9933993399339934]</td>\n",
       "      <td>0.991378</td>\n",
       "      <td>[0.7794117647058824, 0.6911764705882353, 0.5588235294117647, 0.5882352941176471, 0.6119402985074627, 0.6119402985074627, 0.5671641791044776, 0.6417910447761194, 0.5522388059701493, 0.6119402985074...</td>\n",
       "      <td>62.14662</td>\n",
       "      <td>6.577324</td>\n",
       "      <td>[0.7878787878787878, 0.6756756756756757, 0.5588235294117647, 0.6071428571428571, 0.6176470588235294, 0.6052631578947368, 0.5555555555555556, 0.6285714285714286, 0.5555555555555556, 0.6129032258064...</td>\n",
       "      <td>0.620502</td>\n",
       "      <td>[0.7647058823529411, 0.7352941176470589, 0.5588235294117647, 0.5, 0.6176470588235294, 0.6764705882352942, 0.7352941176470589, 0.6666666666666666, 0.45454545454545453, 0.5757575757575758]</td>\n",
       "      <td>0.62852</td>\n",
       "      <td>[0.7761194029850745, 0.7042253521126761, 0.5588235294117647, 0.5483870967741935, 0.6176470588235294, 0.6388888888888888, 0.6329113924050633, 0.6470588235294118, 0.5, 0.59375]</td>\n",
       "      <td>0.621781</td>\n",
       "      <td>89.614243</td>\n",
       "      <td>74.925816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0110165</th>\n",
       "      <td>[0.984, 0.996, 0.992, 0.992, 0.984, 0.984, 0.996, 0.984, 1.0, 0.9960159362549801]</td>\n",
       "      <td>99.080159</td>\n",
       "      <td>[0.9689922480620154, 0.9920634920634921, 0.984251968503937, 0.984251968503937, 0.9763779527559056, 0.991869918699187, 1.0, 0.9763779527559056, 1.0, 1.0]</td>\n",
       "      <td>0.987419</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.992, 0.976, 0.992, 0.992, 1.0, 0.9920634920634921]</td>\n",
       "      <td>0.994406</td>\n",
       "      <td>[0.9842519685039369, 0.9960159362549801, 0.9920634920634921, 0.9920634920634921, 0.9841269841269842, 0.9838709677419355, 0.9959839357429718, 0.9841269841269842, 1.0, 0.9960159362549801]</td>\n",
       "      <td>0.990852</td>\n",
       "      <td>[0.5357142857142857, 0.5, 0.5714285714285714, 0.5357142857142857, 0.4642857142857143, 0.5, 0.7142857142857143, 0.5, 0.5925925925925926, 0.37037037037037035]</td>\n",
       "      <td>52.843915</td>\n",
       "      <td>8.49331</td>\n",
       "      <td>[0.5454545454545454, 0.5, 0.5625, 0.5333333333333333, 0.47368421052631576, 0.5, 0.75, 0.5, 0.6363636363636364, 0.3]</td>\n",
       "      <td>0.530134</td>\n",
       "      <td>[0.42857142857142855, 0.5, 0.6428571428571429, 0.5714285714285714, 0.6428571428571429, 0.5714285714285714, 0.6428571428571429, 0.5, 0.5, 0.23076923076923078]</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>[0.4799999999999999, 0.5, 0.6000000000000001, 0.5517241379310344, 0.5454545454545454, 0.5333333333333333, 0.6923076923076924, 0.5, 0.56, 0.2608695652173913]</td>\n",
       "      <td>0.522369</td>\n",
       "      <td>90.28777</td>\n",
       "      <td>67.266187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                           Training Accuracy scores   \n",
       "GO:0000166  [0.9622559652928416, 0.9557483731019523, 0.9601040763226366, 0.9631396357328708, 0.9601040763226366, 0.9622723330442324, 0.9575021682567216, 0.9566348655680833, 0.9505637467476149, 0.9592367736339...  \\\n",
       "GO:0000287                                                                                                          [1.0, 1.0, 0.9930555555555556, 1.0, 0.9930555555555556, 1.0, 1.0, 0.9861111111111112, 1.0, 1.0]   \n",
       "GO:0000325  [0.9887278582930756, 0.9951690821256038, 0.9855072463768116, 0.9855072463768116, 0.9855072463768116, 0.9887278582930756, 0.9951690821256038, 0.9838969404186796, 0.9871175523349437, 0.9871175523349...   \n",
       "GO:0000398                                                                                                          [1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 1.0, 0.9933333333333333]   \n",
       "GO:0000976  [0.9910313901345291, 0.9895366218236173, 0.9865470852017937, 0.9820627802690582, 0.9895522388059701, 0.9895522388059701, 0.9940298507462687, 0.9940298507462687, 0.9925373134328358, 0.9940298507462...   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                         [1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0098869                                                                            [1.0, 1.0, 1.0, 0.9927536231884058, 1.0, 0.9928057553956835, 0.9928057553956835, 0.9928057553956835, 0.9928057553956835, 1.0]   \n",
       "GO:0099503                                                                                                                         [1.0, 1.0, 1.0, 1.0, 1.0, 0.9851851851851852, 0.9925925925925926, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.9884488448844885, 0.9917491749174917, 0.9867986798679867, 0.9884488448844885, 0.9917627677100495, 0.9983525535420099, 0.9901153212520593, 0.9934102141680395, 0.9917627677100495, 0.9934102141680...   \n",
       "GO:0110165                                                                                                                        [0.984, 0.996, 0.992, 0.992, 0.984, 0.984, 0.996, 0.984, 1.0, 0.9960159362549801]   \n",
       "\n",
       "           Mean Training Accuracy   \n",
       "GO:0000166               95.87562  \\\n",
       "GO:0000287              99.722222   \n",
       "GO:0000325              98.824477   \n",
       "GO:0000398              99.799105   \n",
       "GO:0000976              99.029092   \n",
       "...                           ...   \n",
       "GO:0090502              99.759036   \n",
       "GO:0098869              99.639766   \n",
       "GO:0099503              99.777778   \n",
       "GO:0106310              99.142594   \n",
       "GO:0110165              99.080159   \n",
       "\n",
       "                                                                                                                                                                                          Training Precision scores   \n",
       "GO:0000166  [0.9683377308707124, 0.9518486672398968, 0.9674008810572687, 0.978494623655914, 0.9657594381035997, 0.9741992882562278, 0.9655781112091791, 0.9606299212598425, 0.9537117903930131, 0.9600347523892268]  \\\n",
       "GO:0000287                                                                                                          [1.0, 1.0, 0.9863013698630136, 1.0, 0.9863013698630136, 1.0, 1.0, 0.9861111111111112, 1.0, 1.0]   \n",
       "GO:0000325  [0.9840255591054313, 0.9967637540453075, 0.9870550161812298, 0.9870550161812298, 0.9934426229508196, 0.9840764331210191, 0.9935897435897436, 0.9966996699669967, 0.9967213114754099, 0.9840255591054...   \n",
       "GO:0000398                                                                                                                                        [1.0, 1.0, 0.9866666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0000976                                [0.9939759036144579, 0.9939577039274925, 0.9939393939393939, 0.9938837920489296, 0.9910179640718563, 0.9939759036144579, 1.0, 1.0, 0.9910714285714286, 0.996996996996997]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                        [1.0, 1.0, 0.9761904761904762, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0098869                                                                                           [1.0, 1.0, 1.0, 0.9857142857142858, 1.0, 1.0, 0.9857142857142858, 0.9859154929577465, 0.9859154929577465, 1.0]   \n",
       "GO:0099503                                                                                                                         [1.0, 1.0, 1.0, 1.0, 1.0, 0.9852941176470589, 0.9855072463768116, 1.0, 1.0, 1.0]   \n",
       "GO:0106310                                              [1.0, 0.9966666666666667, 0.9966329966329966, 0.9868421052631579, 0.9933774834437086, 1.0, 0.9966555183946488, 1.0, 0.9966777408637874, 0.9966887417218543]   \n",
       "GO:0110165                                                 [0.9689922480620154, 0.9920634920634921, 0.984251968503937, 0.984251968503937, 0.9763779527559056, 0.991869918699187, 1.0, 0.9763779527559056, 1.0, 1.0]   \n",
       "\n",
       "           Mean Training Precision   \n",
       "GO:0000166                  0.9646  \\\n",
       "GO:0000287                0.995871   \n",
       "GO:0000325                0.990345   \n",
       "GO:0000398                0.998667   \n",
       "GO:0000976                0.994882   \n",
       "...                            ...   \n",
       "GO:0090502                0.997619   \n",
       "GO:0098869                0.994326   \n",
       "GO:0099503                 0.99708   \n",
       "GO:0106310                0.996354   \n",
       "GO:0110165                0.987419   \n",
       "\n",
       "                                                                                                                                                                                             Training Recall scores   \n",
       "GO:0000166   [0.9557291666666666, 0.9601040763226366, 0.9522983521248916, 0.9470945359930616, 0.9540329575021682, 0.9496964440589766, 0.9488291413703382, 0.9522983521248916, 0.9470945359930616, 0.95836947094536]  \\\n",
       "GO:0000287                                                                                                                                        [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9861111111111112, 1.0, 1.0]   \n",
       "GO:0000325  [0.9935483870967742, 0.9935483870967742, 0.9838709677419355, 0.9838709677419355, 0.9774193548387097, 0.9935691318327974, 0.9967845659163987, 0.9710610932475884, 0.977491961414791, 0.9903536977491961]   \n",
       "GO:0000398                                                                                                                         [1.0, 1.0, 1.0, 1.0, 1.0, 0.9866666666666667, 1.0, 1.0, 1.0, 0.9866666666666667]   \n",
       "GO:0000976  [0.9880239520958084, 0.9850299401197605, 0.9791044776119403, 0.9701492537313433, 0.9880597014925373, 0.9850746268656716, 0.9880597014925373, 0.9880597014925373, 0.9940298507462687, 0.991044776119403]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                         [1.0, 1.0, 1.0, 1.0, 1.0, 0.975609756097561, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0098869                                                                                                                                        [1.0, 1.0, 1.0, 1.0, 1.0, 0.9855072463768116, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0099503                                                                                                                                        [1.0, 1.0, 1.0, 1.0, 1.0, 0.9852941176470589, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0106310   [0.976897689768977, 0.9867986798679867, 0.976897689768977, 0.9900990099009901, 0.9900990099009901, 0.9966996699669967, 0.9834983498349835, 0.9868421052631579, 0.9868421052631579, 0.9901315789473685]   \n",
       "GO:0110165                                                                                                                                [1.0, 1.0, 1.0, 1.0, 0.992, 0.976, 0.992, 0.992, 1.0, 0.9920634920634921]   \n",
       "\n",
       "           Mean Training Recall   \n",
       "GO:0000166             0.952555  \\\n",
       "GO:0000287             0.998611   \n",
       "GO:0000325             0.986152   \n",
       "GO:0000398             0.997333   \n",
       "GO:0000976             0.985664   \n",
       "...                         ...   \n",
       "GO:0090502             0.997561   \n",
       "GO:0098869             0.998551   \n",
       "GO:0099503             0.998529   \n",
       "GO:0106310             0.986481   \n",
       "GO:0110165             0.994406   \n",
       "\n",
       "                                                                                                                                                                                                 Training F1 scores   \n",
       "GO:0000166    [0.961992136304063, 0.955958549222798, 0.9597902097902098, 0.9625385632437197, 0.9598603839441535, 0.9617918313570487, 0.9571303587051617, 0.9564459930313589, 0.9503916449086162, 0.959201388888889]  \\\n",
       "GO:0000287                                                                                                            [1.0, 1.0, 0.993103448275862, 1.0, 0.993103448275862, 1.0, 1.0, 0.9861111111111112, 1.0, 1.0]   \n",
       "GO:0000325  [0.9887640449438203, 0.9951534733441034, 0.9854604200323102, 0.9854604200323102, 0.9853658536585366, 0.9887999999999999, 0.9951845906902087, 0.9837133550488598, 0.987012987012987, 0.9871794871794871]   \n",
       "GO:0000398                                                                                                          [1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 0.9932885906040269, 1.0, 1.0, 1.0, 0.9932885906040269]   \n",
       "GO:0000976    [0.990990990990991, 0.9894736842105264, 0.9864661654135337, 0.9818731117824773, 0.9895366218236173, 0.9895052473763117, 0.993993993993994, 0.993993993993994, 0.9925484351713861, 0.9940119760479041]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                         [1.0, 1.0, 0.9879518072289156, 1.0, 1.0, 0.9876543209876543, 1.0, 1.0, 1.0, 1.0]   \n",
       "GO:0098869                                                                            [1.0, 1.0, 1.0, 0.9928057553956835, 1.0, 0.9927007299270074, 0.9928057553956835, 0.9929078014184397, 0.9929078014184397, 1.0]   \n",
       "GO:0099503                                                                                                                         [1.0, 1.0, 1.0, 1.0, 1.0, 0.9852941176470589, 0.9927007299270074, 1.0, 1.0, 1.0]   \n",
       "GO:0106310  [0.988313856427379, 0.9917081260364843, 0.9866666666666667, 0.9884678747940692, 0.9917355371900826, 0.9983471074380166, 0.9900332225913622, 0.9933774834437086, 0.9917355371900826, 0.9933993399339934]   \n",
       "GO:0110165                [0.9842519685039369, 0.9960159362549801, 0.9920634920634921, 0.9920634920634921, 0.9841269841269842, 0.9838709677419355, 0.9959839357429718, 0.9841269841269842, 1.0, 0.9960159362549801]   \n",
       "\n",
       "           Mean Training F1 Score   \n",
       "GO:0000166                0.95851  \\\n",
       "GO:0000287               0.997232   \n",
       "GO:0000325               0.988209   \n",
       "GO:0000398               0.997987   \n",
       "GO:0000976               0.990239   \n",
       "...                           ...   \n",
       "GO:0090502               0.997561   \n",
       "GO:0098869               0.996413   \n",
       "GO:0099503               0.997799   \n",
       "GO:0106310               0.991378   \n",
       "GO:0110165               0.990852   \n",
       "\n",
       "                                                                                                                                                                                         Validation Accuracy scores   \n",
       "GO:0000166                                                                           [0.5447470817120622, 0.603112840466926, 0.6171875, 0.55078125, 0.625, 0.609375, 0.55078125, 0.5234375, 0.55859375, 0.62109375]  \\\n",
       "GO:0000287                                                                                                                                     [0.625, 0.5625, 0.625, 0.625, 0.5, 0.75, 0.3125, 0.4375, 0.625, 0.5]   \n",
       "GO:0000325  [0.7536231884057971, 0.7246376811594203, 0.6521739130434783, 0.6666666666666666, 0.7391304347826086, 0.7101449275362319, 0.6956521739130435, 0.7246376811594203, 0.7536231884057971, 0.782608695652174]   \n",
       "GO:0000398                                                    [0.7058823529411765, 0.6470588235294118, 0.7647058823529411, 0.6470588235294118, 0.7647058823529411, 0.7647058823529411, 0.625, 0.8125, 0.6875, 0.75]   \n",
       "GO:0000976                             [0.6933333333333334, 0.68, 0.64, 0.6666666666666666, 0.6351351351351351, 0.7027027027027027, 0.6891891891891891, 0.7297297297297297, 0.5945945945945946, 0.5675675675675675]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                               [0.7, 0.7, 0.7777777777777778, 0.6666666666666666, 0.4444444444444444, 0.4444444444444444, 0.6666666666666666, 0.6666666666666666, 0.8888888888888888, 0.7777777777777778]   \n",
       "GO:0098869                                                                                  [0.5, 0.5625, 0.6875, 0.6875, 0.4, 0.4666666666666667, 0.6, 0.7333333333333333, 0.7333333333333333, 0.6666666666666666]   \n",
       "GO:0099503                               [0.5333333333333333, 0.5333333333333333, 0.9333333333333333, 0.8, 0.4666666666666667, 0.5333333333333333, 0.5333333333333333, 0.6, 0.5333333333333333, 0.4666666666666667]   \n",
       "GO:0106310  [0.7794117647058824, 0.6911764705882353, 0.5588235294117647, 0.5882352941176471, 0.6119402985074627, 0.6119402985074627, 0.5671641791044776, 0.6417910447761194, 0.5522388059701493, 0.6119402985074...   \n",
       "GO:0110165                                             [0.5357142857142857, 0.5, 0.5714285714285714, 0.5357142857142857, 0.4642857142857143, 0.5, 0.7142857142857143, 0.5, 0.5925925925925926, 0.37037037037037035]   \n",
       "\n",
       "           Mean Validation Accuracy Std Validation Accuracy   \n",
       "GO:0000166                58.041099                3.619364  \\\n",
       "GO:0000287                   55.625               11.675964   \n",
       "GO:0000325                72.028986                 3.83716   \n",
       "GO:0000398                71.691176                6.023132   \n",
       "GO:0000976                65.989189                 4.78885   \n",
       "...                             ...                     ...   \n",
       "GO:0090502                67.333333               13.214283   \n",
       "GO:0098869                   60.375               11.151498   \n",
       "GO:0099503                59.333333               14.437605   \n",
       "GO:0106310                 62.14662                6.577324   \n",
       "GO:0110165                52.843915                 8.49331   \n",
       "\n",
       "                                                                                                                                                                                        Validation Precision scores   \n",
       "GO:0000166  [0.5491803278688525, 0.5955882352941176, 0.5925925925925926, 0.5528455284552846, 0.6159420289855072, 0.6060606060606061, 0.5454545454545454, 0.5182926829268293, 0.5555555555555556, 0.6131386861313...  \\\n",
       "GO:0000287                                                                         [0.6, 0.5714285714285714, 0.6, 0.625, 0.5, 0.8333333333333334, 0.3333333333333333, 0.42857142857142855, 0.6666666666666666, 0.5]   \n",
       "GO:0000325                [0.75, 0.7352941176470589, 0.6666666666666666, 0.6666666666666666, 0.7741935483870968, 0.7058823529411765, 0.696969696969697, 0.7272727272727273, 0.7428571428571429, 0.7435897435897436]   \n",
       "GO:0000398                                                           [0.7, 0.6363636363636364, 0.8571428571428571, 0.6666666666666666, 0.75, 0.6666666666666666, 0.5833333333333334, 0.7777777777777778, 0.8, 0.75]   \n",
       "GO:0000976                            [0.7142857142857143, 0.6666666666666666, 0.6136363636363636, 0.6666666666666666, 0.625, 0.7027027027027027, 0.75, 0.7073170731707317, 0.6060606060606061, 0.5581395348837209]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                   [0.75, 0.75, 0.8, 0.625, 0.5, 0.5, 0.6, 0.6666666666666666, 1.0, 0.75]   \n",
       "GO:0098869                                                                                           [0.5, 0.5714285714285714, 1.0, 0.8, 0.4, 0.5, 0.6, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666]   \n",
       "GO:0099503                                                                                          [0.5555555555555556, 0.6, 1.0, 0.7777777777777778, 0.5, 0.5, 0.5, 0.5714285714285714, 0.5, 0.45454545454545453]   \n",
       "GO:0106310  [0.7878787878787878, 0.6756756756756757, 0.5588235294117647, 0.6071428571428571, 0.6176470588235294, 0.6052631578947368, 0.5555555555555556, 0.6285714285714286, 0.5555555555555556, 0.6129032258064...   \n",
       "GO:0110165                                                                                      [0.5454545454545454, 0.5, 0.5625, 0.5333333333333333, 0.47368421052631576, 0.5, 0.75, 0.5, 0.6363636363636364, 0.3]   \n",
       "\n",
       "           Mean Validation Precision   \n",
       "GO:0000166                  0.574465  \\\n",
       "GO:0000287                  0.565833   \n",
       "GO:0000325                  0.720939   \n",
       "GO:0000398                  0.718795   \n",
       "GO:0000976                  0.661048   \n",
       "...                              ...   \n",
       "GO:0090502                  0.694167   \n",
       "GO:0098869                  0.637143   \n",
       "GO:0099503                  0.595931   \n",
       "GO:0106310                  0.620502   \n",
       "GO:0110165                  0.530134   \n",
       "\n",
       "                                                                                                                                                                                           Validation Recall scores   \n",
       "GO:0000166                                                                                                [0.5193798449612403, 0.6328125, 0.75, 0.53125, 0.6640625, 0.625, 0.609375, 0.6640625, 0.5859375, 0.65625]  \\\n",
       "GO:0000287                                                                                                                                         [0.75, 0.5, 0.75, 0.625, 0.375, 0.625, 0.375, 0.375, 0.5, 0.625]   \n",
       "GO:0000325  [0.7714285714285715, 0.7142857142857143, 0.6285714285714286, 0.6857142857142857, 0.6857142857142857, 0.7058823529411765, 0.6764705882352942, 0.7058823529411765, 0.7647058823529411, 0.8529411764705...   \n",
       "GO:0000398                                                                                                    [0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.5, 0.75, 1.0, 0.875, 0.875, 0.5, 0.75]   \n",
       "GO:0000976  [0.6578947368421053, 0.7368421052631579, 0.7297297297297297, 0.6486486486486487, 0.6756756756756757, 0.7027027027027027, 0.5675675675675675, 0.7837837837837838, 0.5405405405405406, 0.6486486486486...   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502                                                                                                                                                    [0.6, 0.6, 0.8, 1.0, 0.4, 0.2, 0.75, 0.5, 0.75, 0.75]   \n",
       "GO:0098869                                                                                                    [0.625, 0.5, 0.375, 0.5, 0.25, 0.5, 0.75, 0.8571428571428571, 0.8571428571428571, 0.5714285714285714]   \n",
       "GO:0099503                                                                   [0.625, 0.375, 0.875, 0.875, 0.5, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.7142857142857143]   \n",
       "GO:0106310               [0.7647058823529411, 0.7352941176470589, 0.5588235294117647, 0.5, 0.6176470588235294, 0.6764705882352942, 0.7352941176470589, 0.6666666666666666, 0.45454545454545453, 0.5757575757575758]   \n",
       "GO:0110165                                            [0.42857142857142855, 0.5, 0.6428571428571429, 0.5714285714285714, 0.6428571428571429, 0.5714285714285714, 0.6428571428571429, 0.5, 0.5, 0.23076923076923078]   \n",
       "\n",
       "           Mean Validation Recall   \n",
       "GO:0000166               0.623813  \\\n",
       "GO:0000287                   0.55   \n",
       "GO:0000325                0.71916   \n",
       "GO:0000398               0.747222   \n",
       "GO:0000976               0.669203   \n",
       "...                           ...   \n",
       "GO:0090502                  0.635   \n",
       "GO:0098869               0.578571   \n",
       "GO:0099503               0.610714   \n",
       "GO:0106310                0.62852   \n",
       "GO:0110165               0.523077   \n",
       "\n",
       "                                                                                                                                                                                               Validation F1 scores   \n",
       "GO:0000166  [0.5338645418326694, 0.6136363636363635, 0.6620689655172414, 0.5418326693227092, 0.6390977443609022, 0.6153846153846154, 0.5756457564575646, 0.5821917808219178, 0.570342205323194, 0.6339622641509434]  \\\n",
       "GO:0000287           [0.6666666666666665, 0.5333333333333333, 0.6666666666666665, 0.625, 0.42857142857142855, 0.7142857142857143, 0.35294117647058826, 0.39999999999999997, 0.5714285714285715, 0.5555555555555556]   \n",
       "GO:0000325  [0.7605633802816902, 0.7246376811594202, 0.6470588235294118, 0.676056338028169, 0.7272727272727272, 0.7058823529411765, 0.6865671641791046, 0.7164179104477613, 0.7536231884057971, 0.7945205479452054]   \n",
       "GO:0000398                                                           [0.7368421052631577, 0.7000000000000001, 0.75, 0.5714285714285715, 0.75, 0.8, 0.7000000000000001, 0.823529411764706, 0.6153846153846154, 0.75]   \n",
       "GO:0000976                                [0.684931506849315, 0.7, 0.6666666666666666, 0.6575342465753425, 0.6493506493506493, 0.7027027027027027, 0.6461538461538462, 0.7435897435897435, 0.5714285714285714, 0.6]   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "GO:0090502              [0.6666666666666665, 0.6666666666666665, 0.8000000000000002, 0.7692307692307693, 0.4444444444444445, 0.28571428571428575, 0.6666666666666665, 0.5714285714285715, 0.8571428571428571, 0.75]   \n",
       "GO:0098869                                            [0.5555555555555556, 0.5333333333333333, 0.5454545454545454, 0.6153846153846154, 0.3076923076923077, 0.5, 0.6666666666666665, 0.75, 0.75, 0.6153846153846153]   \n",
       "GO:0099503                 [0.5882352941176471, 0.4615384615384615, 0.9333333333333333, 0.823529411764706, 0.5, 0.5333333333333333, 0.5333333333333333, 0.5714285714285714, 0.4615384615384615, 0.5555555555555556]   \n",
       "GO:0106310                           [0.7761194029850745, 0.7042253521126761, 0.5588235294117647, 0.5483870967741935, 0.6176470588235294, 0.6388888888888888, 0.6329113924050633, 0.6470588235294118, 0.5, 0.59375]   \n",
       "GO:0110165                                             [0.4799999999999999, 0.5, 0.6000000000000001, 0.5517241379310344, 0.5454545454545454, 0.5333333333333333, 0.6923076923076924, 0.5, 0.56, 0.2608695652173913]   \n",
       "\n",
       "           Mean Validation F1 Score Mean Unseen Data Accuracy (Threshold 0.5)   \n",
       "GO:0000166                 0.596803                                 73.185012  \\\n",
       "GO:0000287                 0.551445                                     91.25   \n",
       "GO:0000325                  0.71926                                 61.449275   \n",
       "GO:0000398                 0.719718                                 73.493976   \n",
       "GO:0000976                 0.662236                                      87.5   \n",
       "...                             ...                                       ...   \n",
       "GO:0090502                 0.647796                                 93.478261   \n",
       "GO:0098869                 0.583947                                 75.974026   \n",
       "GO:0099503                 0.596183                                 88.666667   \n",
       "GO:0106310                 0.621781                                 89.614243   \n",
       "GO:0110165                 0.522369                                  90.28777   \n",
       "\n",
       "           Mean Unseen Data Accuracy (Threshold 0.7)  \n",
       "GO:0000166                                 56.167057  \n",
       "GO:0000287                                    71.875  \n",
       "GO:0000325                                 53.043478  \n",
       "GO:0000398                                 60.240964  \n",
       "GO:0000976                                 74.327957  \n",
       "...                                              ...  \n",
       "GO:0090502                                 90.217391  \n",
       "GO:0098869                                 61.038961  \n",
       "GO:0099503                                 68.666667  \n",
       "GO:0106310                                 74.925816  \n",
       "GO:0110165                                 67.266187  \n",
       "\n",
       "[227 rows x 19 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CB_Root33_results_df = pd.DataFrame(CB_Root33_results_dict).T\n",
    "csv_file_path = '28Jan2024_DAPSeq0.7_Root33_results_df.csv'\n",
    "CB_Root33_results_df.to_csv(csv_file_path, index_label='GO')\n",
    "CB_Root33_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de6f2403-829e-47ee-8e58-0982a9c9621a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "root33_df = pd.read_csv('02Feb2023_Root33_results_df.csv')\n",
    "\n",
    "# Extract the mean validation accuracies\n",
    "mean_val_acc_df1 = root33_df[\"Mean Validation Accuracy\"]\n",
    "\n",
    "# Assuming that the \"Mean Validation Accuracy\" columns exist in both dataframes\n",
    "mean_val_acc_df2 = CB_Root33_results_df[\"Mean Unseen Data Accuracy (Threshold 0.5)\"]\n",
    "\n",
    "# Create a list of data to plot\n",
    "data_to_plot = [mean_val_acc_df1, mean_val_acc_df2]\n",
    "\n",
    "# Create a figure instance\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Create a boxplot\n",
    "box = ax.boxplot(data_to_plot, labels=[\"without DAP-seq)\", \"with DAP-seq\"])\n",
    "\n",
    "# Calculate and annotate mean values\n",
    "mean_values = [data.mean() for data in data_to_plot]\n",
    "for i, mean in enumerate(mean_values):\n",
    "    ax.text(i + 1, mean, f'Mean: {mean:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_title(\"Comparison of Novel Prediction performance on new dataset\")\n",
    "ax.set_ylabel(\"Mean Validation Accuracy\")\n",
    "\n",
    "jpg_path = 'Fig_7_Novel_Predictions_DAPseq.jpg'\n",
    "plt.savefig(jpg_path, format='jpg', dpi=600)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf281e33-c785-41bb-b9b9-2971957f1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataframes\n",
    "root33_df = pd.read_csv('02Feb2023_Root33_results_df.csv')\n",
    "\n",
    "# Extract the mean validation accuracies\n",
    "mean_val_acc_without_dapseq = root33_df[\"Mean Validation Accuracy\"]\n",
    "mean_val_acc_05 = CB_Root33_results_df[\"Mean Unseen Data Accuracy (Threshold 0.5)\"]\n",
    "mean_val_acc_07 = CB_Root33_results_df[\"Mean Unseen Data Accuracy (Threshold 0.7)\"]\n",
    "\n",
    "# Calculate means and standard errors\n",
    "means = [np.mean(dataset) for dataset in [mean_val_acc_without_dapseq, mean_val_acc_05, mean_val_acc_07]]\n",
    "std_errors = [np.std(dataset) / np.sqrt(len(dataset)) for dataset in [mean_val_acc_without_dapseq, mean_val_acc_05, mean_val_acc_07]]\n",
    "\n",
    "# Create a list of data to plot\n",
    "data_to_plot = [mean_val_acc_without_dapseq, mean_val_acc_05, mean_val_acc_07]\n",
    "\n",
    "# Create a figure instance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create a boxplot\n",
    "bp = ax.boxplot(data_to_plot, labels=[\"Without DAP-seq\", \"With DAP-seq (Threshold 0.5)\", \"With DAP-seq (Threshold 0.7)\"])\n",
    "\n",
    "# Annotate mean and error\n",
    "for i in range(len(data_to_plot)):\n",
    "    ax.text(i+1, max(data_to_plot[i]), f'Mean: {means[i]:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_title(\"Comparison of Novel Predictions Across Different Conditions\")\n",
    "ax.set_ylabel(\"Mean Validation Accuracy\")\n",
    "\n",
    "\n",
    "jpg_path = 'Fig_7_Novel_DAPseq.jpg'\n",
    "plt.savefig(jpg_path, format='jpg', dpi=600)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67edbd19-5e1e-414b-a16f-65a9677d1ebe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Mean Unseen Data Accuracy (Threshold 0.5)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Mean Unseen Data Accuracy (Threshold 0.5)'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Extract the mean validation accuracies\u001b[39;00m\n\u001b[1;32m      9\u001b[0m mean_val_acc_without_dapseq \u001b[38;5;241m=\u001b[39m root33_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Validation Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m mean_val_acc_05 \u001b[38;5;241m=\u001b[39m \u001b[43mroot33_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMean Unseen Data Accuracy (Threshold 0.5)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate means and standard errors\u001b[39;00m\n\u001b[1;32m     13\u001b[0m means \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mmean(mean_val_acc_without_dapseq), np\u001b[38;5;241m.\u001b[39mmean(mean_val_acc_05)]\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/projects/leaph/.pyenv/versions/mambaforge/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Mean Unseen Data Accuracy (Threshold 0.5)'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataframes\n",
    "root33_df = pd.read_csv('02Feb2023_Root33_results_df.csv')\n",
    "\n",
    "# Extract the mean validation accuracies\n",
    "mean_val_acc_without_dapseq = root33_df[\"Mean Validation Accuracy\"]\n",
    "mean_val_acc_05 = root33_df[\"Mean Unseen Data Accuracy (Threshold 0.5)\"]\n",
    "\n",
    "# Calculate means and standard errors\n",
    "means = [np.mean(mean_val_acc_without_dapseq), np.mean(mean_val_acc_05)]\n",
    "std_errors = [np.std(mean_val_acc_without_dapseq) / np.sqrt(len(mean_val_acc_without_dapseq)), np.std(mean_val_acc_05) / np.sqrt(len(mean_val_acc_05))]\n",
    "\n",
    "# Create a list of data to plot\n",
    "data_to_plot = [mean_val_acc_without_dapseq, mean_val_acc_05]\n",
    "\n",
    "# Create a figure instance\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Create a boxplot\n",
    "bp = ax.boxplot(data_to_plot, labels=[\"Without DAP-seq\", \"With DAP-seq (Threshold 0.5)\"])\n",
    "\n",
    "# Annotate mean and error\n",
    "for i in range(len(data_to_plot)):\n",
    "    ax.text(i+1, max(data_to_plot[i]), f'Mean: {means[i]:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_title(\"Comparison of Validation Accuracy\")\n",
    "ax.set_ylabel(\"Mean Validation Accuracy\")\n",
    "\n",
    "jpg_path = 'Fig_7_Comparison.jpg'\n",
    "plt.savefig(jpg_path, format='jpg', dpi=600)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87db53d0-c583-4fb3-9d5e-6dd2019892e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon statistic: 0.0\n",
      "P-value: 5.405572410884125e-39\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import wilcoxon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imbalanced_data = Root33_results_df[\"Mean Validation F1 Score\"].tolist()\n",
    "balanced_data = pd.DataFrame(CB_Root33_results_dict).T[\"Mean Validation F1 Score\"].tolist()\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "statistic, p_value = wilcoxon(imbalanced_data, balanced_data)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Wilcoxon statistic: {statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Visualize the results with a box plot\n",
    "plt.boxplot([imbalanced_data, balanced_data], labels=['Imbalanced', 'Balanced'])\n",
    "plt.title('Comparison of F1 Scores in Imbalanced and Balanced Scenarios')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "420c1545-fc14-4643-bd7d-9271c444c582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon statistic: 0.0\n",
      "P-value: 5.405572410884125e-39\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import wilcoxon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imbalanced_data = Root33_results_df[\"Mean Validation Accuracy\"].tolist()\n",
    "balanced_data = pd.DataFrame(CB_Root33_results_dict).T[\"Mean Validation Accuracy\"].tolist()\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "statistic, p_value = wilcoxon(imbalanced_data, balanced_data)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Wilcoxon statistic: {statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Visualize the results with a box plot\n",
    "plt.boxplot([imbalanced_data, balanced_data], labels=['Imbalanced', 'Balanced'])\n",
    "plt.title('Comparison of F1 Scores in Imbalanced and Balanced Scenarios')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834cafd-a1e2-434e-9d2b-fbcd4dae4c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
